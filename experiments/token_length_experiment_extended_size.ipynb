{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07c679fc-ef70-4d61-901c-17d2324f9f28",
      "metadata": {
        "id": "07c679fc-ef70-4d61-901c-17d2324f9f28"
      },
      "source": [
        "The motivation of the experiments is to populate comparsion study for the impact of pdf versus text to GPT.\n",
        "- inputs consists of text, pdf varied from token length, 10, 20, 30k, full length.\n",
        "- GPT agents consist of chatgpt and GPT4\n",
        "\n",
        "Base assumption is that results stay close with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bc007d-90f6-46de-a38c-9d85abfd3e96",
      "metadata": {
        "id": "63bc007d-90f6-46de-a38c-9d85abfd3e96"
      },
      "outputs": [],
      "source": [
        "# pdf\n",
        "import fitz  # PyMuPDF\n",
        "from PyPDF2 import PdfReader\n",
        "import pymupdf\n",
        "# string parse\n",
        "\n",
        "# langchain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "openai_api_key = ''\n",
        "# dataframe processor\n",
        "import numpy as np\n",
        "# os\n",
        "import os, requests, sys, pathlib\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca6dca0-b064-4415-972c-acfea29ef2c0",
      "metadata": {
        "id": "cca6dca0-b064-4415-972c-acfea29ef2c0"
      },
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc97792f-85e3-412a-8ba4-63ffc5f3a028",
      "metadata": {
        "id": "cc97792f-85e3-412a-8ba4-63ffc5f3a028"
      },
      "source": [
        "## GPT (FULL TEXT, 10K, 30K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d391b1-66fa-4731-a73c-d5e3aa4ec394",
      "metadata": {
        "id": "44d391b1-66fa-4731-a73c-d5e3aa4ec394"
      },
      "source": [
        "### preprocess for groundtruths, relevant result page parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed94845-6b16-4f3f-8dca-8ba4ee38fca7",
      "metadata": {
        "scrolled": true,
        "id": "6ed94845-6b16-4f3f-8dca-8ba4ee38fca7"
      },
      "outputs": [],
      "source": [
        "fn = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/groundtruth_table.csv'\n",
        "groundtruth_df = pd.read_csv(fn)\n",
        "# groundtruth_df\n",
        "\n",
        "find_fname = lambda pdf_path: str(pdf_path).split('/')[-1]\n",
        "\n",
        "def extract_text_from_pdf_as_dict(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF at the specified path using PyMuPDF.\n",
        "    \"\"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        return {\n",
        "            i+1:page.get_text() # starting from 1\n",
        "            for i,page in enumerate(doc)\n",
        "        }\n",
        "\n",
        "\n",
        "key_indices = groundtruth_df.page_key.values\n",
        "accuracies = groundtruth_df['Top-5 Accuracy'].values\n",
        "\n",
        "# first key is file nm; second is page number for answer\n",
        "ground_truths = {\n",
        "    idx: [find_fname(path), key_indices[idx]]\n",
        "    for idx, path in enumerate(sorted_pdf_paths)\n",
        "}\n",
        "ground_truths\n",
        "pdf_keys = {\n",
        "    path : key_indices[idx]\n",
        "    for idx, path in enumerate(sorted_pdf_paths)\n",
        "}\n",
        "pdf_page_lengths = {\n",
        "    idx: [path, key_indices[idx], max(extract_text_from_pdf_as_dict(path))]\n",
        "    for idx, path in enumerate(sorted_pdf_paths)\n",
        "}\n",
        "\n",
        "\n",
        "# pdf_page_lengths\n",
        "sorted_pdf_key_page = {\n",
        "    idx: [path, key_indices[idx], extract_text_from_pdf_as_dict(path)[key_indices[idx]] if key_indices[idx]>0 else '']\n",
        "    for idx, path in enumerate(sorted_pdf_paths)\n",
        "}\n",
        "\n",
        "# sorted_pdf_key_page\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91266ed9-9533-4a07-b2dc-ef8547ab93d2",
      "metadata": {
        "id": "91266ed9-9533-4a07-b2dc-ef8547ab93d2",
        "outputId": "7b85524c-b9f7-4b88-d39c-da890e9be86a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>Paper Name</th>\n",
              "      <th>Model</th>\n",
              "      <th>Top-1 Accuracy</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1312.6229v4.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1406.2732v1.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1412.0296v1.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1412.6598v2.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1502.03167v3.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf</td>\n",
              "      <td>HD-CNN: Hierarchical Deep Convolutional Neural...</td>\n",
              "      <td>HD-CNN</td>\n",
              "      <td>37.92</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Zhang_Compact_Representation_for_2014_CVPR_pap...</td>\n",
              "      <td>Compact Representation for Image Classificatio...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Zhang_Supplementary_Meta-Learning_Towards_ICCV...</td>\n",
              "      <td>Supplementary Meta-Learning: Towards a Dynamic...</td>\n",
              "      <td>MLNN</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf</td>\n",
              "      <td>Saliency Detection by Multi-Context Deep Learn...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>omniVec_2023.pdf</td>\n",
              "      <td>OmniVec: Learning robust representations with ...</td>\n",
              "      <td>OminiVec (FT)</td>\n",
              "      <td>92.40%</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             file_name  \\\n",
              "0                                      1312.6229v4.pdf   \n",
              "1                                      1406.2732v1.pdf   \n",
              "2                                      1412.0296v1.pdf   \n",
              "3                                      1412.6598v2.pdf   \n",
              "4                                     1502.03167v3.pdf   \n",
              "..                                                 ...   \n",
              "115   Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf   \n",
              "116  Zhang_Compact_Representation_for_2014_CVPR_pap...   \n",
              "117  Zhang_Supplementary_Meta-Learning_Towards_ICCV...   \n",
              "118     Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf   \n",
              "119                                   omniVec_2023.pdf   \n",
              "\n",
              "                                            Paper Name          Model  \\\n",
              "0                                                  NaN            NaN   \n",
              "1                                                  NaN            NaN   \n",
              "2                                                  NaN            NaN   \n",
              "3                                                  NaN            NaN   \n",
              "4                                                  NaN            NaN   \n",
              "..                                                 ...            ...   \n",
              "115  HD-CNN: Hierarchical Deep Convolutional Neural...        HD-CNN    \n",
              "116  Compact Representation for Image Classificatio...            NaN   \n",
              "117  Supplementary Meta-Learning: Towards a Dynamic...          MLNN    \n",
              "118  Saliency Detection by Multi-Context Deep Learn...            NaN   \n",
              "119  OmniVec: Learning robust representations with ...  OminiVec (FT)   \n",
              "\n",
              "    Top-1 Accuracy                                               path  \n",
              "0              NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "1              NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "2              NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "3              NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "4              NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "..             ...                                                ...  \n",
              "115          37.92                                                NaN  \n",
              "116              -                                                NaN  \n",
              "117              -                                                NaN  \n",
              "118              -                                                NaN  \n",
              "119         92.40%                                                NaN  \n",
              "\n",
              "[120 rows x 5 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "folder_path_20 = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_20/'\n",
        "sorted_pdf_paths_20 = sorted([path for path in Path(folder_path_20).iterdir() if is_pdf(path)])\n",
        "sorted_pdf_key_page_20 = {\n",
        "        idx: [find_fname(path), path]\n",
        "        for idx, path in enumerate(sorted_pdf_paths_20)\n",
        "    }\n",
        "df_20 = pd.DataFrame.from_dict(\n",
        "    sorted_pdf_key_page_20,orient='index',\n",
        "    columns=['file_name', 'path',]\n",
        ")\n",
        "from functools import reduce\n",
        "fn = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/groundtruth_table.csv'\n",
        "groundtruth_df = pd.read_csv(fn)\n",
        "# groundtruth_df\n",
        "template = groundtruth_df.copy()[['file_name', 'Paper Name', 'Model', 'Top-1 Accuracy']]\n",
        "reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='right'), [template, df_20])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "folder_path_100 = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_100/'\n",
        "sorted_pdf_paths_100 = sorted([path for path in Path(folder_path_100).iterdir() if is_pdf(path)])\n",
        "sorted_pdf_key_page_100 = {\n",
        "        idx: [find_fname(path), path]\n",
        "        for idx, path in enumerate(sorted_pdf_paths_100)\n",
        "    }\n",
        "df_100 = pd.DataFrame.from_dict(\n",
        "    sorted_pdf_key_page_100,orient='index',\n",
        "    columns=['file_name', 'path',]\n",
        ")\n",
        "reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='outer'), [template, df_100])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbca7cb6-86a0-4586-8bd2-b3e64274e4d2",
      "metadata": {
        "scrolled": true,
        "id": "bbca7cb6-86a0-4586-8bd2-b3e64274e4d2",
        "outputId": "57472a02-edd3-431e-a777-ad18703d4759"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_idx</th>\n",
              "      <th>file_name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>found_accuracy_in</th>\n",
              "      <th>found_accu_re</th>\n",
              "      <th>rel_page</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1512.03385v1.pdf</td>\n",
              "      <td>95.51</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>model\\ntop-1 err.\\ntop-5 err.\\nVGG-16 [41]\\n28...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1703.09844v5.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(79, 80), match='-'&gt;</td>\n",
              "      <td>Published as a conference paper at ICLR 2018\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1803.00942v3.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(185, 186), match='-'&gt;</td>\n",
              "      <td>Deep Learning with Importance Sampling\\n0\\n500...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1807.10108v5.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(60, 61), match='-'&gt;</td>\n",
              "      <td>6\\n(a) Gaussian white noise\\n(b) Gaussian colo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1807.10119v3.pdf</td>\n",
              "      <td>8</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(2569, 2570), match=...</td>\n",
              "      <td>0\\n50\\n100\\n0\\n50\\n100\\n150\\n200\\n250\\n(a)\\n0\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1807.11164v1.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(18, 19), match='-'&gt;</td>\n",
              "      <td>13\\nModel\\nFLOPs Top-1 err. (%)\\nShuﬄeNet v2-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1807.11254v2.pdf</td>\n",
              "      <td>78.07</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>Extreme Network Compression via Filter Group A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1807.11459v1.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>1807.11626v3.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(104, 105), match='-'&gt;</td>\n",
              "      <td>7.2. Disentangling Search Space and Reward\\nTo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>1909.11155v1.pdf</td>\n",
              "      <td>93.03</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(608, 613), match='9...</td>\n",
              "      <td>Table 1. Classiﬁcation accuracy on CIFAR (ResN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1909.13863v1.pdf</td>\n",
              "      <td>79.9</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(2023, 2027), match=...</td>\n",
              "      <td>6\\nBULAT, TZIMIROPOULOS: XNOR-NET++: IMPROVED ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>omniVec_2023.pdf</td>\n",
              "      <td>-</td>\n",
              "      <td>True</td>\n",
              "      <td>&lt;regex.Match object; span=(151, 152), match='-'&gt;</td>\n",
              "      <td>Dataset\\nMetric\\nModality Encoder\\nBase Encode...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    file_idx         file_name accuracy  found_accuracy_in  \\\n",
              "0          0  1512.03385v1.pdf    95.51              False   \n",
              "1          1  1703.09844v5.pdf        -               True   \n",
              "2          2  1803.00942v3.pdf        -               True   \n",
              "3          3  1807.10108v5.pdf        -               True   \n",
              "4          4  1807.10119v3.pdf        8               True   \n",
              "5          5  1807.11164v1.pdf        -               True   \n",
              "6          6  1807.11254v2.pdf    78.07              False   \n",
              "7          7  1807.11459v1.pdf        -              False   \n",
              "8          8  1807.11626v3.pdf        -               True   \n",
              "9          9  1909.11155v1.pdf    93.03               True   \n",
              "10        10  1909.13863v1.pdf     79.9               True   \n",
              "11        11  omniVec_2023.pdf        -               True   \n",
              "\n",
              "                                        found_accu_re  \\\n",
              "0                                                None   \n",
              "1      <regex.Match object; span=(79, 80), match='-'>   \n",
              "2    <regex.Match object; span=(185, 186), match='-'>   \n",
              "3      <regex.Match object; span=(60, 61), match='-'>   \n",
              "4   <regex.Match object; span=(2569, 2570), match=...   \n",
              "5      <regex.Match object; span=(18, 19), match='-'>   \n",
              "6                                                None   \n",
              "7                                                None   \n",
              "8    <regex.Match object; span=(104, 105), match='-'>   \n",
              "9   <regex.Match object; span=(608, 613), match='9...   \n",
              "10  <regex.Match object; span=(2023, 2027), match=...   \n",
              "11   <regex.Match object; span=(151, 152), match='-'>   \n",
              "\n",
              "                                             rel_page  \n",
              "0   model\\ntop-1 err.\\ntop-5 err.\\nVGG-16 [41]\\n28...  \n",
              "1   Published as a conference paper at ICLR 2018\\n...  \n",
              "2   Deep Learning with Importance Sampling\\n0\\n500...  \n",
              "3   6\\n(a) Gaussian white noise\\n(b) Gaussian colo...  \n",
              "4   0\\n50\\n100\\n0\\n50\\n100\\n150\\n200\\n250\\n(a)\\n0\\...  \n",
              "5   13\\nModel\\nFLOPs Top-1 err. (%)\\nShuﬄeNet v2-5...  \n",
              "6   Extreme Network Compression via Filter Group A...  \n",
              "7                                                      \n",
              "8   7.2. Disentangling Search Space and Reward\\nTo...  \n",
              "9   Table 1. Classiﬁcation accuracy on CIFAR (ResN...  \n",
              "10  6\\nBULAT, TZIMIROPOULOS: XNOR-NET++: IMPROVED ...  \n",
              "11  Dataset\\nMetric\\nModality Encoder\\nBase Encode...  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preprocessing\n",
        "import regex as re\n",
        "\n",
        "def clean_accuracy(v: str)->str:\n",
        "    return v.strip('0%')\n",
        "\n",
        "def find_groundtruth_accuracy_in_page(idx, pdf_path, rel_page):\n",
        "    accuracy = clean_accuracy(accuracies[idx])\n",
        "    found_accuracy_in = accuracy in rel_page\n",
        "    found_accuracy_re = re.search(pattern=accuracy, string=rel_page)\n",
        "    return idx, find_fname(pdf_path), accuracy, found_accuracy_in, found_accuracy_re, rel_page\n",
        "\n",
        "\n",
        "pd.DataFrame(data=[find_groundtruth_accuracy_in_page(idx, pdf_path, rel_page) for idx, (pdf_path, rel_key, rel_page) in sorted_pdf_key_page.items()],\n",
        "             columns=['file_idx', 'file_name', 'accuracy', 'found_accuracy_in', 'found_accu_re', 'rel_page',])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794d094c-1451-449d-9158-be7740ab6421",
      "metadata": {
        "id": "794d094c-1451-449d-9158-be7740ab6421"
      },
      "source": [
        "## Chunk methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16534e0e-30d6-44cd-a9ca-4fa7b6c3a94a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "16534e0e-30d6-44cd-a9ca-4fa7b6c3a94a"
      },
      "source": [
        "## Results with increased paper size (12, 20, 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdcdcd6-9a28-418c-b0c8-9626f55ab4bb",
      "metadata": {
        "id": "ccdcdcd6-9a28-418c-b0c8-9626f55ab4bb",
        "outputId": "6ffdb1d0-0b5c-4876-cd67-1af2f2489627"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|                                                                                                                                                     | 0/8 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "expect  ['OmniVec: Learning robust representations with cross modal sharing'\n",
            " 'OminiVec (FT)' '92.40%']\n",
            "groundtruth resulst\n",
            "[1, 2, 3] 3815.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3545.75\n",
            "['Expected Output:\\nSentence: \"In each case, the models are pre-trained on the ImageNet [8] and then fine-tuned on the target datasets.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"In each case, the models are pre-trained on the ImageNet [8] and then ﬁne-tuned on the target datasets.\"\\nAccuracy: 404', '404', '404']\n",
            "[7, 8, 9] 3841.0\n",
            "['404', '404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 12%|█████████████████▌                                                                                                                          | 1/8 [06:36<46:18, 396.95s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3863.0\n",
            "['Expected Output:\\nSentence: \"A major advantage of the proposed method in comparison to previous work is that, is does not require any additive image dataset nor very costly manual annotation, while it achieves state-of-the-art performances on four publicly available benchmarks in image classification.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"Above the proposal itself and the demonstration of it performance in practice, the other major contribution of this paper lies in Section 4 in which we analyze and clarify the reasons why our approach works. Compared to representations obtained from standard CNNs trained with specific labels, an advantage of MuCaLe appears when the filters fail at the subordinate-level (e.g. in Fig 1, the filters for Tesla and Ford are both weakly activated), which is often the case since the categories are finer thus harder to identify. With our proposal, the descriptor at least contains features that capture common properties among basic-level categories (e.g. filters of Car are highly activated), making it more robust for classification problems.\"\\nAccuracy: 404', '404', '404']\n",
            "[4, 5, 6] 3755.5\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 3658.25\n",
            "['404', '404', '404', '404', 'Expected Output:\\nSentence: \"Szegedy et al. [33]* GoogleNet 90.5 77.7 82.7 81.9 Simonyan et al. [32]* VGG-16 88.8 78.0 86.1 84.5 He et al. [13]* ResNet-50 90.8 78.9 84.4 83.1 He et al. [13]* ResNet-101 91.4 80.1 85.6 84.4\"\\nAccuracy: 90.8']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 25%|███████████████████████████████████                                                                                                         | 2/8 [13:17<39:47, 397.93s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3324.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3113.5\n",
            "['404', '404', 'Expected Output:\\nSentence: \"We can see that the overall accuracy is 61.54%, and some pairs of classes are very confusing with each other (e.g. Knitwear and Sweater), which means that the noisy labels are not so reliable.\"\\nAccuracy: 61.54', 'Expected Output:\\nSentence: \"We can see that the overall accuracy is 61.54%, and some pairs of classes are very confusing with each other (e.g. Knitwear and Sweater), which means that the noisy labels are not so reliable.\"\\nAccuracy: 61.54', 'Expected Output:\\nSentence: \"We can see that the overall accuracy is 61.54%, and some pairs of classes are very confusing with each other (e.g. Knitwear and Sweater), which means that the noisy labels are not so reliable.\"\\nAccuracy: 61.54']\n",
            "[7, 8, 9] 3131.0\n",
            "['Expected Output:\\nSentence: \"However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 38%|████████████████████████████████████████████████████▌                                                                                       | 3/8 [19:56<33:12, 398.44s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3512.0\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3278.0\n",
            "['Expected Output:\\nSentence: \"Our instance of the model attains an error rate of 41.6% on the validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our instance of the model attains an error rate of 41.6% on the validation set.\"\\nAccuracy: 58.4', 'Expected Output:\\nSentence: \"Our instance of the model attains an error rate of 41.6% on the validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our instance of the model attains an error rate of 41.6% on the validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our instance of the model attains an error rate of 41.6% on the validation set.\"\\nAccuracy: 58.4']\n",
            "[7, 8, 9] 3605.75\n",
            "['Expected Output:\\nSentence: \"The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 50%|██████████████████████████████████████████████████████████████████████                                                                      | 4/8 [26:34<26:32, 398.18s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3668.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3624.5\n",
            "['Expected Output:\\nSentence: \"The top-1 and top-5 errors are 39.76% and 17.71%, respectively.\"\\nAccuracy: 39.76', 'Expected Output:\\nSentence: \"The top-1 and top-5 errors are 39.76% and 17.71%, respectively.\"\\nAccuracy: 39.76', 'Expected Output:\\nSentence: \"The top-1 and top-5 errors are 39.76% and 17.71%, respectively.\"\\nAccuracy: 39.76', 'Expected Output:\\nSentence: \"The top-1 and top-5 errors are 39.76% and 17.71%, respectively.\"\\nAccuracy: 39.76', '404']\n",
            "[7, 8, 9] 3754.25\n",
            "['Expected Output:\\nSentence: \"On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively.\"\\nAccuracy: 24.79', 'Expected Output:\\nSentence: \"On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively.\"\\nAccuracy: 24.79', 'Expected Output:\\nSentence: \"On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively.\"\\nAccuracy: 24.79', 'Expected Output:\\nSentence: \"On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively.\"\\nAccuracy: 24.79', 'Expected Output:\\nSentence: \"On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively.\"\\nAccuracy: 24.79']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 62%|███████████████████████████████████████████████████████████████████████████████████████▌                                                    | 5/8 [33:12<19:54, 398.13s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3682.5\n",
            "['Expected Output:\\nSentence: \"Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3270.25\n",
            "['Expected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -', 'Expected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Because we did not ﬁnd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classiﬁers using a DCD linear SVM classiﬁer.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 2. Top-5 accuracy on the ILSVRC 2010 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We evaluate the proposed mutual information based im- portance sorting feature selection method on several large scale benchmarks.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"It is compared with PQ (product quan- tization) and BPBC.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Since the datasets are large scale and time consuming to evaluate, we use PQ results from the lit- erature when they are available for a dataset, otherwise we report PQ results from our own implementation.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We use the Fisher Vector to represent all images, follow- ing the setup in [22].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Only the mean and variance part in FV are used.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The base visual descriptor is SIFT, which is re- duced from 128 to 64 dimensional using PCA.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The number of Gaussian components is 256.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We use the spatial pyramid matching structure in [3] which extracts 8 spatial regions from an image.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Its structure is: the whole image, three hori- zontal regions, and two by two split regions.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The total num- ber of dimensions in FV is D = 64×2×256×8 = 262144.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We revise the dual coordinate descent algorithm to learn a linear SVM classiﬁer from our selected and quantized fea- tures or BPBC; and use the LIBLINEAR software package in our PQ experiments.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The following benchmark datasets are used:\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"• VOC 2007 [6].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"It has 20 object classes.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Each image may contain more than one object.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We use all the train- ing and validation images (5K) for training and the test- ing images (5K) for testing.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"• ILSVRC 2010 [2].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"It has 1000 classes and 1.2M train- ing images.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We use all provided training and testing images for training and testing, respectively.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"• SUN 397 [28].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"It has 397 classes.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In each class, we use 50 training images and 50 testing images.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"• Scene 15 [15].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"It has 15 classes.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In each class, 100 images are used for training, and the rest images are used for testing.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In PQ, we use the segment length d = 8, which has the overall best performance in [22] under different compres- sion ratios and also used in BPBC [10].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We use k-means to generate codebooks.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Then, we change the codebook size K to achieve different compression ratios in PQ.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In BPBC, we reshaped FV into a 128×2048 matrix, and learn bilinear projections to achieve different compression ratios.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"BPBC parameters need iterative updates, for which a maximum of 10 iterations is used in our experiments.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The results are averaged on 5 random train/test splits in Scene 15 and SUN 397.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In VOC 2007, we use the prede- ﬁned split, but run 5 times to get different GMM models and report the average mAP.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For ILSVRC 2010, we run one time using the given split.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"All experiments are tested on a computer with Intel i7-3930K CPU and 32G main mem- ory.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"All CPU cores are used during feature compression.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In classiﬁer learning and testing, only one core is used.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We ﬁrst report the absolute classiﬁcation performance (top 1 accuracy, top 5 accuracy, or mAP).\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Where space permits, we also report the loss of performance (delta be- tween the performance obtained from uncompressed and compressed data) for easier comparisons.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Finally, we com- pare the efﬁciency of feature selection or feature compres- sion, classiﬁer learning and testing for these three methods.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"4.1.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"VOC 2007 Mean average precisions (mAP) of various methods are shown in Table 1.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We use the code from [3]2 to generate FV with the same length as [22], thus it is fair to compare MI’s performance with the PQ result from [22].\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Under the same compression ratio, MI’s mAP is higher than that of PQ on VOC 2007.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The uncompressed result in our experiment and two cited PQ methods are close, but the accuracy loss of MI is less than that of PQ.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For example, when the ratio is 256, MI only loses 1.75% mAP, while PQ in [26] lost 8.5%.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In MI, a compression ratio 32 means that all dimen- sions are kept but quantized.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Similarly, ratio 128 means that a quarter dimensions are selected and quantized.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Ratio 32 provides the best discriminative ability in classiﬁcation, which conﬁrms yet another time that the 1-BIT quantization is effective in keeping useful information in features.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Another important fact is that when compression ratio is smaller than 128, MI’s mAP is higher than the uncom- pressed one.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For example, ratio 64 (half dimensions used) has almost the same mAP as ratio 32 (all dimensions used).\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"This observation corroborates that removing (a large por- tion of) noisy features will not hurt classiﬁcation.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In contrast, PQ’s accuracy decreases quickly and mono- tonically when the compression ratio increases.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"The clas- siﬁcation results of MI with more compression ratios are shown from 32 to 1024.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"When the compression ratio is 256, MI is comparable to that of PQ with compression ratio 32.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Even with compression ratio 1024, MI’s mAP (46.52%) is still acceptable—remember that only 1024 bytes are needed to store the FV for an image at this compression level!\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"MI’s classiﬁer training time on VOC 2007 is shown in Fig. 4.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"When the compression ratio changes from 32 to 1024 (doubling each time), the training time approximately halves each time.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"In other words, training time is roughly linearly proportional to storage size.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"4.2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"ILSVRC 2010 We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Limited by our computer’s memory capacity, we need to start from compression ratio 64 in MI.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"As shown in Table 2, MI’s result is better than PQ’s [22] with the same FV setup and the same compression ratio.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"MI with com- pression ratio 128 has similar result as PQ at ratio 32.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"If absolute accuracy rates are concerned, [19] reported that PQ with a well-tailored SGD classiﬁer achieves 66.5% top-5 accuracy.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"When combining more visual descriptors like color descriptors [22, 23] and LBP [16], higher accu- racy can be achieved on this dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We conjecture that the proposed feature selection framework can also achieve better results than PQ in these richer representations.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"4.3.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"SUN 397 We show accuracy of MI and PQ on the SUN 397 dataset in Table 3.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Limited by our main memory size, we do not evaluate accuracy of the uncompressed dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Because we did not ﬁnd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classiﬁers using a DCD linear SVM classiﬁer.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with PQ, MI is 0.8% in- ferior to PQ when compression ratio is 32, but better than\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Method Compression ratio Accuracy (%) dense FV [23] 1 43.3 multiple features [28] 1 38.0 spatial HOG [8] 1 26.8 MI 32 41.88±0.31 64 42.05±0.36 128 40.42±0.40 256 37.36±0.34 PQ 32 42.72±0.45 64 41.74±0.38 128 40.13±0.33 256 37.84±0.33\"\\nAccuracy: 43.3', 'Expected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -', 'Expected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Because we did not ﬁnd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classiﬁers using a DCD linear SVM classiﬁer.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 2. Top-5 accuracy on the ILSVRC 2010 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Because we did not ﬁnd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classiﬁers using a DCD linear SVM classiﬁer.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than\"\\nAccuracy: -\\n\\nExpected Output:\\n', 'Expected Output:\\nSentence: \"We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 3. Top-1 accuracy on the SUN 397 dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Because we did not ﬁnd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classiﬁers using a DCD linear SVM classiﬁer.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than\"\\nAccuracy: -']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                   | 6/8 [38:05<12:13, 366.65s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3713.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3794.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 4022.0\n",
            "['Expected Output:\\nSentence: \"ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02\"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: \"ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02\"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: \"ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02\"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: \"ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02\"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: \"Table 5 reports the top-1 and top-5 error rates on the validation data. Convolutional nets with MLNN outperform the base models by a large margin (0.4%-1.5% in top-1 error rate and 0.6%-1.4% in top-5 error rate).\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 7/8 [44:45<06:16, 376.70s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3815.5\n",
            "['Expected Output:\\nSentence: \"Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3595.5\n",
            "['404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In Table 1, we show two settings of pre-training schemes. The R-CNN [16] for object detection and segmentation adopted strategy 2 in training (denoted by S1).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404']\n",
            "[7, 8, 9] 2353.5\n",
            "['Expected Output:\\nSentence: \"Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy.\"\\nAccuracy: 67.0', 'Expected Output:\\nSentence: \"Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy.\"\\nAccuracy: 67.0', 'Expected Output:\\nSentence: \"Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy.\"\\nAccuracy: 67.0', 'Expected Output:\\nSentence: \"Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy.\"\\nAccuracy: 67.0', 'Expected Output:\\nSentence: \"Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy.\"\\nAccuracy: 67.0']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [51:27<00:00, 385.99s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>Paper Name</th>\n",
              "      <th>Model</th>\n",
              "      <th>Top-1 Accuracy</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1512.03385v1.pdf</td>\n",
              "      <td>Deep Residual Learning for Image Recognition</td>\n",
              "      <td>ResNet-152</td>\n",
              "      <td>-</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1703.09844v5.pdf</td>\n",
              "      <td>Multiscale Dense Networks for Resource Efficie...</td>\n",
              "      <td>MSDNet</td>\n",
              "      <td>75%</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1803.00942v3.pdf</td>\n",
              "      <td>Not All Samples Are Created Equal: Deep Learni...</td>\n",
              "      <td>ResNet-50</td>\n",
              "      <td>-</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1807.10108v5.pdf</td>\n",
              "      <td>Effects of Degradations on Deep Neural Network...</td>\n",
              "      <td>V-CapsNet</td>\n",
              "      <td>-</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1807.10119v3.pdf</td>\n",
              "      <td>A Unified Approximation Framework for Compress...</td>\n",
              "      <td>AlexNet</td>\n",
              "      <td>-</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1807.11164v1.pdf</td>\n",
              "      <td>ShuffleNet V2: Practical Guidelines for Effici...</td>\n",
              "      <td>ShuffleNet v2-50</td>\n",
              "      <td>77.20%</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1807.11254v2.pdf</td>\n",
              "      <td>Extreme Network Compression via Filter Group A...</td>\n",
              "      <td>VGG16</td>\n",
              "      <td>77.86</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1807.11459v1.pdf</td>\n",
              "      <td>Improving Transferability of Deep Neural Networks</td>\n",
              "      <td>ResNet-27</td>\n",
              "      <td>-</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1807.11626v3.pdf</td>\n",
              "      <td>MnasNet: Platform-Aware Neural Architecture Se...</td>\n",
              "      <td>MnasNet</td>\n",
              "      <td>75.20%</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1909.11155v1.pdf</td>\n",
              "      <td>Anchor Loss: Modulating Loss Scale based on Pr...</td>\n",
              "      <td>ResNet-50</td>\n",
              "      <td>76.82%</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1909.13863v1.pdf</td>\n",
              "      <td>XNOR-Net++: Improved Binary Neural Networks</td>\n",
              "      <td>Binary ResNet-18</td>\n",
              "      <td>57.10%</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sharma_Classification-Driven_Dynamic_Image_CVP...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Tamaazousti_MuCaLe-Net_Multi_Categorical-Level...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Xiao_Learning_From_Massive_2015_CVPR_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Xie_Hyper-Class_Augmented_and_2015_CVPR_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Zhang_Compact_Representation_for_2014_CVPR_pap...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Zhang_Supplementary_Meta-Learning_Towards_ICCV...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>omniVec_2023.pdf</td>\n",
              "      <td>OmniVec: Learning robust representations with ...</td>\n",
              "      <td>OminiVec (FT)</td>\n",
              "      <td>92.40%</td>\n",
              "      <td>/Users/dawn.duan/Library/CloudStorage/OneDrive...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            file_name  \\\n",
              "0                                    1512.03385v1.pdf   \n",
              "1                                    1703.09844v5.pdf   \n",
              "2                                    1803.00942v3.pdf   \n",
              "3                                    1807.10108v5.pdf   \n",
              "4                                    1807.10119v3.pdf   \n",
              "5                                    1807.11164v1.pdf   \n",
              "6                                    1807.11254v2.pdf   \n",
              "7                                    1807.11459v1.pdf   \n",
              "8                                    1807.11626v3.pdf   \n",
              "9                                    1909.11155v1.pdf   \n",
              "10                                   1909.13863v1.pdf   \n",
              "11  Sharma_Classification-Driven_Dynamic_Image_CVP...   \n",
              "12  Tamaazousti_MuCaLe-Net_Multi_Categorical-Level...   \n",
              "13     Xiao_Learning_From_Massive_2015_CVPR_paper.pdf   \n",
              "14  Xie_Hyper-Class_Augmented_and_2015_CVPR_paper.pdf   \n",
              "15   Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf   \n",
              "16  Zhang_Compact_Representation_for_2014_CVPR_pap...   \n",
              "17  Zhang_Supplementary_Meta-Learning_Towards_ICCV...   \n",
              "18     Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf   \n",
              "19                                   omniVec_2023.pdf   \n",
              "\n",
              "                                           Paper Name             Model  \\\n",
              "0        Deep Residual Learning for Image Recognition        ResNet-152   \n",
              "1   Multiscale Dense Networks for Resource Efficie...            MSDNet   \n",
              "2   Not All Samples Are Created Equal: Deep Learni...         ResNet-50   \n",
              "3   Effects of Degradations on Deep Neural Network...        V-CapsNet    \n",
              "4   A Unified Approximation Framework for Compress...           AlexNet   \n",
              "5   ShuffleNet V2: Practical Guidelines for Effici...  ShuffleNet v2-50   \n",
              "6   Extreme Network Compression via Filter Group A...             VGG16   \n",
              "7   Improving Transferability of Deep Neural Networks         ResNet-27   \n",
              "8   MnasNet: Platform-Aware Neural Architecture Se...          MnasNet    \n",
              "9   Anchor Loss: Modulating Loss Scale based on Pr...         ResNet-50   \n",
              "10        XNOR-Net++: Improved Binary Neural Networks  Binary ResNet-18   \n",
              "11                                                NaN               NaN   \n",
              "12                                                NaN               NaN   \n",
              "13                                                NaN               NaN   \n",
              "14                                                NaN               NaN   \n",
              "15                                                NaN               NaN   \n",
              "16                                                NaN               NaN   \n",
              "17                                                NaN               NaN   \n",
              "18                                                NaN               NaN   \n",
              "19  OmniVec: Learning robust representations with ...     OminiVec (FT)   \n",
              "\n",
              "   Top-1 Accuracy                                               path  \n",
              "0               -  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "1             75%  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "2               -  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "3               -  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "4               -  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "5          77.20%  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "6           77.86  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "7               -  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "8          75.20%  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "9          76.82%  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "10         57.10%  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "11            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "12            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "13            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "14            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "15            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "16            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "17            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "18            NaN  /Users/dawn.duan/Library/CloudStorage/OneDrive...  \n",
              "19         92.40%  /Users/dawn.duan/Library/CloudStorage/OneDrive...  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_self_veri = \"\"\"\n",
        "extract the top1 accuracy of ImageNet from the given text and return both the sentence containing the accuracy.\n",
        "Answer in a number, eg. 90.2% and the accuracy value in 1 number. 404 if it's not mentioned. Use the examples below as a guide.\n",
        "\n",
        "Example 1:\n",
        "Expected Output:\n",
        "Sentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\n",
        "Accuracy: 57.1\n",
        "\n",
        "Example 2:\n",
        "Expected Output:\n",
        "Sentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\n",
        "Accuracy: 82.1\n",
        "\n",
        "Example 3:\n",
        "Expected Output:\n",
        "Sentence: \"The evaluation results showed a top-1 accuracy of 78.3% on the test data on Imagenet.\"\n",
        "Accuracy: 78.3\n",
        "\n",
        "Example 4:\n",
        "Expected Output:\n",
        "Sentence: \"Our proposed model achieved a top-1 accuracy of 74.2% when evaluated on the Imagenet dataset.\"\n",
        "Accuracy: 74.2\n",
        "\n",
        "Example 4:\n",
        "Expected Output:\n",
        "Sentence: \"Our proposed model achieved a top-5 accuracy of 66.2% when evaluated on the Imagenet dataset.\"\n",
        "Accuracy: -\n",
        "\n",
        "Now extract the top1 accuracy of ImageNet from the following texts, {page}\n",
        "\n",
        "Expected Output:\n",
        "\"\"\"\n",
        "\n",
        "prompt_vote_accuracy = \"\"\"Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.\n",
        "\n",
        "Example 1:\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "Expected Output: 92.4\n",
        "\n",
        "Example 2:\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4\"\\nAccuracy: 82.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 82.4',\n",
        "Expected Output: 82.4\n",
        "\n",
        "Example 3:\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: 404',\n",
        "Expected Output: -\n",
        "\n",
        "Example 4:\n",
        "'Sentence: \"It's not mentioned.\"\\nAccuracy: -',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's mentioned top-5 accuracy on ImageNet\"\\nAccuracy: -',\n",
        "'Sentence: \"Cocoa 23.3 21.2\"\\nAccuracy: 23.3',\n",
        "Expected Output: -\n",
        "\n",
        "Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}\n",
        "\n",
        "Expected Output:\n",
        "\"\"\"\n",
        "\n",
        "prompt_vote_accuracy = \"\"\"Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.\n",
        "\n",
        "Example 1:\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "Expected Output: 92.4\n",
        "\n",
        "Example 2:\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4\"\\nAccuracy: 82.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 82.4',\n",
        "Expected Output: 82.4\n",
        "\n",
        "Example 3:\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: 404',\n",
        "Expected Output: -\n",
        "\n",
        "Example 4:\n",
        "'Sentence: \"It's not mentioned.\"\\nAccuracy: -',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's mentioned top-5 accuracy on ImageNet\"\\nAccuracy: -',\n",
        "'Sentence: \"Cocoa 23.3 21.2\"\\nAccuracy: 23.3',\n",
        "Expected Output: -\n",
        "\n",
        "Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}\n",
        "\n",
        "Expected Output:\n",
        "\"\"\"\n",
        "\n",
        "def parse_gpt_with_page_prompt(page_with_res, prompt):\n",
        "    openai_api_key = ''\n",
        "    prompt2 = ChatPromptTemplate.from_template(prompt)\n",
        "    output_parser = StrOutputParser()\n",
        "    model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key, temperature=0.0,)\n",
        "\n",
        "    chain = prompt2 | model | output_parser\n",
        "    try:\n",
        "        prompt_value = chain.invoke(\n",
        "            {'page': page_with_res}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        prompt_value = str(e)\n",
        "    return prompt_value\n",
        "\n",
        "def delayed_completion(delays_in_sec, **kwargs):\n",
        "    time.sleep(delays_in_sec)\n",
        "    return parse_gpt_with_page_prompt(**kwargs)\n",
        "\n",
        "def parse_gpt_with_vote(sentences_and_accuracies, prompt):\n",
        "    openai_api_key = ''\n",
        "    model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key, temperature=0.0, max_tokens=5)\n",
        "\n",
        "    chain = ChatPromptTemplate.from_template(prompt) | model | StrOutputParser()\n",
        "    try:\n",
        "        prompt_value = chain.invoke(\n",
        "            {'sentences_and_accuracies': sentences_and_accuracies}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        prompt_value = str(e)\n",
        "    return prompt_value\n",
        "\n",
        "##### PARTIAL PROCESS FOR SINGLE PAPER ############\n",
        "idx = 11\n",
        "pdf_path, rel_key, rel_page = sorted_pdf_key_page[idx]\n",
        "print('expect ', groundtruth_df[['Paper Name','Model','Top-1 Accuracy']].iloc[idx].values)\n",
        "rel_key = rel_key if rel_key>0 else 2\n",
        "rel_pg_num = [rel_key, rel_key-1, rel_key+1]\n",
        "rel_pg_num.sort()\n",
        "text_dict = extract_text_from_pdf_as_dict(pdf_path)\n",
        "rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in rel_pg_num])\n",
        "\n",
        "# Calculate the delay based on your rate limit\n",
        "token_limit_per_minute = 10000\n",
        "delay = 60.0 / token_limit_per_minute * len(rel_page) / 3\n",
        "\n",
        "print('groundtruth resulst')\n",
        "# responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]\n",
        "# print(responses)\n",
        "\n",
        "# print('groundtruth resulst')\n",
        "def chunk_text_keys(keys, stepsize):\n",
        "    lis = []\n",
        "    sublis = []\n",
        "    for k in keys:\n",
        "        sublis += [k]\n",
        "        if k%3 == 0:\n",
        "            lis.append(sublis)\n",
        "            sublis = []\n",
        "    return lis\n",
        "\n",
        "# responses_lis = []\n",
        "# for sublis in tqdm(chunk_text_keys(text_dict.keys(), stepsize=3)):\n",
        "#     rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in sublis])\n",
        "#     print(sublis, len(rel_page)/4)\n",
        "#     responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]\n",
        "#     print(responses)\n",
        "#     responses_lis.append(responses)\n",
        "\n",
        "# [parse_gpt_with_vote(res, prompt_vote_accuracy) for res in responses_lis]\n",
        "##### PARTIAL PROCESS FOR SINGLE PAPER END ############\n",
        "def _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy):\n",
        "    # print('expect ', groundtruth_df[['Paper Name','Model','Top-1 Accuracy']].iloc[idx].values)\n",
        "    text_dict = extract_text_from_pdf_as_dict(pdf_path)\n",
        "\n",
        "    # compose sublist\n",
        "    responses_lis = []\n",
        "    for _, sublis in enumerate(chunk_text_keys(text_dict.keys(), stepsize=4)):\n",
        "        rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in sublis])\n",
        "        print(sublis, len(rel_page)/4)\n",
        "        responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]\n",
        "        responses_lis.append(responses)\n",
        "        print(responses)\n",
        "\n",
        "    # assumption is that the vote_ensemble will consistently return NA or Results\n",
        "    relevant_response = [parse_gpt_with_vote(res, prompt_vote_accuracy) for res in responses_lis]\n",
        "    rel_res = [ans for ans in relevant_response if ans != '404']\n",
        "    final_res = parse_gpt_with_vote(rel_res, prompt_vote_accuracy) if rel_res else '404'\n",
        "    print('#### EOD #####')\n",
        "    return final_res\n",
        "\n",
        "res_df_whole_paper = pd.DataFrame(data=[[idx, find_fname(pdf_path), _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy)] \\\n",
        "                                          for idx, (fname, pdf_path) in tqdm(sorted_pdf_key_page_20.items())], \\\n",
        "                                    columns = ['file_idx', 'file_name', 'gpt_vote_ensemble_whole_paper'])\n",
        "\n",
        "reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='right'), [template, new])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a22f18a-5725-411b-94ff-aacf4601e3ce",
      "metadata": {
        "scrolled": true,
        "id": "6a22f18a-5725-411b-94ff-aacf4601e3ce",
        "outputId": "d82b95f8-14f8-42b1-ff67-695ee3446677"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>Paper Name</th>\n",
              "      <th>Model</th>\n",
              "      <th>Top-1 Accuracy</th>\n",
              "      <th>file_idx</th>\n",
              "      <th>gpt_vote_ensemble_whole_paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1512.03385v1.pdf</td>\n",
              "      <td>Deep Residual Learning for Image Recognition</td>\n",
              "      <td>ResNet-152</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1703.09844v5.pdf</td>\n",
              "      <td>Multiscale Dense Networks for Resource Efficie...</td>\n",
              "      <td>MSDNet</td>\n",
              "      <td>75%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1803.00942v3.pdf</td>\n",
              "      <td>Not All Samples Are Created Equal: Deep Learni...</td>\n",
              "      <td>ResNet-50</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1807.10108v5.pdf</td>\n",
              "      <td>Effects of Degradations on Deep Neural Network...</td>\n",
              "      <td>V-CapsNet</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1807.10119v3.pdf</td>\n",
              "      <td>A Unified Approximation Framework for Compress...</td>\n",
              "      <td>AlexNet</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1807.11164v1.pdf</td>\n",
              "      <td>ShuffleNet V2: Practical Guidelines for Effici...</td>\n",
              "      <td>ShuffleNet v2-50</td>\n",
              "      <td>77.20%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1807.11254v2.pdf</td>\n",
              "      <td>Extreme Network Compression via Filter Group A...</td>\n",
              "      <td>VGG16</td>\n",
              "      <td>77.86</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1807.11459v1.pdf</td>\n",
              "      <td>Improving Transferability of Deep Neural Networks</td>\n",
              "      <td>ResNet-27</td>\n",
              "      <td>-</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1807.11626v3.pdf</td>\n",
              "      <td>MnasNet: Platform-Aware Neural Architecture Se...</td>\n",
              "      <td>MnasNet</td>\n",
              "      <td>75.20%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1909.11155v1.pdf</td>\n",
              "      <td>Anchor Loss: Modulating Loss Scale based on Pr...</td>\n",
              "      <td>ResNet-50</td>\n",
              "      <td>76.82%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1909.13863v1.pdf</td>\n",
              "      <td>XNOR-Net++: Improved Binary Neural Networks</td>\n",
              "      <td>Binary ResNet-18</td>\n",
              "      <td>57.10%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sharma_Classification-Driven_Dynamic_Image_CVP...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Tamaazousti_MuCaLe-Net_Multi_Categorical-Level...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>90.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Xiao_Learning_From_Massive_2015_CVPR_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>61.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Xie_Hyper-Class_Augmented_and_2015_CVPR_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>24.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Zhang_Compact_Representation_for_2014_CVPR_pap...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Zhang_Supplementary_Meta-Learning_Towards_ICCV...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>24.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>67.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>omniVec_2023.pdf</td>\n",
              "      <td>OmniVec: Learning robust representations with ...</td>\n",
              "      <td>OminiVec (FT)</td>\n",
              "      <td>92.40%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            file_name  \\\n",
              "0                                    1512.03385v1.pdf   \n",
              "1                                    1703.09844v5.pdf   \n",
              "2                                    1803.00942v3.pdf   \n",
              "3                                    1807.10108v5.pdf   \n",
              "4                                    1807.10119v3.pdf   \n",
              "5                                    1807.11164v1.pdf   \n",
              "6                                    1807.11254v2.pdf   \n",
              "7                                    1807.11459v1.pdf   \n",
              "8                                    1807.11626v3.pdf   \n",
              "9                                    1909.11155v1.pdf   \n",
              "10                                   1909.13863v1.pdf   \n",
              "11  Sharma_Classification-Driven_Dynamic_Image_CVP...   \n",
              "12  Tamaazousti_MuCaLe-Net_Multi_Categorical-Level...   \n",
              "13     Xiao_Learning_From_Massive_2015_CVPR_paper.pdf   \n",
              "14  Xie_Hyper-Class_Augmented_and_2015_CVPR_paper.pdf   \n",
              "15   Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf   \n",
              "16  Zhang_Compact_Representation_for_2014_CVPR_pap...   \n",
              "17  Zhang_Supplementary_Meta-Learning_Towards_ICCV...   \n",
              "18     Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf   \n",
              "19                                   omniVec_2023.pdf   \n",
              "\n",
              "                                           Paper Name             Model  \\\n",
              "0        Deep Residual Learning for Image Recognition        ResNet-152   \n",
              "1   Multiscale Dense Networks for Resource Efficie...            MSDNet   \n",
              "2   Not All Samples Are Created Equal: Deep Learni...         ResNet-50   \n",
              "3   Effects of Degradations on Deep Neural Network...        V-CapsNet    \n",
              "4   A Unified Approximation Framework for Compress...           AlexNet   \n",
              "5   ShuffleNet V2: Practical Guidelines for Effici...  ShuffleNet v2-50   \n",
              "6   Extreme Network Compression via Filter Group A...             VGG16   \n",
              "7   Improving Transferability of Deep Neural Networks         ResNet-27   \n",
              "8   MnasNet: Platform-Aware Neural Architecture Se...          MnasNet    \n",
              "9   Anchor Loss: Modulating Loss Scale based on Pr...         ResNet-50   \n",
              "10        XNOR-Net++: Improved Binary Neural Networks  Binary ResNet-18   \n",
              "11                                                NaN               NaN   \n",
              "12                                                NaN               NaN   \n",
              "13                                                NaN               NaN   \n",
              "14                                                NaN               NaN   \n",
              "15                                                NaN               NaN   \n",
              "16                                                NaN               NaN   \n",
              "17                                                NaN               NaN   \n",
              "18                                                NaN               NaN   \n",
              "19  OmniVec: Learning robust representations with ...     OminiVec (FT)   \n",
              "\n",
              "   Top-1 Accuracy  file_idx gpt_vote_ensemble_whole_paper  \n",
              "0               -       NaN                           NaN  \n",
              "1             75%       NaN                           NaN  \n",
              "2               -       NaN                           NaN  \n",
              "3               -       NaN                           NaN  \n",
              "4               -       NaN                           NaN  \n",
              "5          77.20%       NaN                           NaN  \n",
              "6           77.86       NaN                           NaN  \n",
              "7               -       NaN                           NaN  \n",
              "8          75.20%       NaN                           NaN  \n",
              "9          76.82%       NaN                           NaN  \n",
              "10         57.10%       NaN                           NaN  \n",
              "11            NaN       0.0                           404  \n",
              "12            NaN       1.0                          90.8  \n",
              "13            NaN       2.0                         61.54  \n",
              "14            NaN       3.0                           404  \n",
              "15            NaN       4.0                         24.79  \n",
              "16            NaN       5.0                             -  \n",
              "17            NaN       6.0                         24.53  \n",
              "18            NaN       7.0                          67.0  \n",
              "19         92.40%       NaN                           NaN  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='outer'), [template, res_df_whole_paper])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a82008-284a-46ef-99f3-96ffb585f54e",
      "metadata": {
        "id": "12a82008-284a-46ef-99f3-96ffb585f54e"
      },
      "source": [
        "## Results with increased paper size (100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3fc6b2-18e7-4e65-9a8a-a178bc0d93af",
      "metadata": {
        "id": "7a3fc6b2-18e7-4e65-9a8a-a178bc0d93af"
      },
      "source": [
        "#### Parse 100 pdfs to the destination folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb92800-18e7-4107-be1c-cd6668a0b903",
      "metadata": {
        "id": "cdb92800-18e7-4107-be1c-cd6668a0b903",
        "outputId": "c714c748-bd3d-4c9e-a20c-69babc328bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moved 100 PDF files from /Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/ to /Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_100/\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define source and destination folder paths\n",
        "source_folder = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/'\n",
        "destination_folder = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_100/'\n",
        "\n",
        "existing_fns = set([find_fname(pdf_path) for pdf_path in sorted_pdf_paths] + [find_fname(pdf_path) for pdf_path in sorted_pdf_paths_20])\n",
        "files = os.listdir(source_folder)\n",
        "pdf_files = [f for f in files if f.endswith('.pdf') if f not in existing_fns]\n",
        "pdf_files\n",
        "\n",
        "# Loop through the list of PDF files and move them\n",
        "for pdf_file in pdf_files[:100]:  # Move only 100 PDF files\n",
        "    source_file = os.path.join(source_folder, pdf_file)\n",
        "    destination_file = os.path.join(destination_folder, pdf_file)\n",
        "\n",
        "    # Move the file\n",
        "    shutil.move(source_file, destination_file)\n",
        "\n",
        "print(f'Moved {len(pdf_files[:100])} PDF files from {source_folder} to {destination_folder}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716c2dd5-f003-4fc0-9bd8-4fa385915a03",
      "metadata": {
        "id": "716c2dd5-f003-4fc0-9bd8-4fa385915a03"
      },
      "source": [
        "## Chunk 100 pdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df057691-7da0-45d9-bea6-6322348fbaff",
      "metadata": {
        "id": "df057691-7da0-45d9-bea6-6322348fbaff",
        "outputId": "e5172205-e5f2-4af2-be23-fada9bccb680"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                                                   | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "expect  ['OmniVec: Learning robust representations with cross modal sharing'\n",
            " 'OminiVec (FT)' '92.40%']\n",
            "groundtruth resulst\n",
            "[1, 2, 3] 2444.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 2535.75\n",
            "['Expected Output:\\nSentence: \"The approach described above, with 6 scales, achieves a top-5 error rate of 13.6%.\"\\nAccuracy: -', 'Expected Output:\\nSentence: \"The accurate model is more accurate than the fast one (14.18% classiﬁcation error as opposed to 16.39% in Table 2), however it requires nearly twice as many connections.\"\\nAccuracy: 85.82', 'Expected Output:\\nSentence: \"The accurate model is more accurate than the fast one (14.18% classiﬁcation error as opposed to 16.39% in Table 2), however it requires nearly twice as many connections.\"\\nAccuracy: 85.82', 'Expected Output:\\nSentence: \"The approach described above, with 6 scales, achieves a top-5 error rate of 13.6%.\"\\nAccuracy: -', 'Expected Output:\\nSentence: \"The accurate model is more accurate than the fast one (14.18% classiﬁcation error as opposed to 16.39% in Table 2), however it requires nearly twice as many connections.\"\\nAccuracy: 85.82']\n",
            "[7, 8, 9] 1253.5\n",
            "['404', '404', 'Expected Output:\\nSentence: \"OverFeat - 7 accurate models, 4 scales, ﬁne stride\"\\nAccuracy: 66.04', 'Expected Output:\\nSentence: \"OverFeat - 7 accurate models, 4 scales, ﬁne stride\"\\nAccuracy: 66.04', '404']\n",
            "[10, 11, 12] 2097.0\n",
            "['404', '404', '404', '404', '404']\n",
            "[13, 14, 15] 2174.5\n",
            "['Expected Output:\\nSentence: \"We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classiﬁcation, 1st in localization and 1st in detection.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classiﬁcation, 1st in localization and 1st in detection.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classiﬁcation, 1st in localization and 1st in detection.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classiﬁcation, 1st in localization and 1st in detection.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classification, 1st in localization and 1st in detection.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|█▎                                                                                                                                     | 1/100 [11:09<18:24:20, 669.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2906.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
            "Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f32b310>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n",
            "Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f5dbbd0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Expected Output:\\nSentence: \"Our best mini-epitomic variant achieves 13.6% top-5 error on the validation set, which is 0.6% better than a conventional max-pooled convolutional network of comparable structure whose error rate is 14.2%.\"\\nAccuracy: 404', 'Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f543c10>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))', 'Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f601710>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))', 'Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f5da550>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))', 'Expected Output:\\nSentence: \"Our best mini-epitomic variant achieves 13.6% top-5 error on the validation set, which is 0.6% better than a conventional max-pooled convolutional network of comparable structure whose error rate is 14.2%.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 2206.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Expected Output:\\nSentence: \"We have found the mean and contrast normalization of Eq. (3) to be crucial for learning the topographic version of the proposed model. We have also found that it significantly accelerates learning of the mini-epitome version of the proposed model, as well as the standard max-pooled convolutional model, without however significantly affecting the final performance of these two model.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have found the mean and contrast normalization of Eq. (3) to be crucial for learning the topographic version of the proposed model. We have also found that it significantly accelerates learning of the mini-epitome version of the proposed model, as well as the standard max-pooled convolutional model, without however significantly affecting the final performance of these two model.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have found the mean and contrast normalization of Eq. (3) to be crucial for learning the topographic version of the proposed model. We have also found that it significantly accelerates learning of the mini-epitome version of the proposed model, as well as the standard max-pooled convolutional model, without however significantly affecting the final performance of these two model.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have found the mean and contrast normalization of Eq. (3) to be crucial for learning the topographic version of the proposed model. We have also found that it significantly accelerates learning of the mini-epitome version of the proposed model, as well as the standard max-pooled convolutional model, without however significantly affecting the final performance of these two model.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have found the mean and contrast normalization of Eq. (3) to be crucial for learning the topographic version of the proposed model. We have also found that it significantly accelerates learning of the mini-epitome version of the proposed model, as well as the standard max-pooled convolutional model, without however significantly affecting the final performance of these two model.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 2493.5\n",
            "['Expected Output:\\nSentence: \"We have shown that the proposed epitomic model performs around 0.5% better than the max-pooled baseline on the challenging Imagenet benchmark and other image classiﬁcation tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have shown that the proposed epitomic model performs around 0.5% better than the max-pooled baseline on the challenging Imagenet benchmark and other image classiﬁcation tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have shown that the proposed epitomic model performs around 0.5% better than the max-pooled baseline on the challenging Imagenet benchmark and other image classification tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have shown that the proposed epitomic model performs around 0.5% better than the max-pooled baseline on the challenging Imagenet benchmark and other image classification tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have shown that the proposed epitomic model performs around 0.5% better than the max-pooled baseline on the challenging Imagenet benchmark and other image classification tasks.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|██▌                                                                                                                                | 2/100 [3:57:17<123:33:38, 4538.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3802.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 2974.0\n",
            "['404', '404', '404', '404', 'Expected Output:\\nSentence: \"Our Class-C deep epitomic network achieves 10.0% error rate in the Imagenet task.\"\\nAccuracy: 90.0']\n",
            "[7, 8, 9] 4021.75\n",
            "['Expected Output:\\nSentence: \"The network outlined above also provides cues for the scale and position of the dominant object in the image. A simple fixed mapping of the “argmax” patchwork position in the last max-pooling layer (computed by averaging the bounding box positions in the training set) yields 48.3% error rate in the Imagenet 2012 localization task without incurring any extra computation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The network outlined above also provides cues for the scale and position of the dominant object in the image. A simple fixed mapping of the “argmax” patchwork position in the last max-pooling layer (computed by averaging the bounding box positions in the training set) yields 48.3% error rate in the Imagenet 2012 localization task without incurring any extra computation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The network outlined above also provides cues for the scale and position of the dominant object in the image. A simple fixed mapping of the “argmax” patchwork position in the last max-pooling layer (computed by averaging the bounding box positions in the training set) yields 48.3% error rate in the Imagenet 2012 localization task without incurring any extra computation.\"\\nAccuracy: 51.7', '404', 'Expected Output:\\nSentence: \"The network outlined above also provides cues for the scale and position of the dominant object in the image. A simple fixed mapping of the “argmax” patchwork position in the last max-pooling layer (computed by averaging the bounding box positions in the training set) yields 48.3% error rate in the Imagenet 2012 localization task without incurring any extra computation.\"\\nAccuracy: 51.7']\n",
            "[10, 11, 12] 2304.75\n",
            "['Expected Output:\\nSentence: \"Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classification task.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classification task.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classification task.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classification task.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classification task.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|███▉                                                                                                                                | 3/100 [4:06:11<89:55:30, 3337.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3016.25\n",
            "['Expected Output:\\nSentence: \"We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[4, 5, 6] 3047.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 2507.5\n",
            "['Expected Output:\\nSentence: \"Razavian et al. (2014) get 58.4% using CNN trained on ImageNet.\"\\nAccuracy: 58.4', 'Expected Output:\\nSentence: \"Razavian et al. (2014) get 58.4% using CNN trained on ImageNet.\"\\nAccuracy: 58.4', 'Expected Output:\\nSentence: \"Razavian et al. (2014) get 58.4% using CNN trained on ImageNet.\"\\nAccuracy: 58.4', 'Expected Output:\\nSentence: \"Razavian et al. (2014) get 58.4% using CNN trained on ImageNet.\"\\nAccuracy: 58.4', 'Expected Output:\\nSentence: \"Razavian et al. (2014) get 58.4% using CNN trained on ImageNet.\"\\nAccuracy: 58.4']\n",
            "[10, 11, 12] 1771.75\n",
            "['Expected Output:\\nSentence: \"Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classication with deep convolutional neural networks. In NIPS, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classication with deep convolutional neural networks. In NIPS, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classication with deep convolutional neural networks. In NIPS, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classication with deep convolutional neural networks. In NIPS, 2012.\"\\nAccuracy: 404']\n",
            "[13, 14, 15] 2477.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 2070.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|█████▎                                                                                                                              | 4/100 [4:19:30<68:41:27, 2575.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3715.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3818.25\n",
            "['404', 'Expected Output:\\nSentence: \"In our experiments, we evaluated several modifications of Inception with Batch Normalization. In all cases, Batch Normalization was applied to the input of each nonlinearity, in a convolutional way, as described in section 3.2, while keeping the rest of the architecture constant.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, we evaluated several modifications of Inception with Batch Normalization. In all cases, Batch Normalization was applied to the input of each nonlinearity, in a convolutional way, as described in section 3.2, while keeping the rest of the architecture constant.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"In our experiments, we evaluated several modifications of Inception with Batch Normalization. In all cases, Batch Normalization was applied to the input of each nonlinearity, in a convolutional way, as described in section 3.2, while keeping the rest of the architecture constant.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3450.75\n",
            "['Expected Output:\\nSentence: \"BN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\"\\nAccuracy: 74.8', 'Expected Output:\\nSentence: \"BN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\"\\nAccuracy: 74.8', 'Expected Output:\\nSentence: \"BN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\"\\nAccuracy: 74.8', 'Expected Output:\\nSentence: \"BN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\"\\nAccuracy: 74.8', 'Expected Output:\\nSentence: \"BN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\"\\nAccuracy: 74.8']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|██████▌                                                                                                                             | 5/100 [4:26:10<50:45:12, 1923.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4446.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[4, 5, 6] 3944.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|███████▉                                                                                                                            | 6/100 [4:30:36<37:14:07, 1426.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3124.25\n",
            "['Expected Output:\\nSentence: \"They both have 42% top 1 error rate over 1000 image classes.\"\\nAccuracy: 58.0', 'Expected Output:\\nSentence: \"They both have 42% top 1 error rate over 1000 image classes.\"\\nAccuracy: 58.0', 'Expected Output:\\nSentence: \"They both have 42% top 1 error rate over 1000 image classes.\"\\nAccuracy: 58.0', 'Expected Output:\\nSentence: \"They both have 42% top 1 error rate over 1000 image classes.\"\\nAccuracy: 58.0', 'Expected Output:\\nSentence: \"They both have 42% top 1 error rate over 1000 image classes.\"\\nAccuracy: 58.0']\n",
            "[4, 5, 6] 3245.5\n",
            "['Expected Output:\\nSentence: \"Figure 6.  Classification result of 4 images in group 4 validation set from ImageNet [7] dataset. Top five predicted labels are most probable ones. Correct label is under each image and the probability of correct label is in red. (a) hits top 1, (b) and (d) hit within top 5. (c) gets wrong label. The mouse in this image is too small.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Figure 6.  Classification result of 4 images in group 4 validation set from ImageNet [7] dataset. Top five predicted labels are most probable ones. Correct label is under each image and the probability of correct label is in red. (a) hits top 1, (b) and (d) hit within top 5. (c) gets wrong label. The mouse in this image is too small.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Figure 6.  Classification result of 4 images in group 4 validation set from ImageNet [7] dataset. Top five predicted labels are most probable ones. Correct label is under each image and the probability of correct label is in red. (a) hits top 1, (b) and (d) hit within top 5. (c) gets wrong label. The mouse in this image is too small.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Figure 6.  Classification result of 4 images in group 4 validation set from ImageNet [7] dataset. Top five predicted labels are most probable ones. Correct label is under each image and the probability of correct label is in red. (a) hits top 1, (b) and (d) hit within top 5. (c) gets wrong label. The mouse in this image is too small.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Figure 6.  Classification result of 4 images in group 4 validation set from ImageNet [7] dataset. Top five predicted labels are most probable ones. Correct label is under each image and the probability of correct label is in red. (a) hits top 1, (b) and (d) hit within top 5. (c) gets wrong label. The mouse in this image is too small.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|█████████▏                                                                                                                          | 7/100 [4:35:07<27:53:03, 1079.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3329.5\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 4043.25\n",
            "['Expected Output:\\nSentence: \"Note that even if the problem does not directly involve the identification of an object that appears in the network training task, e.g., the 1000 categories in the ImageNet subset, category-level pattern detection may be still beneficial.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Note that even if the problem does not directly involve the identification of an object that appears in the network training task, e.g., the 1000 categories in the ImageNet subset, category-level pattern detection may be still beneficial.\"\\nAccuracy: 404', '404', '404', '404']\n",
            "[7, 8, 9] 4310.5\n",
            "['Expected Output:\\nSentence: \"Object classiﬁcation: PASCAL-2007. PASCAL VOC 2007 contains 9963 images with 20 object categories. The task is to predict the presence of each object in each image. Note that most object categories in PASCAL-2007 are also included in ImageNet which is the training set of the Alex net and the VGGVD net. So ImageNet can be seen as a superset of PASCAL-2007. The results on this dataset are shown in Table 3. From Table 3, we can see that again the best performance is achieved by using cross-layer pooling and the VGGVD net. Not surprisingly, the AConv layer performs better than the OConv layer in this dataset because the training categories of the DCNN overlaps with PASCAL-2007 and the AConv layer contains this category-level information. The per-class performance of three best comparing methods, that is, CNN jitter with the VGGVD net, SCFV with the AConv layer from the VGGVD net and cross-layer pooling with the AConv layer from the VGGVD net, is shown in Table 4. As seen, the proposed cross-layer pooling achieves the best performance in most classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Object classiﬁcation: PASCAL-2007. PASCAL VOC 2007 contains 9963 images with 20 object categories. The task is to predict the presence of each object in each image. Note that most object categories in PASCAL-2007 are also included in ImageNet which is the training set of the Alex net and the VGGVD net. So ImageNet can be seen as a superset of PASCAL-2007. The results on this dataset are shown in Table 3. From Table 3, we can see that again the best performance is achieved by using cross-layer pooling and the VGGVD net. Not surprisingly, the AConv layer performs better than the OConv layer in this dataset because the training categories of the DCNN overlaps with PASCAL-2007 and the AConv layer contains this category-level information. The per-class performance of three best comparing methods, that is, CNN jitter with the VGGVD net, SCFV with the AConv layer from the VGGVD net and cross-layer pooling with the AConv layer from the VGGVD net, is shown in Table 4. As seen, the proposed cross-layer pooling achieves the best performance in most classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Object classiﬁcation: PASCAL-2007. PASCAL VOC 2007 contains 9963 images with 20 object categories. The task is to predict the presence of each object in each image. Note that most object categories in PASCAL-2007 are also included in ImageNet which is the training set of the Alex net and the VGGVD net. So ImageNet can be seen as a superset of PASCAL-2007. The results on this dataset are shown in Table 3. From Table 3, we can see that again the best performance is achieved by using cross-layer pooling and the VGGVD net. Not surprisingly, the AConv layer performs better than the OConv layer in this dataset because the training categories of the DCNN overlaps with PASCAL-2007 and the AConv layer contains this category-level information. The per-class performance of three best comparing methods, that is, CNN jitter with the VGGVD net, SCFV with the AConv layer from the VGGVD net and cross-layer pooling with the AConv layer from the VGGVD net, is shown in Table 4. As seen, the proposed cross-layer pooling achieves the best performance in most classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Object classiﬁcation: PASCAL-2007. PASCAL VOC 2007 contains 9963 images with 20 object categories. The task is to predict the presence of each object in each image. Note that most object categories in PASCAL-2007 are also included in ImageNet which is the training set of the Alex net and the VGGVD net. So ImageNet can be seen as a superset of PASCAL-2007. The results on this dataset are shown in Table 3. From Table 3, we can see that again the best performance is achieved by using cross-layer pooling and the VGGVD net. Not surprisingly, the AConv layer performs better than the OConv layer in this dataset because the training categories of the DCNN overlaps with PASCAL-2007 and the AConv layer contains this category-level information. The per-class performance of three best comparing methods, that is, CNN jitter with the VGGVD net, SCFV with the AConv layer from the VGGVD net and cross-layer pooling with the AConv layer from the VGGVD net, is shown in Table 4. As seen, the proposed cross-layer pooling achieves the best performance in most classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Object classiﬁcation: PASCAL-2007. PASCAL VOC 2007 contains 9963 images with 20 object categories. The task is to predict the presence of each object in each image. Note that most object categories in PASCAL-2007 are also included in ImageNet which is the training set of the Alex net and the VGGVD net. So ImageNet can be seen as a superset of PASCAL-2007. The results on this dataset are shown in Table 3. From Table 3, we can see that again the best performance is achieved by using cross-layer pooling and the VGGVD net. Not surprisingly, the AConv layer performs better than the OConv layer in this dataset because the training categories of the DCNN overlaps with PASCAL-2007 and the AConv layer contains this category-level information. The per-class performance of three best comparing methods, that is, CNN jitter with the VGGVD net, SCFV with the AConv layer from the VGGVD net and cross-layer pooling with the AConv layer from the VGGVD net, is shown in Table 4. As seen, the proposed cross-layer pooling achieves the best performance in most classes.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 3623.25\n",
            "['Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|██████████▋                                                                                                                          | 8/100 [4:44:06<23:26:33, 917.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2842.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[4, 5, 6] 516.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[7, 8, 9] 2779.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[10, 11, 12] 3103.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[13, 14, 15] 2899.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 2567.0\n",
            "['404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[19, 20, 21] 1858.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[22, 23, 24] 1948.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[25, 26, 27] 1150.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[28, 29, 30] 1307.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[31, 32, 33] 3281.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[34, 35, 36] 1367.0\n",
            "['Sentence: \"For the first experiment on attribute prediction, we converge after around 700 iterations with 18.97% top-one accuracy and 43.11% top-five accuracy.\"\\nAccuracy: 18.97', 'Sentence: \"For the first experiment on attribute prediction, we converge after around 700 iterations with 18.97% top-one accuracy and 43.11% top-five accuracy.\"\\nAccuracy: 18.97', 'Sentence: \"For the first experiment on attribute prediction, we converge after around 700 iterations with 18.97% top-one accuracy and 43.11% top-five accuracy.\"\\nAccuracy: 18.97', 'Sentence: \"For the first experiment on attribute prediction, we converge after around 700 iterations with 18.97% top-one accuracy and 43.11% top-five accuracy.\"\\nAccuracy: 18.97\\n\\nSentence: \"For the second experiment where we also predict the object class, we converge after around 400 iterations with 43.17% top-one accuracy and 71.97% top-five accuracy.\"\\nAccuracy: 43.17', 'Sentence: \"For the first experiment on attribute prediction, we converge after around 700 iterations with 18.97% top-one accuracy and 43.11% top-five accuracy.\"\\nAccuracy: 18.97']\n",
            "[37, 38, 39] 3067.75\n",
            "['Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1\\n\\nExpected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1\\n\\nExpected Output:\\nSentence: \"The evaluation results showed a top-1 accuracy of 78.3% on the test data on Imagenet.\"\\nAccuracy: 78.3\\n\\nExpected Output:\\nSentence: \"Our proposed model achieved a top-1 accuracy of 74.2% when evaluated on the Imagenet dataset.\"\\nAccuracy: 74.2\\n\\nExpected Output:\\nSentence: \"Our proposed model achieved a top-5 accuracy of 66.2% when evaluated on the Imagenet dataset.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: 404\\nAccuracy: 404']\n",
            "[40, 41, 42] 4436.0\n",
            "['Expected Output:\\nSentence: \"Krizhevsky et al., 2012. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky et al., 2012. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky et al., 2012. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky et al., 2012. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Krizhevsky et al., 2012. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|███████████▉                                                                                                                        | 9/100 [5:15:18<30:25:42, 1203.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3013.5\n",
            "['Expected Output:\\nSentence: \"In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classiﬁcation tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classiﬁcation tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classiﬁcation tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classiﬁcation tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classiﬁcation tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 2751.75\n",
            "['Expected Output:\\nSentence: \"For ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For ImageNet, we choose a fractal architecture to facilitate direct comparison with the 34-layer ResNet of He et al. (2016a).\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 2855.75\n",
            "['404', '404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█████████████▏                                                                                                                      | 10/100 [5:21:58<24:04:07, 962.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 1793.75\n",
            "['Expected Output:\\nSentence: \"When applied to big image datasets such as ImageNet [17], the classification accuracy of the single layer feature approach may suffer from the information loss during the feature extraction.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"When applied to big image datasets such as ImageNet [17], the classification accuracy of the single layer feature approach may suffer from the information loss during the feature extraction.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"When applied to big image datasets such as ImageNet [17], the classification accuracy of the single layer feature approach may suffer from the information loss during the feature extraction.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 1659.25\n",
            "['Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1']\n",
            "[7, 8, 9] 1509.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404']\n",
            "[10, 11, 12] 1276.0\n",
            "['Expected Output:\\nSentence: \"In addition, Figure 5 shows that when the model depth is the same, the whitening operation can help DDRL model achieve higher classification accuracy, both on ImageNet and CIFAR-100 dataset.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"From the experimental results, both on ImageNet and CIFAR-100 dataset, we observe that the performance of our model gets improved when the layer number increases, and this increase takes place no matter whether or not the whitening operation is included.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In addition, Figure 5 shows that when the model depth is the same, the whitening operation can help DDRL model achieve higher classification accuracy, both on ImageNet and CIFAR-100 dataset.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In addition, Figure 5 shows that when the model depth is the same, the whitening operation can help DDRL model achieve higher classification accuracy, both on ImageNet and CIFAR-100 dataset.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"From the experimental results, both on ImageNet and CIFAR-100 dataset, we observe that the performance of our model gets improved when the layer number increases, and this increase takes place no matter whether or not the whitening operation is included.\"\\nAccuracy: 404']\n",
            "[13, 14, 15] 1277.25\n",
            "['Sentence: \"Table 1: Comparison of the classiﬁcation performance on ImageNet dataset. layer 1 2 3 4 5 DDRL 70.19% 72.58% 74.86% 75.14% 75.53% [15] 70.01% N/A N/A N/A N/A\"\\n\\nAccuracy: 75.53', 'Sentence: \"Table 1: Comparison of the classiﬁcation performance on ImageNet dataset. layer 1 2 3 4 5 DDRL 70.19% 72.58% 74.86% 75.14% 75.53% [15] 70.01% N/A N/A N/A N/A\"\\n\\nAccuracy: 75.53', 'Sentence: \"Table 1: Comparison of the classiﬁcation performance on ImageNet dataset.\"\\nAccuracy: 75.53', 'Sentence: \"Table 1: Comparison of the classiﬁcation performance on ImageNet dataset. layer 1 2 3 4 5 DDRL 70.19% 72.58% 74.86% 75.14% 75.53% [15] 70.01% N/A N/A N/A N/A\"\\n\\nAccuracy: 75.53', 'Sentence: \"Table 1: Comparison of the classiﬁcation performance on ImageNet dataset. layer 1 2 3 4 5 DDRL 70.19% 72.58% 74.86% 75.14% 75.53% [15] 70.01% N/A N/A N/A N/A\"\\n\\nAccuracy: 75.53']\n",
            "[16, 17, 18] 1361.25\n",
            "['Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|██████████████▌                                                                                                                     | 11/100 [5:35:23<22:37:43, 915.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3553.75\n",
            "['404', '404', 'Expected Output:\\nSentence: \"Our models tend to require much fewer parameters than existing algorithms with comparable accuracy. Further, we significantly outperform the current state-of-the-art results on most of the benchmark tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance.\"\\nAccuracy: 404', '404']\n",
            "[4, 5, 6] 3472.25\n",
            "['Expected Output:\\nSentence: \"Model top-1 top-5 DenseNet-121 25.02 / 23.61 7.71 / 6.66 DenseNet-169 23.80 / 22.08 6.85 / 5.92 DenseNet-201 22.58 / 21.46 6.34 / 5.54 DenseNet-264 22.15 / 20.80 6.12 / 5.29 Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop / 10-crop testing.\"\\nAccuracy: 25.02', 'Expected Output:\\nSentence: \"Model top-1 top-5 DenseNet-121 25.02 / 23.61 7.71 / 6.66 DenseNet-169 23.80 / 22.08 6.85 / 5.92 DenseNet-201 22.58 / 21.46 6.34 / 5.54 DenseNet-264 22.15 / 20.80 6.12 / 5.29 Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop / 10-crop testing.\"\\nAccuracy: 25.02', 'Expected Output:\\nSentence: \"Model top-1 top-5 DenseNet-121 25.02 / 23.61 7.71 / 6.66 DenseNet-169 23.80 / 22.08 6.85 / 5.92 DenseNet-201 22.58 / 21.46 6.34 / 5.54 DenseNet-264 22.15 / 20.80 6.12 / 5.29 Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop / 10-crop testing.\"\\nAccuracy: 25.02', 'Expected Output:\\nSentence: \"Model top-1 top-5 DenseNet-121 25.02 / 23.61 7.71 / 6.66 DenseNet-169 23.80 / 22.08 6.85 / 5.92 DenseNet-201 22.58 / 21.46 6.34 / 5.54 DenseNet-264 22.15 / 20.80 6.12 / 5.29 Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop / 10-crop testing.\"\\nAccuracy: 25.02', 'Expected Output:\\nSentence: \"Model top-1 top-5 DenseNet-121 25.02 / 23.61 7.71 / 6.66 DenseNet-169 23.80 / 22.08 6.85 / 5.92 DenseNet-201 22.58 / 21.46 6.34 / 5.54 DenseNet-264 22.15 / 20.80 6.12 / 5.29 Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop / 10-crop testing.\"\\nAccuracy: 25.02']\n",
            "[7, 8, 9] 4024.0\n",
            "['Expected Output:\\nSentence: \"The results presented in the ﬁgure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring signiﬁcantly fewer parameters and computation to achieve comparable performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The results presented in the ﬁgure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring signiﬁcantly fewer parameters and computation to achieve comparable performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The results presented in the ﬁgure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring signiﬁcantly fewer parameters and computation to achieve comparable performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The results presented in the ﬁgure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring signiﬁcantly fewer parameters and computation to achieve comparable performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The results presented in the ﬁgure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring signiﬁcantly fewer parameters and computation to achieve comparable performance.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|███████████████▊                                                                                                                    | 12/100 [5:42:08<18:37:58, 762.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3209.5\n",
            "['Expected Output:\\nSentence: \"We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 2027.75\n",
            "['Expected Output:\\nSentence: \"On ImageNet, Xception shows marginally better results than Inception V3.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On ImageNet, Xception shows marginally better results than Inception V3.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On ImageNet, Xception shows marginally better results than Inception V3.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 1. Classiﬁcation performance comparison on ImageNet (single crop, single model). VGG-16 and ResNet-152 numbers are only included as a reminder. The version of Inception V3 being benchmarked does not include the auxiliary tower. Top-1 accuracy Top-5 accuracy VGG-16 0.715 0.901 ResNet-152 0.770 0.933 Inception V3 0.782 0.941 Xception 0.790 0.945\"\\nAccuracy: 79.0', 'Expected Output:\\nSentence: \"On ImageNet, Xception shows marginally better results than Inception V3.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On ImageNet, Xception shows marginally better results than Inception V3.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█████████████████▏                                                                                                                  | 13/100 [5:46:37<14:50:26, 614.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3614.25\n",
            "['Expected Output:\\nSentence: \"Recently introduced deep residual networks (ResNets) [24] are over 200-layers deep and have shown state-of-the-art performance in several challenging tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Recently introduced deep residual networks (ResNets) [24] are over 200-layers deep and have shown state-of-the-art performance in several challenging tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Recently introduced deep residual networks (ResNets) [24] are over 200-layers deep and have shown state-of-the-art performance in several challenging tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Recently introduced deep residual networks (ResNets) [24] are over 200-layers deep and have shown state-of-the-art performance in several challenging tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Recently introduced deep residual networks (ResNets) [24] are over 200-layers deep and have shown state-of-the-art performance in several challenging tasks.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3280.75\n",
            "['404', '404', 'Expected Output:\\nSentence: \"Grad-CAM for VGG-16 also achieves better top-1 localization error than CAM [59], which requires a change in the model architecture, necessitates re-training and thereby achieves worse classiﬁcation errors (2.98% worse top-1), while Grad-CAM does not compromise on classiﬁcation performance.\"\\nAccuracy: 404', '404', '404']\n",
            "[7, 8, 9] 3449.25\n",
            "['Expected Output:\\nSentence: \"In this section, we further demonstrate the use of Grad-CAM in analyzing failure modes of image classification CNNs, understanding the effect of adversarial noise, and identifying and removing biases in datasets, in the context of VGG-16 pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In this section, we further demonstrate the use of Grad-CAM in analyzing failure modes of image classification CNNs, understanding the effect of adversarial noise, and identifying and removing biases in datasets, in the context of VGG-16 pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In this section, we further demonstrate the use of Grad-CAM in analyzing failure modes of image classification CNNs, understanding the effect of adversarial noise, and identifying and removing biases in datasets, in the context of VGG-16 pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In this section, we further demonstrate the use of Grad-CAM in analyzing failure modes of image classification CNNs, understanding the effect of adversarial noise, and identifying and removing biases in datasets, in the context of VGG-16 pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In this section, we further demonstrate the use of Grad-CAM in analyzing failure modes of image classification CNNs, understanding the effect of adversarial noise, and identifying and removing biases in datasets, in the context of VGG-16 pretrained on ImageNet.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 2897.5\n",
            "['Expected Output:\\nSentence: \"The re-trained model not only generalizes better (90% test accuracy), but also looks at the right regions (last column of Fig. 8).\"\\nAccuracy: 90.0', 'Expected Output:\\nSentence: \"The re-trained model not only generalizes better (90% test accuracy), but also looks at the right regions (last column of Fig. 8).\"\\nAccuracy: 90.0', '404', 'Expected Output:\\nSentence: \"The re-trained model not only generalizes better (90% test accuracy), but also looks at the right regions (last column of Fig. 8).\"\\nAccuracy: 90.0', 'Expected Output:\\nSentence: \"The re-trained model not only generalizes better (90% test accuracy), but also looks at the right regions (last column of Fig. 8).\"\\nAccuracy: 90.0']\n",
            "[13, 14, 15] 2575.75\n",
            "['Expected Output:\\nSentence: \"The results reported in Fig. 17 correspond to the VGG-16 [52] network trained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The results reported in Fig. 17 correspond to the VGG-16 [52] network trained on ImageNet.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"The results reported in Fig. 17 correspond to the VGG-16 [52] network trained on ImageNet.\"\\nAccuracy: 404', '404']\n",
            "[16, 17, 18] 753.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[19, 20, 21] 829.75\n",
            "['Expected Output:\\nSentence: \"As mentioned in Section 4.2 of the main paper, we find that our approach Grad-CAM outperforms c-MWP by a significant margin (70.58% vs 60.30% on VGG-16).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"As mentioned in Section 4.2 of the main paper, we find that our approach Grad-CAM outperforms c-MWP by a significant margin (70.58% vs 60.30% on VGG-16).\"\\nAccuracy: 70.58', 'Expected Output:\\nSentence: \"As mentioned in Section 4.2 of the main paper, we find that our approach Grad-CAM outperforms c-MWP by a significant margin (70.58% vs 60.30% on VGG-16).\"\\nAccuracy: 70.58', 'Expected Output:\\nSentence: \"As mentioned in Section 4.2 of the main paper, we find that our approach Grad-CAM outperforms c-MWP by a significant margin (70.58% vs 60.30% on VGG-16).\"\\nAccuracy: 70.58', 'Expected Output:\\nSentence: \"As mentioned in Section 4.2 of the main paper, we find that our approach Grad-CAM outperforms c-MWP by a significant margin (70.58% vs 60.30% on VGG-16).\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|██████████████████▍                                                                                                                 | 14/100 [6:02:12<16:58:23, 710.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4307.75\n",
            "['Expected Output:\\nSentence: \"The proposed deep Re-ID network architecture in this work is designed specifically for transferring generalisable feature representations learned from ImageNet to Re-ID datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed deep Re-ID network architecture in this work is designed specifically for transferring generalisable feature representations learned from ImageNet to Re-ID datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed deep Re-ID network architecture in this work is designed specifically for transferring generalisable feature representations learned from ImageNet to Re-ID datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed deep Re-ID network architecture in this work is designed specifically for transferring generalisable feature representations learned from ImageNet to Re-ID datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed deep Re-ID network architecture in this work is designed specifically for transferring generalisable feature representations learned from ImageNet to Re-ID datasets.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 4041.0\n",
            "['Expected Output:\\nSentence: \"Among the recently proposed networks that achieved good classification performance on ImageNet, GoogLeNet is chosen over VGG net [44] due to the fact that it has much smaller parameter size2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Among the recently proposed networks that achieved good classification performance on ImageNet, GoogLeNet is chosen over VGG net [44] due to the fact that it has much smaller parameter size2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Among the recently proposed networks that achieved good classification performance on ImageNet, GoogLeNet is chosen over VGG net [44] due to the fact that it has much smaller parameter size2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Among the recently proposed networks that achieved good classification performance on ImageNet, GoogLeNet is chosen over VGG net [44] due to the fact that it has much smaller parameter size2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Among the recently proposed networks that achieved good classification performance on ImageNet, GoogLeNet is chosen over VGG net [44] due to the fact that it has much smaller parameter size2.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3844.75\n",
            "['Expected Output:\\nSentence: \"However, with our model, the gap is clear now. The main reason, as we explained earlier, is that our model is able to transfer feature representations learned from ImageNet thanks to the selected base network (GoogLeNet) and the training objectives (classiﬁcation + veriﬁcation loss). In contrast, none of the compared models transfer knowledge from other auxiliary sources – we found that they cannot even if they are pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, with our model, the gap is clear now. The main reason, as we explained earlier, is that our model is able to transfer feature representations learned from ImageNet thanks to the selected base network (GoogLeNet) and the training objectives (classiﬁcation + veriﬁcation loss). In contrast, none of the compared models transfer knowledge from other auxiliary sources – we found that they cannot even if they are pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, with our model, the gap is clear now. The main reason, as we explained earlier, is that our model is able to transfer feature representations learned from ImageNet thanks to the selected base network (GoogLeNet) and the training objectives (classiﬁcation + veriﬁcation loss). In contrast, none of the compared models transfer knowledge from other auxiliary sources – we found that they cannot even if they are pretrained on ImageNet.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, with the SID+PV combination, the results with DGDNet are much improved, but transfer learning from Imagenet now has a negative effect.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, with the SID+PV combination, the results with DGDNet are much improved, but transfer learning from Imagenet now has a negative effect.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 3536.25\n",
            "['Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', '404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|███████████████████▊                                                                                                                | 15/100 [6:11:11<15:33:29, 658.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3601.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3284.25\n",
            "['Expected Output:\\nSentence: \"Comparing with ResNet-50 (Table 3 top and Fig. 5 left), the 32×4d ResNeXt-50 has a validation error of 22.2%, which is 1.7% lower than the ResNet baseline’s 23.9%.\"\\nAccuracy: 22.2', 'Expected Output:\\nSentence: \"Comparing with ResNet-50 (Table 3 top and Fig. 5 left), the 32×4d ResNeXt-50 has a validation error of 22.2%, which is 1.7% lower than the ResNet baseline’s 23.9%.\"\\nAccuracy: 22.2', '404', 'Expected Output:\\nSentence: \"Comparing with ResNet-50 (Table 3 top and Fig. 5 left), the 32×4d ResNeXt-50 has a validation error of 22.2%, which is 1.7% lower than the ResNet baseline’s 23.9%.\"\\nAccuracy: 22.2', '404']\n",
            "[7, 8, 9] 3411.75\n",
            "['Expected Output:\\nSentence: \"We had a single-model top-1/top-5 error rates of 17.7%/3.7% using the multi-scale dense testing in [14], on par with Inception-ResNet-v2’s single-model results of 17.8%/3.7% that adopts multi-scale, multi-crop testing.\"\\nAccuracy: 82.3', 'Expected Output:\\nSentence: \"We had a single-model top-1/top-5 error rates of 17.7%/3.7% using the multi-scale dense testing in [14], on par with Inception-ResNet-v2’s single-model results of 17.8%/3.7% that adopts multi-scale, multi-crop testing.\"\\nAccuracy: 82.3', 'Expected Output:\\nSentence: \"We had a single-model top-1/top-5 error rates of 17.7%/3.7% using the multi-scale dense testing in [14], on par with Inception-ResNet-v2’s single-model results of 17.8%/3.7% that adopts multi-scale, multi-crop testing.\"\\nAccuracy: 82.3', 'Expected Output:\\nSentence: \"We had a single-model top-1/top-5 error rates of 17.7%/3.7% using the multi-scale dense testing in [14], on par with Inception-ResNet-v2’s single-model results of 17.8%/3.7% that adopts multi-scale, multi-crop testing.\"\\nAccuracy: 82.3', 'Expected Output:\\nSentence: \"We had a single-model top-1/top-5 error rates of 17.7%/3.7% using the multi-scale dense testing in [14], on par with Inception-ResNet-v2’s single-model results of 17.8%/3.7% that adopts multi-scale, multi-crop testing.\"\\nAccuracy: 82.3']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█████████████████████                                                                                                               | 16/100 [6:17:52<13:34:23, 581.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3504.25\n",
            "['Expected Output:\\nSentence: \"On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 2980.75\n",
            "['Expected Output:\\nSentence: \"R-MG-34 32.9 5.76 22.42 6.12 24.51 7.46 R-PMG-30-SEG 31.9 2.77 23.60 6.89 26.50 8.63 Table 4. ImageNet classiﬁcation error (%) on validation set.\"\\nAccuracy: 77.58', 'Expected Output:\\nSentence: \"R-MG-34 32.9 5.76 22.42 6.12 24.51 7.46 R-PMG-30-SEG 31.9 2.77 23.60 6.89 26.50 8.63 Table 4. ImageNet classiﬁcation error (%) on validation set.\"\\nAccuracy: 77.58', 'Expected Output:\\nSentence: \"R-MG-34 achieved a top-1 accuracy of 24.51% on the ImageNet validation set with single-crop evaluation.\"\\nAccuracy: 24.51', 'Expected Output:\\nSentence: \"R-MG-34 achieved a top-1 accuracy of 24.51% on the ImageNet validation set.\"\\nAccuracy: 24.51', 'Expected Output:\\nSentence: \"R-MG-34 32.9 5.76 22.42 6.12 24.51 7.46 R-PMG-30-SEG 31.9 2.77 23.60 6.89 26.50 8.63 Table 4. ImageNet classiﬁcation error (%) on validation set.\"\\nAccuracy: 77.58']\n",
            "[7, 8, 9] 1913.75\n",
            "['Expected Output:\\nSentence: \"Table 4 compares our performance to that of ResNets and wide residual networks (WRN) [38].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Table 4 compares our performance to that of ResNets and wide residual networks (WRN) [38]. We observe: • Multigrid substantially improves performance. R-MG-34 even outperforms the deeper ResNet-50.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Table 4 compares our performance to that of ResNets and wide residual networks (WRN) [38]. We observe: • Multigrid substantially improves performance. R-MG-34 even outperforms the deeper ResNet-50.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Table 4 compares our performance to that of ResNets and wide residual networks (WRN) [38]. We observe: • Multigrid substantially improves performance. R-MG-34 even outperforms the deeper ResNet-50.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Table 4 compares our performance to that of ResNets and wide residual networks (WRN) [38].\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|██████████████████████▍                                                                                                             | 17/100 [6:24:34<12:10:06, 527.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4026.25\n",
            "['404', '404', '404', '404', 'Expected Output:\\nSentence: \"On the ImageNet dataset, GoCNN improves the performance of state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses privileged information of 10% of the training images, confirming effectiveness of GoCNN on utilizing available privileged knowledge to train better CNNs.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 4016.5\n",
            "['Expected Output:\\nSentence: \"The GoCNN achieves 21.8% top-1 error while the vanilla ResNet-152 has 23.0% top-1 error.\"\\nAccuracy: 78.2', 'Expected Output:\\nSentence: \"The GoCNN achieves 21.8% top-1 error while the vanilla ResNet-152 has 23.0% top-1 error.\"\\nAccuracy: 78.2', 'Expected Output:\\nSentence: \"The GoCNN achieves 21.8% top-1 error while the vanilla ResNet-152 has 23.0% top-1 error.\"\\nAccuracy: 78.2', 'Expected Output:\\nSentence: \"The GoCNN achieves 21.8% top-1 error while the vanilla ResNet-152 has 23.0% top-1 error.\"\\nAccuracy: 78.2', 'Expected Output:\\nSentence: \"The GoCNN achieves 21.8% top-1 error while the vanilla ResNet-152 has 23.0% top-1 error.\"\\nAccuracy: 78.2']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|███████████████████████▊                                                                                                            | 18/100 [6:29:02<10:14:34, 449.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2941.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 4020.0\n",
            "['404', 'Expected Output:\\nSentence: \"In a first series of experiments we demonstrate the advantages of the proposed part-based model and compare it with (i) its non end-to-end counterpart (i.e, the CNN-NBNL method in [13]) and (ii) traditional CNN-based approaches not accounting for local representations. To implement [13] following the original paper, we split the input image into multiple patches, extracting features from the last fully-connected layer of a pre-trained CNN. The patches were extracted at three different scales (32,64,128 pixels) after the original image was rescaled (longest side 200 pixels). We adopted the sparse protocol in [13], based on which features from 100 random patches are extracted. The features are equally distributed between the three scales and an additional descriptor representing the full image is considered. As representative for deep models based on holistic representations, we chose the successful approach of Zhou et al. [27], [28]: they pre-train a CNN on huge datasets (i.e., ImageNet [29], Places [27], [28] or both in the hybrid configuration) and used it as features extractor for learning a linear SVM model. Note that this is a strong baseline, widely used in the computer vision community for scene recognition tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In a first series of experiments we demonstrate the advantages of the proposed part-based model and compare it with (i) its non end-to-end counterpart (i.e, the CNN-NBNL method in [13]) and (ii) traditional CNN-based approaches not accounting for local representations. To implement [13] following the original paper, we split the input image into multiple patches, extracting features from the last fully-connected layer of a pre-trained CNN. The patches were extracted at three different scales (32,64,128 pixels) after the original image was rescaled (longest side 200 pixels). We adopted the sparse protocol in [13], based on which features from 100 random patches are extracted. The features are equally distributed between the three scales and an additional descriptor representing the full image is considered. As representative for deep models based on holistic representations, we chose the successful approach of Zhou et al. [27], [28]: they pre-train a CNN on huge datasets (i.e., ImageNet [29], Places [27], [28] or both in the hybrid configuration) and used it as features extractor for learning a linear SVM model. Note that this is a strong baseline, widely used in the computer vision community for scene recognition tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In a first series of experiments we demonstrate the advantages of the proposed part-based model and compare it with (i) its non end-to-end counterpart (i.e, the CNN-NBNL method in [13]) and (ii) traditional CNN-based approaches not accounting for local representations. To implement [13] following the original paper, we split the input image into multiple patches, extracting features from the last fully-connected layer of a pre-trained CNN. The patches were extracted at three different scales (32,64,128 pixels) after the original image was rescaled (longest side 200 pixels). We adopted the sparse protocol in [13], based on which features from 100 random patches are extracted. The features are equally distributed between the three scales and an additional descriptor representing the full image is considered. As representative for deep models based on holistic representations, we chose the successful approach of Zhou et al. [27], [28]: they pre-train a CNN on huge datasets (i.e., ImageNet [29], Places [27], [28] or both in the hybrid configuration) and used it as features extractor for learning a linear SVM model. Note that this is a strong baseline, widely used in the computer vision community for scene recognition tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In a first series of experiments we demonstrate the advantages of the proposed part-based model and compare it with (i) its non end-to-end counterpart (i.e, the CNN-NBNL method in [13]) and (ii) traditional CNN-based approaches not accounting for local representations. To implement [13] following the original paper, we split the input image into multiple patches, extracting features from the last fully-connected layer of a pre-trained CNN. The patches were extracted at three different scales (32,64,128 pixels) after the original image was rescaled (longest side 200 pixels). We adopted the sparse protocol in [13], based on which features from 100 random patches are extracted. The features are equally distributed between the three scales and an additional descriptor representing the full image is considered. As representative for deep models based on holistic representations, we chose the successful approach of Zhou et al. [27], [28]: they pre-train a CNN on huge datasets (i.e., ImageNet [29], Places [27], [28] or both in the hybrid configuration) and used it as features extractor for learning a linear SVM model. Note that this is a strong baseline, widely used in the computer vision community for scene recognition tasks.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 4654.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█████████████████████████▎                                                                                                           | 19/100 [6:35:50<9:50:11, 437.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3661.5\n",
            "['404', 'Expected Output:\\nSentence: \"We also apply DCL to some state-of-the-art network structures, and achieve high accuracy on some generic recognition tasks, including SVHN, CIFAR and ILSVRC2012.\"\\nAccuracy: 404', '404', '404', '404']\n",
            "[4, 5, 6] 3550.25\n",
            "['Expected Output:\\nSentence: \"In our experiments, M (1) and M (2) are always large (e.g., tens or hundreds), thus using DCL leads to a less complicated model and, consequently, less risk of over-ﬁtting.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1']\n",
            "[7, 8, 9] 3517.75\n",
            "['Expected Output:\\nSentence: \"With DCL, the top-1 and top-5 recognition error rates are 42.98% and 19.69%, respectively.\"\\nAccuracy: 57.02', 'Expected Output:\\nSentence: \"With DCL, the top-1 and top-5 recognition error rates are 42.98% and 19.69%, respectively.\"\\nAccuracy: 57.02', 'Expected Output:\\nSentence: \"With DCL, the top-1 and top-5 recognition error rates are 42.98% and 19.69%, respectively.\"\\nAccuracy: 57.02', 'Expected Output:\\nSentence: \"With DCL, the top-1 and top-5 recognition error rates are 42.98% and 19.69%, respectively.\"\\nAccuracy: 57.02', 'Expected Output:\\nSentence: \"With DCL, the top-1 and top-5 recognition error rates are 42.98% and 19.69%, respectively.\"\\nAccuracy: 57.02']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██████████████████████████▌                                                                                                          | 20/100 [6:42:31<9:28:31, 426.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3060.0\n",
            "['404', 'Expected Output:\\nSentence: \"The CNN-F architecture is similar to the one used by [Krizhevsky et al., 2012]. On the other hand, the CNN-M architecture is similar to the one employed by [Zeiler and Fergus, 2014], whereas the CNN-S architecture is related to the ’accurate’ network from the OverFeat package [Sermanet et al., 2013]. All these baseline CNN architectures are built on the Caffe framework [Jia et al., 2014] and are pre-trained on ImageNet [Deng et al., 2009]. Each network comprises 5 convolutional and 3 fully connected layers for a total of 8 learnable layers. For further design and implementation details for these architectures, please see Table 1 in [Chatﬁeld et al., 2014]. Please note that the results of the penultimate layer (layer 7) are used for the SVM classiﬁer in this particular work. Each test yields a feature vector of 4096 dimensions per image. The CNN-M is also tested in situations when the feature dimensionality is reduced to 2048, 1024, and 128, and in cases where the images are turned into grey scales.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"The CNN-F architecture is similar to the one used by [Krizhevsky et al., 2012].\"\\nAccuracy: 404', '404']\n",
            "[4, 5, 6] 2768.25\n",
            "['Expected Output:\\nSentence: \"Finally, the creators of the ImageNet7 [Hu et al., 2011] reported 60% recognition accuracy on their database by combining all 5 available kernel descriptors.\"\\nAccuracy: 60.0', '404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|███████████████████████████▉                                                                                                         | 21/100 [6:46:57<8:18:19, 378.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4558.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|█████████████████████████████▎                                                                                                       | 22/100 [6:49:11<6:36:30, 305.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2248.0\n",
            "['Expected Output:\\nSentence: \"In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86% reduction in training time and an 88.4% reduction in size.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86% reduction in training time and an 88.4% reduction in size.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86% reduction in training time and an 88.4% reduction in size.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86% reduction in training time and an 88.4% reduction in size.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86% reduction in training time and an 88.4% reduction in size.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 2032.5\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 2267.25\n",
            "['Expected Output:\\nSentence: \"Find the results of the experiment described above in Table 2 which provides Top-1 and Top-5 accuracy in validation classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To conduct training, we utilize Berkeley Vision and Learning Center’s open source deep learning framework, Caffe [40]. We pair Caffe and an open source deep learning GPU training system, NVIDIA DIGITS [31] allowing users to build and examine artificial neural networks with real-time graphical representations. Our physical hardware consists of four NVIDIA GeForce GTX TITAN X GPUs and two Intel Xeon processors providing us with 48/24 logical/physical cores and a 256GB hard disk. All images in the training and validation datasets are resized to (256 x 256). For preprocessing we perform a subtraction of the average pixel from each color channel of RGB color space. Batch size for training is 128 and validation is 64. This is compared to VGG16 [26] where batch size for training is 256 while validation is 128. The epoch attribute is set to 50 and learning rate to 0.01. Every 10 epochs, the learning rate will degrade 5x and learning average decay will resolve to ½ of the previous value. In VGG16 [26] the epoch attribute is set to 20 and learning rate to 0.001. After the completion of every 4 epochs, the learning rate will degrade 5x and learning average decay will resolve to ½ of the previous value. A randomized crop of size (227 x 227) is applied before introduction to the first convolutional layer. We adapt the weights of all layers from the Xavier distribution with a standard deviation of 0.01 as opposed to VGG16 [26] which used a Gaussian distribution with a 0.01 standard deviation for the weights of each layer. The final convolutional layer of: ResSquVGG16 serves as an output layer with a weight adapted from the Gaussian distribution with a 0.01 standard deviation. Reflection is the only process of augmentation that is performed. During training the model converged after two days and nineteen hours with a size of 1.23gb. By comparison, the original VGG16 [26] converged after three days and sixteen hours with a size of 10.6gb. From this we see that the: ResSquVGG16 model sees a 23.86% speed improvement paired with an 88.40% size reduction from VGG16 [26]. We can see from above that our ResSquVGG16 model surpasses the Simonyan and Zisserman VGG16 [26] in, training from scratch while the VGG16 [25] cannot trained from scratch even in the original paper, ResSquVGG16 model has very less training time if we taking into consideration the batch size (for training is 128 and validation is 64) while VGG16 [26] used (for training is 256 and validation is 128); furthermore, our ResSquVGG16 model trained with 50 epochs while the VGG16 [26] trained with 20 epochs. Find the results of the experiment described above in Table 2 which provides Top-1 and Top-5 accuracy in validation classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Find the results of the experiment described above in Table 2 which provides Top-1 and Top-5 accuracy in validation classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Find the results of the experiment described above in Table 2 which provides Top-1 and Top-5 accuracy in validation classification.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Find the results of the experiment described above in Table 2 which provides Top-1 and Top-5 accuracy in validation classification.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██████████████████████████████▌                                                                                                      | 23/100 [6:55:55<7:09:22, 334.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3585.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3779.75\n",
            "['Expected Output:\\nSentence: \"Initialization Top-1 Acc. Top-5 Acc. MSRA checkpoint [16] 76.4 92.9 Random initialization 77.5 93.9 Fine-tune from JFT-300M 79.2 94.7 Table 1. Top-1 and top-5 classiﬁcation accuracy on the ImageNet ‘val’ set (single model and single crop inference are used).\"\\nAccuracy: 79.2', 'Expected Output:\\nSentence: \"Initialization Top-1 Acc. Top-5 Acc. MSRA checkpoint [16] 76.4 92.9 Random initialization 77.5 93.9 Fine-tune from JFT-300M 79.2 94.7 Table 1. Top-1 and top-5 classiﬁcation accuracy on the ImageNet ‘val’ set (single model and single crop inference are used).\"\\nAccuracy: 79.2', 'Expected Output:\\nSentence: \"Initialization Top-1 Acc. Top-5 Acc. MSRA checkpoint [16] 76.4 92.9 Random initialization 77.5 93.9 Fine-tune from JFT-300M 79.2 94.7 Table 1. Top-1 and top-5 classiﬁcation accuracy on the ImageNet ‘val’ set (single model and single crop inference are used).\"\\nAccuracy: 79.2', 'Expected Output:\\nSentence: \"Initialization Top-1 Acc. Top-5 Acc. MSRA checkpoint [16] 76.4 92.9 Random initialization 77.5 93.9 Fine-tune from JFT-300M 79.2 94.7 Table 1. Top-1 and top-5 classiﬁcation accuracy on the ImageNet ‘val’ set (single model and single crop inference are used).\"\\nAccuracy: 79.2', 'Expected Output:\\nSentence: \"Initialization Top-1 Acc. Top-5 Acc. MSRA checkpoint [16] 76.4 92.9 Random initialization 77.5 93.9 Fine-tune from JFT-300M 79.2 94.7 Table 1. Top-1 and top-5 classiﬁcation accuracy on the ImageNet ‘val’ set (single model and single crop inference are used).\"\\nAccuracy: 79.2']\n",
            "[7, 8, 9] 3763.0\n",
            "['Expected Output:\\nSentence: \"mean ImageNet 79.7 80.6 77.1 65.9 64.2 85.3 81.0 88.4 60.5 83.1 70.8 86.7 86.2 79.7 79.5 49.5 78.3 80.2 79.2 69.7 76.3 300M 87.2 88.8 79.6 75.2 67.9 88.2 89.3 88.6 64.3 86.1 73.6 88.7 89.1 86.5 86.4 57.7 84.2 82.1 86.7 78.6 81.4 ImageNet+300M 86.9 88.0 80.1 74.7 68.8 88.9 89.6 88.0 69.7 86.9 71.9 88.5 89.6 86.9 86.8 53.7 78.2 82.3 87.7 77.9 81.3\"\\nAccuracy: 79.7', 'Expected Output:\\nSentence: \"Comparison with ImageNet Models. We present quantitative comparison of JFT-300M checkpoints with ImageNet checkpoints in Figure 6 (left). We see that the JFT-300M checkpoint outperforms ImageNet by 1.7% points. We further observe that the JFT-300M model trained from the ImageNet checkpoint provides 2.9% points boost over the vanilla ImageNet checkpoint.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Comparison with ImageNet Models. We present quantitative comparison of JFT-300M checkpoints with ImageNet checkpoints in Figure 6 (left). We see that the JFT-300M checkpoint outperforms ImageNet by 1.7% points. We further observe that the JFT-300M model trained from the ImageNet checkpoint provides 2.9% points boost over the vanilla ImageNet checkpoint.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Comparison with ImageNet Models. We present quantitative comparison of JFT-300M checkpoints with ImageNet checkpoints in Figure 6 (left). We see that the JFT-300M checkpoint outperforms ImageNet by 1.7% points. We further observe that the JFT-300M model trained from the ImageNet checkpoint provides 2.9% points boost over the vanilla ImageNet checkpoint.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Comparison with ImageNet Models. We present quantitative comparison of JFT-300M checkpoints with ImageNet checkpoints in Figure 6 (left). We see that the JFT-300M checkpoint outperforms ImageNet by 1.7% points. We further observe that the JFT-300M model trained from the ImageNet checkpoint provides 2.9% points boost over the vanilla ImageNet checkpoint.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 1189.0\n",
            "['Expected Output:\\nSentence: \"Table 8. Top-1 and top-5 classiﬁcation accuracy on ImageNet validation set, before and after de-duplication. Single model and single crop are used.\"\\nAccuracy: 79.3', 'Expected Output:\\nSentence: \"Table 8. Top-1 and top-5 classiﬁcation accuracy on ImageNet validation set, before and after de-duplication. Single model and single crop are used.\"\\nAccuracy: 79.3', 'Expected Output:\\nSentence: \"Table 8. Top-1 and top-5 classiﬁcation accuracy on ImageNet validation set, before and after de-duplication. Single model and single crop are used.\"\\nAccuracy: 79.3', 'Expected Output:\\nSentence: \"Fine-tune from JFT-300M 79.2 94.7 79.3 94.7 Table 8. Top-1 and top-5 classiﬁcation accuracy on ImageNet validation set, before and after de-duplication.\"\\nAccuracy: 79.2', 'Expected Output:\\nSentence: \"Table 8. Top-1 and top-5 classiﬁcation accuracy on ImageNet validation set, before and after de-duplication. Single model and single crop are used.\"\\nAccuracy: 79.3']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|███████████████████████████████▉                                                                                                     | 24/100 [7:04:56<8:22:18, 396.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3607.5\n",
            "['Expected Output:\\nSentence: \"Our model achieves a top-1 accuracy of 75.3% on the ImageNet dataset, outperforming previous methods.\"\\nAccuracy: 75.3', 'Expected Output:\\nSentence: \"Our model achieves a top-1 accuracy of 75.3% on the ImageNet dataset.\"\\nAccuracy: 75.3', 'Expected Output:\\nSentence: \"Our model achieves a top-1 accuracy of 75.3% on the ImageNet dataset.\"\\nAccuracy: 75.3', 'Expected Output:\\nSentence: \"Our model achieves a top-1 accuracy of 75.3% on the ImageNet dataset.\"\\nAccuracy: 75.3', 'Expected Output:\\nSentence: \"Our model achieves a top-1 accuracy of 75.3% on the ImageNet dataset.\"\\nAccuracy: 75.3']\n",
            "[4, 5, 6] 3274.25\n",
            "['Expected Output:\\nSentence: \"AMTFL obtains lower errors on both easy and hard tasks to STL, while Go-MTL results in even higher errors than those obtained by STL on hard tasks.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We report averaged performance of each model on five random splits for all datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Deep Asymmetric Multi-task Feature Learning\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We report averaged performance of each model on five random splits for all datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Deep Asymmetric Multi-task Feature Learning\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3479.0\n",
            "['Expected Output:\\nSentence: \"We report the average per-class classification performances of baselines and our models in Table 2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We report the average per-class classification performances of baselines and our models in Table 2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We report the average per-class classification performances of baselines and our models in Table 2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We report the average per-class classification performances of baselines and our models in Table 2.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We report the average per-class classification performances of baselines and our models in Table 2.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|█████████████████████████████████▎                                                                                                   | 25/100 [7:11:36<8:16:54, 397.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3309.0\n",
            "['Expected Output:\\nSentence: \"When used as a pre-training module, our method outperforms state-of-the-art approaches on the UCF-101 [36] and HMDB-51 [18] action benchmark datasets. While our model learns features from human action videos, we also demonstrate the generalizability for generic object classification and detection tasks, and show competitive performance on the PASCAL VOC 2007 dataset [7] when compared with the state-of-the-arts.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"When used as a pre-training module, our method outperforms state-of-the-art approaches on the UCF-101 [36] and HMDB-51 [18] action benchmark datasets. While our model learns features from human action videos, we also demonstrate the generalizability for generic object classification and detection tasks, and show competitive performance on the PASCAL VOC 2007 dataset [7] when compared with the state-of-the-arts.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"When used as a pre-training module, our method outperforms state-of-the-art approaches on the UCF-101 [36] and HMDB-51 [18] action benchmark datasets. While our model learns features from human action videos, we also demonstrate the generalizability for generic object classification and detection tasks, and show competitive performance on the PASCAL VOC 2007 dataset [7] when compared with the state-of-the-arts.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"When used as a pre-training module, our method outperforms state-of-the-art approaches on the UCF-101 [36] and HMDB-51 [18] action benchmark datasets. While our model learns features from human action videos, we also demonstrate the generalizability for generic object classification and detection tasks, and show competitive performance on the PASCAL VOC 2007 dataset [7] when compared with the state-of-the-arts.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"When used as a pre-training module, our method outperforms state-of-the-art approaches on the UCF-101 [36] and HMDB-51 [18] action benchmark datasets. While our model learns features from human action videos, we also demonstrate the generalizability for generic object classification and detection tasks, and show competitive performance on the PASCAL VOC 2007 dataset [7] when compared with the state-of-the-arts.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3383.0\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 3303.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██████████████████████████████████▌                                                                                                  | 26/100 [7:18:18<8:12:11, 399.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3441.0\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3359.0\n",
            "['Expected Output:\\nSentence: \"For Random Erasing, we set p = 0.5, sl = 0.02, sh = 0.2, and r1 = 1/r2 = 0.3.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We observe that, 1) all erasing schemes outperform the baseline, 2) RE-R achieves approximately equal performance to RE-M, and 3) both RE-R and RE-M are superior to RE-0 and RE-255. If not specified, we use RE-R in the following experiment.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We observe that, 1) all erasing schemes outperform the baseline, 2) RE-R achieves approximately equal performance to RE-M, and 3) both RE-R and RE-M are superior to RE-0 and RE-255. If not specified, we use RE-R in the following experiment.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We observe that, 1) all erasing schemes outperform the baseline, 2) RE-R achieves approximately equal performance to RE-M, and 3) both RE-R and RE-M are superior to RE-0 and RE-255. If not specified, we use RE-R in the following experiment.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The model is initialized by the ImageNet classification models, and then fine-tuned on the object detection data.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3652.5\n",
            "['Expected Output:\\nSentence: \"We fine-tune them on the model pre-trained on ImageNet [5].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We fine-tune them on the model pre-trained on ImageNet [5].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We fine-tune them on the model pre-trained on ImageNet [5].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We fine-tune them on the model pre-trained on ImageNet [5].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We fine-tune them on the model pre-trained on ImageNet [5].\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|███████████████████████████████████▉                                                                                                 | 27/100 [7:24:58<8:05:54, 399.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4227.0\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3374.75\n",
            "['Expected Output:\\nSentence: \"The pruning model achieves around 94 percent classification accuracy on artificial images (using two-fold cross-validation) and significantly reduces the number of artificial images in the raw image dataset.\"\\nAccuracy: 94.0', 'Expected Output:\\nSentence: \"The pruning model achieves around 94 percent classification accuracy on artificial images (using two-fold cross-validation) and significantly reduces the number of artificial images in the raw image dataset.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The pruning model achieves around 94 percent classification accuracy on artificial images (using two-fold cross-validation) and significantly reduces the number of artificial images in the raw image dataset.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The pruning model achieves around 94 percent classification accuracy on artificial images (using two-fold cross-validation) and significantly reduces the number of artificial images in the raw image dataset.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The pruning model achieves around 94 percent classification accuracy on artificial images (using two-fold cross-validation) and significantly reduces the number of artificial images in the raw image dataset.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3399.0\n",
            "['Expected Output:\\nSentence: \"ImageNet provides an average of 1000 images to represent each category and is organized according to the WordNet hierarchy.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"ImageNet provides an average of 1000 images to represent each category and is organized according to the WordNet hierarchy.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"ImageNet provides an average of 1000 images to represent each category and is organized according to the WordNet hierarchy.\"\\nAccuracy: 404', '404', '404']\n",
            "[10, 11, 12] 3734.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|█████████████████████████████████████▏                                                                                               | 28/100 [7:33:49<8:46:30, 438.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3361.0\n",
            "['Expected Output:\\nSentence: \"However, even the largest image dataset i.e Imagenet [9] has only 21841 classes, with many classes having very few images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, even the largest image dataset i.e Imagenet [9] has only 21841 classes, with many classes having very few images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, even the largest image dataset i.e Imagenet [9] has only 21841 classes, with many classes having very few images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, even the largest image dataset i.e Imagenet [9] has only 21841 classes, with many classes having very few images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"However, even the largest image dataset i.e Imagenet [9] has only 21841 classes, with many classes having very few images.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3121.75\n",
            "['Expected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -', '404', '404', '404', 'Expected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"For the Imagenet dataset, we measure the top-K accuracy i.e the classification of a test image is correct if the true label occurs in the top K predictions of the model. Similar to [13] and [28], we set the value of K to 5.\"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: \"Table 4. Results on the Imagenet dataset (top-5 accuracy)\"\\nAccuracy: -\\n\\nExpected Output:\\n']\n",
            "[7, 8, 9] 3070.0\n",
            "['Expected Output:\\nSentence: \"Imagenet is a much more challenging dataset, particularly due to the lack of explicit attribute vectors. On this we achieve an improvement of about 7.9% over other approaches.\"\\nAccuracy: 7.9', 'Expected Output:\\nSentence: \"Imagenet is a much more challenging dataset, particularly due to the lack of explicit attribute vectors. On this we achieve an improvement of about 7.9% over other approaches.\"\\nAccuracy: 7.9', 'Expected Output:\\nSentence: \"Imagenet is a much more challenging dataset, particularly due to the lack of explicit attribute vectors. On this we achieve an improvement of about 7.9% over other approaches.\"\\nAccuracy: 7.9', 'Expected Output:\\nSentence: \"Imagenet is a much more challenging dataset, particularly due to the lack of explicit attribute vectors. On this we achieve an improvement of about 7.9% over other approaches.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Imagenet is a much more challenging dataset, particularly due to the lack of explicit attribute vectors. On this we achieve an improvement of about 7.9% over other approaches.\"\\nAccuracy: 7.9']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██████████████████████████████████████▌                                                                                              | 29/100 [7:40:56<8:35:10, 435.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4008.0\n",
            "['404', 'Expected Output:\\nSentence: \"Training these CNNs has traditionally been done as a pre-processing step, whether on large datasets such as ImageNet [14] [20], on synthetic datasets [13], or on collected datasets [1].\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"Training these CNNs has traditionally been done as a pre-processing step, whether on large datasets such as ImageNet [14] [20], on synthetic datasets [13], or on collected datasets [1].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Training these CNNs has traditionally been done as a pre-processing step, whether on large datasets such as ImageNet [14] [20], on synthetic datasets [13], or on collected datasets [1].\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3569.0\n",
            "['404', 'Expected Output:\\nSentence: \"For the CNN, we use the GoogLeNet+GAP formulation suggested in [24], an augmented GoogLeNet [20], for its relatively good performance on ImageNet, lightweight nature both in size of weights as well as execution time, and its ability to provide simple localization based on class activation maps.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For the CNN, we use the GoogLeNet+GAP formulation suggested in [24], an augmented GoogLeNet [20], for its relatively good performance on ImageNet, lightweight nature both in size of weights as well as execution time, and its ability to provide simple localization based on class activation maps.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For the CNN, we use the GoogLeNet+GAP formulation suggested in [24], an augmented GoogLeNet [20], for its relatively good performance on ImageNet, lightweight nature both in size of weights as well as execution time, and its ability to provide simple localization based on class activation maps.\"\\nAccuracy: 404', '404']\n",
            "[7, 8, 9] 2990.25\n",
            "['Expected Output:\\nSentence: \"All tests use as a base the GoogLeNet-GAP model, pre-trained on ImageNet as provided by [24].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"All tests use as a base the GoogLeNet-GAP model, pre-trained on ImageNet as provided by [24].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"All tests use as a base the GoogLeNet-GAP model, pre-trained on ImageNet as provided by [24].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"All tests use as a base the GoogLeNet-GAP model, pre-trained on ImageNet as provided by [24].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"All tests use as a base the GoogLeNet-GAP model, pre-trained on ImageNet as provided by [24].\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███████████████████████████████████████▉                                                                                             | 30/100 [7:47:39<8:16:22, 425.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 4658.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3973.5\n",
            "['Expected Output:\\nSentence: \"First, we train the MAEN. This network is ﬁrst pre-trained on the ImageNet 1K dataset [28], and then ﬁne-tuned on the ﬁne-grained image classiﬁcation dataset, such as CUB-200-2011 dataset [1].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"First, we train the MAEN. This network is ﬁrst pre-trained on the ImageNet 1K dataset [28], and then ﬁne-tuned on the ﬁne-grained image classiﬁcation dataset, such as CUB-200-2011 dataset [1].\"\\nAccuracy: 404', '404', '404', '404']\n",
            "[7, 8, 9] 3678.0\n",
            "['Expected Output:\\nSentence: \"Our WSDL approach achieves the highest classiﬁcation accuracy among all the state-of-the-art methods under the same weakly supervised setting, which indicates that neither object nor part annotations are used both in training and testing phases. Our WSDL achieves the improvement by 1.02% than the best state-of-the-art result of TSC [12] (85.71% vs. 84.69%), which jointly considers two spatial constraints in part selection.\"\\nAccuracy: 85.71', 'Expected Output:\\nSentence: \"Our WSDL approach achieves the highest classiﬁcation accuracy among all the state-of-the-art methods under the same weakly supervised setting, which indicates that neither object nor part annotations are used both in training and testing phases. Our WSDL achieves the improvement by 1.02% than the best state-of-the-art result of TSC [12] (85.71% vs. 84.69%), which jointly considers two spatial constraints in part selection.\"\\nAccuracy: 85.71', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our WSDL approach achieves the highest classiﬁcation accuracy among all the state-of-the-art methods under the same weakly supervised setting, which indicates that neither object nor part annotations are used both in training and testing phases. Our WSDL achieves the improvement by 1.02% than the best state-of-the-art result of TSC [12] (85.71% vs. 84.69%), which jointly considers two spatial constraints in part selection.\"\\nAccuracy: 85.71', 'Expected Output:\\nSentence: \"Our WSDL approach achieves the highest classiﬁcation accuracy among all the state-of-the-art methods under the same weakly supervised setting, which indicates that neither object nor part annotations are used both in training and testing phases. Our WSDL achieves the improvement by 1.02% than the best state-of-the-art result of TSC [12] (85.71% vs. 84.69%), which jointly considers two spatial constraints in part selection.\"\\nAccuracy: 85.71']\n",
            "[10, 11, 12] 3385.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|█████████████████████████████████████████▏                                                                                           | 31/100 [7:56:35<8:47:35, 458.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2562.25\n",
            "['404', 'Expected Output:\\nSentence: \"These computer vision tasks have been facilitated by the ready availability of two-dimensional imaging as well as tools for labelling 2D images such as MNIST and ImageNet.4,88,9 There has been substantial work in applying and refining 2D-CNNs to these datasets, which have resulted in numerous architectural improvements, most recently the use of skip connections and residual blocks.7,10\"\\nAccuracy: 404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|██████████████████████████████████████████▌                                                                                          | 32/100 [7:58:48<6:49:15, 361.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3639.5\n",
            "['Expected Output:\\nSentence: \"Just like Krizhevsky et al. [5], we trained the network on ILSVRC-12 and reached comparable results, i.e. a top-1 accuracy of 59.23 % and a top-5 accuracy of 83.58 %.\"\\nAccuracy: 59.23', 'Expected Output:\\nSentence: \"Just like Krizhevsky et al. [5], we trained the network on ILSVRC-12 and reached comparable results, i.e. a top-1 accuracy of 59.23 % and a top-5 accuracy of 83.58 %.\"\\nAccuracy: 59.23', 'Expected Output:\\nSentence: \"Just like Krizhevsky et al. [5], we trained the network on ILSVRC-12 and reached comparable results, i.e. a top-1 accuracy of 59.23 % and a top-5 accuracy of 83.58 %.\"\\nAccuracy: 59.23', 'Expected Output:\\nSentence: \"Just like Krizhevsky et al. [5], we trained the network on ILSVRC-12 and reached comparable results, i.e. a top-1 accuracy of 59.23 % and a top-5 accuracy of 83.58 %.\"\\nAccuracy: 59.23', 'Expected Output:\\nSentence: \"Just like Krizhevsky et al. [5], we trained the network on ILSVRC-12 and reached comparable results, i.e. a top-1 accuracy of 59.23 % and a top-5 accuracy of 83.58 %.\"\\nAccuracy: 59.23']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███████████████████████████████████████████▉                                                                                         | 33/100 [8:01:05<5:28:06, 293.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 234.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[4, 5, 6] 1072.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 1136.25\n",
            "['Expected Output:\\nSentence: \"To measure its performance, in term of accuracy, it is evaluated on the image classification task [1], taking into account several settings that could occur in a typical situation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To measure its performance, in term of accuracy, it is evaluated on the image classification task [1], taking into account several settings that could occur in a typical situation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To measure its performance, in term of accuracy, it is evaluated on the image classification task [1], taking into account several settings that could occur in a typical situation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To measure its performance, in term of accuracy, it is evaluated on the image classification task [1], taking into account several settings that could occur in a typical situation.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 1162.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[13, 14, 15] 1251.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 1402.75\n",
            "['Expected Output:\\nSentence: \"In particular a short video of a scene is analyzed by the system and classified according to the most likely object between the top-5 being detected.\"\\nAccuracy: -\\n\\nSince the provided text does not mention any top-1 accuracy for ImageNet, the output is as shown above.', 'Expected Output:\\nSentence: \"In particular a short video of a scene is analyzed by the system and classified according to the most likely object between the top-5 being detected.\"\\nAccuracy: -', 'Expected Output:\\nSentence: \"In particular a short video of a scene is analyzed by the system and classified according to the most likely object between the top-5 being detected.\"\\nAccuracy: -\\n\\nSince the provided text does not mention any top-1 accuracy for ImageNet, the output is as shown above.', 'Expected Output:\\nSentence: \"In particular a short video of a scene is analyzed by the system and classified according to the most likely object between the top-5 being detected.\"\\nAccuracy: -\\n\\nSince the provided text does not mention any top-1 accuracy for ImageNet, the output is as shown above.', 'Expected Output:\\nSentence: \"In particular a short video of a scene is analyzed by the system and classified according to the most likely object between the top-5 being detected.\"\\nAccuracy: -']\n",
            "[19, 20, 21] 935.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[22, 23, 24] 952.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[25, 26, 27] 1307.75\n",
            "['Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[28, 29, 30] 1338.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[31, 32, 33] 1266.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[34, 35, 36] 1225.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[37, 38, 39] 1317.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[40, 41, 42] 1212.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[43, 44, 45] 1190.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[46, 47, 48] 1125.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[49, 50, 51] 1534.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[52, 53, 54] 989.75\n",
            "['404', 'Expected Output:\\nSentence: \"The extracted model has a ﬁnal accuracy of 45% and is evaluated on the classiﬁcation task, considering the simple case and the one with segmentation.\"\\nAccuracy: 45.0', 'Expected Output:\\nSentence: \"The extracted model has a ﬁnal accuracy of 45% and is evaluated on the classiﬁcation task, considering the simple case and the one with segmentation.\"\\nAccuracy: 45.0', '404', '404']\n",
            "[55, 56, 57] 433.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[58, 59, 60] 827.0\n",
            "['Expected Output:\\nSentence: \"In particular we have trained the AlexNet deep convolutional architecture on the ILSVRC 2012 and used the extracted model to perform image classiﬁcation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In particular we have trained the AlexNet deep convolutional architecture on the ILSVRC 2012 and used the extracted model to perform image classiﬁcation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In particular we have trained the AlexNet deep convolutional architecture on the ILSVRC 2012 and used the extracted model to perform image classiﬁcation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In particular we have trained the AlexNet deep convolutional architecture on the ILSVRC 2012 and used the extracted model to perform image classiﬁcation.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In particular we have trained the AlexNet deep convolutional architecture on the ILSVRC 2012 and used the extracted model to perform image classiﬁcation.\"\\nAccuracy: 404']\n",
            "[61, 62, 63] 853.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[64, 65, 66] 726.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|████████████████████████████████████████████▌                                                                                      | 34/100 [8:49:36<19:46:42, 1078.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3822.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3542.0\n",
            "['Expected Output:\\nSentence: \"In each case, the models are pre-trained on the ImageNet [8] and then fine-tuned on the target datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In each case, the models are pre-trained on the ImageNet [8] and then fine-tuned on the target datasets.\"\\nAccuracy: 404', '404', '404', '404']\n",
            "[7, 8, 9] 3837.25\n",
            "['404', '404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|██████████████████████████████████████████████▏                                                                                     | 35/100 [8:56:17<15:48:27, 875.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3180.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 2662.75\n",
            "['Expected Output:\\nSentence: \"We next evaluate our method on fine-tuning a pretrained CNN. We use a new domain adaptation benchmark called the VisDA Challenge (Peng et al. (2017))\"\\nAccuracy: 404', '404', '404', '404', 'Expected Output:\\nSentence: \"We compare the accuracy of C and C′ in experiments on image classification.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 2529.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[10, 11, 12] 2191.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███████████████████████████████████████████████▌                                                                                    | 36/100 [9:05:09<13:43:51, 772.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3562.0\n",
            "['404', '404', 'Expected Output:\\nSentence: \"We demonstrate that a single type of DAU-based ﬁlters achieve comparable performance to standard ConvNets on classiﬁcation as well as dilated ConvNets on a segmentation task.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We demonstrate that a single type of DAU-based ﬁlters achieve comparable performance to standard ConvNets on classiﬁcation as well as dilated ConvNets on a segmentation task.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We demonstrate that a single type of DAU-based ﬁlters achieve comparable performance to standard ConvNets on classiﬁcation as well as dilated ConvNets on a segmentation task.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3398.75\n",
            "['Expected Output:\\nSentence: \"Table 3: Results on ILSVRC 2012 validation set using AlexNet architecture and corresponding number of parameters on convolutional layers. We report top-1 accuracy.\"\\nAccuracy: 56.99', 'Expected Output:\\nSentence: \"Table 3: Results on ILSVRC 2012 validation set using AlexNet architecture and corresponding number of parameters on convolutional layers. We report top-1 accuracy.\"\\nAccuracy: 56.89', 'Expected Output:\\nSentence: \"Table 3: Results on ILSVRC 2012 validation set using AlexNet architecture and corresponding number of parameters on convolutional layers. We report top-1 accuracy.\"\\nAccuracy: 56.89', 'Expected Output:\\nSentence: \"Table 3: Results on ILSVRC 2012 validation set using AlexNet architecture and corresponding number of parameters on convolutional layers. We report top-1 accuracy.\"\\nAccuracy: 56.89', 'Expected Output:\\nSentence: \"Table 3: Results on ILSVRC 2012 validation set using AlexNet architecture and corresponding number of parameters on convolutional layers. We report top-1 accuracy.\"\\nAccuracy: 56.89']\n",
            "[7, 8, 9] 3115.25\n",
            "['Expected Output:\\nSentence: \"The results are reported in Tab. 5. We observe that all three networks achieve classiﬁcation accuracy of approximately 56-57% on ILSVRC 2012.\"\\nAccuracy: 56-57', 'Expected Output:\\nSentence: \"The results are reported in Tab. 5. We observe that all three networks achieve classiﬁcation accuracy of approximately 56-57% on ILSVRC 2012.\"\\nAccuracy: 56-57', 'Expected Output:\\nSentence: \"The results are reported in Tab. 5. We observe that all three networks achieve classiﬁcation accuracy of approximately 56-57% on ILSVRC 2012.\"\\nAccuracy: 56-57', 'Expected Output:\\nSentence: \"The results are reported in Tab. 5. We observe that all three networks achieve classiﬁcation accuracy of approximately 56-57% on ILSVRC 2012.\"\\nAccuracy: 56-57', 'Expected Output:\\nSentence: \"The results are reported in Tab. 5. We observe that all three networks achieve classiﬁcation accuracy of approximately 56-57% on ILSVRC 2012.\"\\nAccuracy: 56-57']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|████████████████████████████████████████████████▊                                                                                   | 37/100 [9:11:54<11:35:14, 662.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3464.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 2933.25\n",
            "['404', '404', '404', 'Expected Output:\\nSentence: \"The original IncV3 without any defense is used as a baseline, denoted as NA.\"\\nAccuracy: 76.7', '404']\n",
            "[7, 8, 9] 3051.25\n",
            "['404', 'Expected Output:\\nSentence: \"In NIPS 2017 competition track, Google Brain organized competition on adversarial attacks and defenses 4. The dataset used in this competition contains 5000 ImageNet-compatible clean images unknown to the teams.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"In NIPS 2017 competition track, Google Brain organized competition on adversarial attacks and defenses 4. The dataset used in this competition contains 5000 ImageNet-compatible clean images unknown to the teams.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In NIPS 2017 competition track, Google Brain organized competition on adversarial attacks and defenses 4. The dataset used in this competition contains 5000 ImageNet-compatible clean images unknown to the teams.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|██████████████████████████████████████████████████▏                                                                                 | 38/100 [9:18:33<10:02:40, 583.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3800.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3866.5\n",
            "['Expected Output:\\nSentence: \"ImageNet ILSVRC2012 (Deng et al., 2009) contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"ImageNet ILSVRC2012 (Deng et al., 2009) contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"ImageNet ILSVRC2012 (Deng et al., 2009) contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"ImageNet ILSVRC2012 (Deng et al., 2009) contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3577.5\n",
            "['404', '404', 'Expected Output:\\nSentence: \"Table 4 shows the comparison. As shown, MentorNet improves the performance of both the inception-resnet without regularization (NoReg) and with full regularization (FullModel). It also outperforms the forgetting baseline (dropout keep probability = 0.2). The results suggest that MentorNet can improve deep CNNs on the large-scale training on corrupted labels.\"\\nAccuracy: 404', '404', '404']\n",
            "[10, 11, 12] 1584.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[13, 14, 15] 1810.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 2390.25\n",
            "['Expected Output:\\nSentence: \"Training in this way on the clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10, and about 49.2% and 78.8% on CIFAR-100.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Training in this way on the clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10, and about 49.2% and 78.8% on CIFAR-100.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Training in this way on the clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10, and about 49.2% and 78.8% on CIFAR-100.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Training in this way on the clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10, and about 49.2% and 78.8% on CIFAR-100.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Training in this way on the clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10, and about 49.2% and 78.8% on CIFAR-100.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███████████████████████████████████████████████████▍                                                                                | 39/100 [9:31:50<10:58:22, 647.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3257.25\n",
            "['Expected Output:\\nSentence: \"Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3011.25\n",
            "['Expected Output:\\nSentence: \"Performance curve of MobileNetV2 vs MobileNetV1, ShufﬂeNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Performance curve of MobileNetV2 vs MobileNetV1, ShufﬂeNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Performance curve of MobileNetV2 vs MobileNetV1, ShufﬂeNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Performance curve of MobileNetV2 vs MobileNetV1, ShufﬂeNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Performance curve of MobileNetV2 vs MobileNetV1, ShufﬂeNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3192.75\n",
            "['Expected Output:\\nSentence: \"Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms ShufﬂeNet (1.5) 71.5 3.4M 292M - ShufﬂeNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for different networks.\"\\nAccuracy: 70.6', 'Expected Output:\\nSentence: \"Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms ShufﬂeNet (1.5) 71.5 3.4M 292M - ShufﬂeNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for different networks.\"\\nAccuracy: 70.6', 'Expected Output:\\nSentence: \"Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms ShufﬂeNet (1.5) 71.5 3.4M 292M - ShufﬂeNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for different networks.\"\\nAccuracy: 70.6', 'Expected Output:\\nSentence: \"Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms ShufﬂeNet (1.5) 71.5 3.4M 292M - ShufﬂeNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for different networks.\"\\nAccuracy: 70.6', 'Expected Output:\\nSentence: \"Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms ShufﬂeNet (1.5) 71.5 3.4M 292M - ShufﬂeNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for different networks.\"\\nAccuracy: 70.6']\n",
            "[10, 11, 12] 3107.25\n",
            "['Expected Output:\\nSentence: \"On Figure 7 we show how this distribution looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization symmetries). For fully trained network, while the standard deviation grew significantly, all but the two layers are still above the invertibility thresholds.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On Figure 7 we show how this distribution looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization symmetries). For fully trained network, while the standard deviation grew significantly, all but the two layers are still above the invertibility thresholds.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On Figure 7 we show how this distribution looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization symmetries). For fully trained network, while the standard deviation grew significantly, all but the two layers are still above the invertibility thresholds.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On Figure 7 we show how this distribution looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization symmetries). For fully trained network, while the standard deviation grew significantly, all but the two layers are still above the invertibility thresholds.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"On Figure 7 we show how this distribution looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization symmetries). For fully trained network, while the standard deviation grew significantly, all but the two layers are still above the invertibility thresholds.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████████████████████████████████████████████████████▊                                                                               | 40/100 [9:40:58<10:17:34, 617.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3248.75\n",
            "['Expected Output:\\nSentence: \"Later, in [8], the concept of Transfer Learning [25] was used to improve the recognition accuracy on the same standard dataset by using a CNN pre-trained on a ImageNet dataset [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later, in [8], the concept of Transfer Learning [25] was used to improve the recognition accuracy on the same standard dataset by using a CNN pre-trained on a ImageNet dataset [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later, in [8], the concept of Transfer Learning [25] was used to improve the recognition accuracy on the same standard dataset by using a CNN pre-trained on a ImageNet dataset [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later, in [8], the concept of Transfer Learning [25] was used to improve the recognition accuracy on the same standard dataset by using a CNN pre-trained on a ImageNet dataset [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later, in [8], the concept of Transfer Learning [25] was used to improve the recognition accuracy on the same standard dataset by using a CNN pre-trained on a ImageNet dataset [26].\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 4262.75\n",
            "['Expected Output:\\nSentence: \"The GoogLeNet architecture was used with ImageNet based transfer learning.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The GoogLeNet architecture was used with ImageNet based transfer learning.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The GoogLeNet architecture was used with ImageNet based transfer learning.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The GoogLeNet architecture was used with ImageNet based transfer learning.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The GoogLeNet architecture was used with ImageNet based transfer learning.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|██████████████████████████████████████████████████████▌                                                                              | 41/100 [9:45:27<8:24:28, 513.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3895.0\n",
            "['Expected Output:\\nSentence: \"We empirically validate our claims by doing an exhaustive evaluation with VGG-16 and ResNet-50. Further, we also evaluate a real world scenario where a CNN trained on all 1000 ImageNet classes needs to be tested on only a small set of classes at test time (say, only animals).\"\\nAccuracy: 404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 3827.0\n",
            "['Expected Output:\\nSentence: \"When evaluated on the standard ImageNet test set, this trained model gives us a top-1 accuracy of 69% which is comparable to the accuracy reported elsewhere in the literature.\"\\nAccuracy: 69.0', 'Expected Output:\\nSentence: \"When evaluated on the standard ImageNet test set, this trained model gives us a top-1 accuracy of 69% which is comparable to the accuracy reported elsewhere in the literature.\"\\nAccuracy: 69', 'Expected Output:\\nSentence: \"When evaluated on the standard ImageNet test set, this trained model gives us a top-1 accuracy of 69% which is comparable to the accuracy reported elsewhere in the literature.\"\\nAccuracy: 69.0', 'Expected Output:\\nSentence: \"When evaluated on the standard ImageNet test set, this trained model gives us a top-1 accuracy of 69% which is comparable to the accuracy reported elsewhere in the literature.\"\\nAccuracy: 69', 'Expected Output:\\nSentence: \"When evaluated on the standard ImageNet test set, this trained model gives us a top-1 accuracy of 69% which is comparable to the accuracy reported elsewhere in the literature.\"\\nAccuracy: 69']\n",
            "[7, 8, 9] 3418.75\n",
            "['Sentence: \"We observe that while in the first setup we get a top-1 accuracy of 74%, in the second setup we get an accuracy of 87%.\"\\nAccuracy: 74.0', 'Expected Output:\\nSentence: \"We observe that while in the first setup we get a top-1 accuracy of 74%, in the second setup we get an accuracy of 87%.\"\\nAccuracy: 74.0', 'Sentence: \"We observe that while in the first setup we get a top-1 accuracy of 74%, in the second setup we get an accuracy of 87%.\"\\nAccuracy: 74.0', 'Expected Output:\\nSentence: \"We observe that while in the first setup we get a top-1 accuracy of 74%, in the second setup we get an accuracy of 87%.\"\\nAccuracy: 74.0', 'Sentence: \"We observe that while in the first setup we get a top-1 accuracy of 74%, in the second setup we get an accuracy of 87%.\"\\n\\nAccuracy: 74.0']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|███████████████████████████████████████████████████████▊                                                                             | 42/100 [9:52:10<7:44:00, 480.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3964.0\n",
            "['404', 'Expected Output:\\nSentence: \"Inspired by the demonstration of the so-called AlexNet [5] where state-of-the-art image classification results were reported on the ILSVRC dataset, many publications describe the usage of the ILSVRC data for training deep learning image classification models.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Inspired by the demonstration of the so-called AlexNet [5] where state-of-the-art image classification results were reported on the ILSVRC dataset, many publications describe the usage of the ILSVRC data for training deep learning image classification models.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Inspired by the demonstration of the so-called AlexNet [5] where state-of-the-art image classification results were reported on the ILSVRC dataset, many publications describe the usage of the ILSVRC data for training deep learning image classification models.\"\\nAccuracy: 404', '404']\n",
            "[4, 5, 6] 3320.75\n",
            "['Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Running on a desktop computer with an NVIDIA Quadro K620, the update rate with camera capture, preprocessing, neural computation and display would take around 80 milliseconds for TensorFlow running on the GPU, while around 180 milliseconds if only the CPU was used.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|█████████████████████████████████████████████████████████▏                                                                           | 43/100 [9:56:37<6:35:24, 416.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3389.75\n",
            "['Expected Output:\\nSentence: \"In particular, running SWA for just 10 epochs on ImageNet we are able to achieve 0.8% improvement for ResNet-50 and DenseNet-161, and 0.6% improvement for ResNet-150.\"\\nAccuracy: 404', '404', '404', 'Expected Output:\\nSentence: \"In particular, running SWA for just 10 epochs on ImageNet we are able to achieve 0.8% improvement for ResNet-50 and DenseNet-161, and 0.6% improvement for ResNet-150.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In particular, running SWA for just 10 epochs on ImageNet we are able to achieve 0.8% improvement for ResNet-50 and DenseNet-161, and 0.6% improvement for ResNet-150.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3081.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[7, 8, 9] 3260.5\n",
            "['Expected Output:\\nSentence: \"For these architectures we used pretrained models from PyTorch.torchvision. For each of the models we ran SWA for 10 epochs with a cyclical learning rate schedule with the same parameters for all models (the details can be found in the Appendix), and report the mean and standard deviation of test error averaged over 3 runs. The results are shown in Table 2.\"\\nAccuracy: 76.97', 'Expected Output:\\nSentence: \"For these architectures we used pretrained models from PyTorch.torchvision. For each of the models we ran SWA for 10 epochs with a cyclical learning rate schedule with the same parameters for all models (the details can be found in the Appendix), and report the mean and standard deviation of test error averaged over 3 runs. The results are shown in Table 2.\"\\nAccuracy: 76.97', 'Expected Output:\\nSentence: \"For these architectures we used pretrained models from PyTorch.torchvision. For each of the models we ran SWA for 10 epochs with a cyclical learning rate schedule with the same parameters for all models (the details can be found in the Appendix), and report the mean and standard deviation of test error averaged over 3 runs. The results are shown in Table 2.\"\\nAccuracy: 76.97', 'Expected Output:\\nSentence: \"For these architectures we used pretrained models from PyTorch.torchvision. For each of the models we ran SWA for 10 epochs with a cyclical learning rate schedule with the same parameters for all models (the details can be found in the Appendix), and report the mean and standard deviation of test error averaged over 3 runs. The results are shown in Table 2.\"\\nAccuracy: 76.97', 'Expected Output:\\nSentence: \"For each of the models we ran SWA for 10 epochs with a cyclical learning rate schedule with the same parameters for all models (the details can be found in the Appendix), and report the mean and standard deviation of test error averaged over 3 runs. The results are shown in Table 2.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 2317.25\n",
            "['Expected Output:\\nSentence: \"Models for ImageNet are from here. Pretrained networks can be found here.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Models for ImageNet are from here. Pretrained networks can be found here.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For all experiments with ImageNet we used cyclic learning rate schedule with the same hyperparameters α1 = 0.001, α2 = 10−5 and c = 1.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Models for ImageNet are from here. Pretrained networks can be found here.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|██████████████████████████████████████████████████████████                                                                          | 44/100 [10:05:34<7:02:15, 452.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3700.75\n",
            "['Expected Output:\\nSentence: \"Later on, ResNeXt model was proposed by Facebook that won 2nd place in ILSVRC 2016 classiﬁcation task and also showed performance improvements in COCO detection [21] and ImageNet-5k set than their ResNet counter part.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later on, ResNeXt model was proposed by Facebook that won 2nd place in ILSVRC 2016 classiﬁcation task and also showed performance improvements in COCO detection [21] and ImageNet-5k set than their ResNet counter part.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The ResNet architecture was the first to pass human level performance on ImageNet, and their main contribution of residual learning is often used by default in many state-of-the-art networks today.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later on, ResNeXt model was proposed by Facebook that won 2nd place in ILSVRC 2016 classiﬁcation task and also showed performance improvements in COCO detection [21] and ImageNet-5k set than their ResNet counter part.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Later on, ResNeXt model was proposed by Facebook that won 2nd place in ILSVRC 2016 classiﬁcation task and also showed performance improvements in COCO detection [21] and ImageNet-5k set than their ResNet counter part.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3562.75\n",
            "['Expected Output:\\nSentence: \"For CIFAR datasets, the original ResNeXt model had a depth of 29 while it had a depth of 50 and 101 for the ImageNet datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For CIFAR datasets, the original ResNeXt model had a depth of 29 while it had a depth of 50 and 101 for the ImageNet datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For CIFAR datasets, the original ResNeXt model had a depth of 29 while it had a depth of 50 and 101 for the ImageNet datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For CIFAR datasets, the original ResNeXt model had a depth of 29 while it had a depth of 50 and 101 for the ImageNet datasets.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For CIFAR datasets, the original ResNeXt model had a depth of 29 while it had a depth of 50 and 101 for the ImageNet datasets.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|███████████████████████████████████████████████████████████▍                                                                        | 45/100 [10:10:05<6:04:38, 397.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3726.25\n",
            "['Expected Output:\\nSentence: \"On ImageNet, we achieve a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"On ImageNet, we achieve a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"On ImageNet, we achieve a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"On ImageNet, we achieve a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"On ImageNet, we achieve a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%.\"\\nAccuracy: 83.5']\n",
            "[4, 5, 6] 3913.0\n",
            "['Expected Output:\\nSentence: \"Our ImageNet results are shown in Table 3. As can be seen from the results, AutoAugment improves over the widely-used Inception Pre-processing [59] across a wide range of models, from ResNet-50 to the state-of-art AmoebaNets [48]. Secondly, applying AutoAugment to AmoebaNet-C improves its top-1 and top-5 accuracy from 83.1% / 96.1% to 83.5% / 96.5%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"Our ImageNet results are shown in Table 3. As can be seen from the results, AutoAugment improves over the widely-used Inception Pre-processing [59] across a wide range of models, from ResNet-50 to the state-of-art AmoebaNets [48]. Secondly, applying AutoAugment to AmoebaNet-C improves its top-1 and top-5 accuracy from 83.1% / 96.1% to 83.5% / 96.5%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"Our ImageNet results are shown in Table 3. As can be seen from the results, AutoAugment improves over the widely-used Inception Pre-processing [59] across a wide range of models, from ResNet-50 to the state-of-art AmoebaNets [48]. Secondly, applying AutoAugment to AmoebaNet-C improves its top-1 and top-5 accuracy from 83.1% / 96.1% to 83.5% / 96.5%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"Our ImageNet results are shown in Table 3. As can be seen from the results, AutoAugment improves over the widely-used Inception Pre-processing [59] across a wide range of models, from ResNet-50 to the state-of-art AmoebaNets [48]. Secondly, applying AutoAugment to AmoebaNet-C improves its top-1 and top-5 accuracy from 83.1% / 96.1% to 83.5% / 96.5%.\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"Our ImageNet results are shown in Table 3. As can be seen from the results, AutoAugment improves over the widely-used Inception Pre-processing [59] across a wide range of models, from ResNet-50 to the state-of-art AmoebaNets [48]. Secondly, applying AutoAugment to AmoebaNet-C improves its top-1 and top-5 accuracy from 83.1% / 96.1% to 83.5% / 96.5%.\"\\nAccuracy: 83.5']\n",
            "[7, 8, 9] 3977.5\n",
            "['Expected Output:\\nSentence: \"The accuracy of 83.5% / 96.5% is also the new state-of-art top-1/top-5 accuracy on this dataset (without multicrop / ensembling).\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"The accuracy of 83.5% / 96.5% is also the new state-of-art top-1/top-5 accuracy on this dataset (without multicrop / ensembling).\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"The accuracy of 83.5% / 96.5% is also the new state-of-art top-1/top-5 accuracy on this dataset (without multicrop / ensembling).\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"The accuracy of 83.5% / 96.5% is also the new state-of-art top-1/top-5 accuracy on this dataset (without multicrop / ensembling).\"\\nAccuracy: 83.5', 'Expected Output:\\nSentence: \"The accuracy of 83.5% / 96.5% is also the new state-of-art top-1/top-5 accuracy on this dataset (without multicrop / ensembling).\"\\nAccuracy: 83.5']\n",
            "[10, 11, 12] 1791.0\n",
            "['404', '404', '404', 'Expected Output:\\nSentence: \"Learning transferable architectures for scalable image recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2017. 1, 2, 3, 4\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Learning transferable architectures for scalable image recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2017. 1, 2, 3, 4\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████████████████████████████████████████████████████████████▋                                                                       | 46/100 [10:19:08<6:37:20, 441.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3408.5\n",
            "['Expected Output:\\nSentence: \"We have obtained close to best accuracy, 91.66% for image classiﬁcation on CALTECH dataset, CALTECH RGB, by retraining on the VGG-16 deep learning architecture pre-trained using RGB IMAGENET dataset [23].\"\\nAccuracy: 91.66', 'Expected Output:\\nSentence: \"We have obtained close to best accuracy, 91.66% for image classiﬁcation on CALTECH dataset, CALTECH RGB, by retraining on the VGG-16 deep learning architecture pre-trained using RGB IMAGENET dataset [23].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We have obtained close to best accuracy, 91.66% for image classiﬁcation on CALTECH dataset, CALTECH RGB, by retraining on the VGG-16 deep learning architecture pre-trained using RGB IMAGENET dataset [23].\"\\nAccuracy: 91.66', 'Expected Output:\\nSentence: \"We have obtained close to best accuracy, 91.66% for image classiﬁcation on CALTECH dataset, CALTECH RGB, by retraining on the VGG-16 deep learning architecture pre-trained using RGB IMAGENET dataset [23].\"\\nAccuracy: 91.66', 'Expected Output:\\nSentence: \"Also, we have obtained close to best accuracy, 91.66% for image classiﬁcation on CALTECH dataset, CALTECH RGB, by retraining on the VGG-16 deep learning architecture pre-trained using RGB IMAGENET dataset [23].\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|██████████████████████████████████████████████████████████████                                                                      | 47/100 [10:21:23<5:08:46, 349.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2580.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[4, 5, 6] 3289.75\n",
            "['Expected Output:\\nSentence: \"In striking contrast, CoordConv models attain perfect performance on both data splits and do so with only 7.5k parameters and in only 10–20 seconds. The parsimony of parameters further confirms they are simply more appropriate models for the task of coordinate transform [28, 10, 19].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The convolution models never achieve more than about 86% accuracy, and training is slow: the fastest learning models still take over an hour to converge.\"\\nAccuracy: 86', 'Expected Output:\\nSentence: \"In striking contrast, CoordConv models attain perfect performance on both data splits and do so with only 7.5k parameters and in only 10–20 seconds. The parsimony of parameters further confirms they are simply more appropriate models for the task of coordinate transform [28, 10, 19].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In striking contrast, CoordConv models attain perfect performance on both data splits and do so with only 7.5k parameters and in only 10–20 seconds. The parsimony of parameters further confirms they are simply more appropriate models for the task of coordinate transform [28, 10, 19].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In striking contrast, CoordConv models attain perfect performance on both data splits and do so with only 7.5k parameters and in only 10–20 seconds. The parsimony of parameters further confirms they are simply more appropriate models for the task of coordinate transform [28, 10, 19].\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 2322.75\n",
            "['Expected Output:\\nSentence: \"Adding a single extra 1×1 CoordConv layer with 8 output channels improves ResNet-50 [9] Top-5 accuracy by a meager 0.04% averaged over five runs for each treatment; however, this difference is not statistically significant.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Adding a single extra 1×1 CoordConv layer with 8 output channels improves ResNet-50 [9] Top-5 accuracy by a meager 0.04% averaged over five runs for each treatment; however, this difference is not statistically significant.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Adding a single extra 1×1 CoordConv layer with 8 output channels improves ResNet-50 [9] Top-5 accuracy by a meager 0.04% averaged over ﬁve runs for each treatment; however, this difference is not statistically signiﬁcant.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Adding a single extra 1×1 CoordConv layer with 8 output channels improves ResNet-50 [9] Top-5 accuracy by a meager 0.04% averaged over ﬁve runs for each treatment; however, this difference is not statistically signiﬁcant.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Adding a single extra 1×1 CoordConv layer with 8 output channels improves ResNet-50 [9] Top-5 accuracy by a meager 0.04% averaged over ﬁve runs for each treatment; however, this difference is not statistically signiﬁcant.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 1826.75\n",
            "['Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.\"\\nAccuracy: 404']\n",
            "[13, 14, 15] 1827.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 1576.75\n",
            "['Expected Output:\\nSentence: \"Table S2: ImageNet classiﬁcation result comparison between a baseline ResNet-50 and CoordConv ResNet-50. For each model three experiments are run, listed in three separate rows below.\"\\nAccuracy: 75.8', 'Expected Output:\\nSentence: \"Table S2: ImageNet classiﬁcation result comparison between a baseline ResNet-50 and CoordConv ResNet-50. For each model three experiments are run, listed in three separate rows below.\"\\nAccuracy: 75.8', 'Expected Output:\\nSentence: \"Table S2: ImageNet classiﬁcation result comparison between a baseline ResNet-50 and CoordConv ResNet-50. For each model three experiments are run, listed in three separate rows below.\"\\nAccuracy: 75.8', 'Expected Output:\\nSentence: \"Table S2: ImageNet classiﬁcation result comparison between a baseline ResNet-50 and CoordConv ResNet-50. For each model three experiments are run, listed in three separate rows below.\"\\nAccuracy: 75.8', 'Expected Output:\\nSentence: \"Table S2: ImageNet classiﬁcation result comparison between a baseline ResNet-50 and CoordConv ResNet-50. For each model three experiments are run, listed in three separate rows below.\"\\nAccuracy: 75.8']\n",
            "[19, 20, 21] 1390.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[22, 23, 24] 710.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|███████████████████████████████████████████████████████████████▎                                                                    | 48/100 [10:39:22<8:12:39, 568.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2013.25\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 1784.0\n",
            "['Expected Output:\\nSentence: \"To thoroughly evaluate the eﬀectiveness of our ﬁnal module, we ﬁrst perform extensive ablation experiments. Then, we verify that CBAM outperforms all the\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To thoroughly evaluate the eﬀectiveness of our ﬁnal module, we ﬁrst perform extensive ablation experiments. Then, we verify that CBAM outperforms all the\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To thoroughly evaluate the eﬀectiveness of our ﬁnal module, we ﬁrst perform extensive ablation experiments. Then, we verify that CBAM outperforms all the\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To thoroughly evaluate the eﬀectiveness of our ﬁnal module, we ﬁrst perform extensive ablation experiments. Then, we verify that CBAM outperforms all the\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To thoroughly evaluate the eﬀectiveness of our ﬁnal module, we ﬁrst perform extensive ablation experiments. Then, we verify that CBAM outperforms all the\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 2073.25\n",
            "['Expected Output:\\nSentence: \"Our final module(i.e. ResNet50 + CBAM) achieves top-1 error of 22.66%, which is much lower than SE [28](i.e. ResNet50 + SE), as shown in Table 4.\"\\nAccuracy: 77.34', 'Expected Output:\\nSentence: \"Our final module(i.e. ResNet50 + CBAM) achieves top-1 error of 22.66%, which is much lower than SE [28](i.e. ResNet50 + SE), as shown in Table 4.\"\\nAccuracy: 77.34', 'Expected Output:\\nSentence: \"Our final module(i.e. ResNet50 + CBAM) achieves top-1 error of 22.66%, which is much lower than SE [28](i.e. ResNet50 + SE), as shown in Table 4.\"\\nAccuracy: 77.34', 'Expected Output:\\nSentence: \"Our final module(i.e. ResNet50 + CBAM) achieves top-1 error of 22.66%, which is much lower than SE [28](i.e. ResNet50 + SE), as shown in Table 4.\"\\nAccuracy: 77.34', 'Expected Output:\\nSentence: \"Our final module(i.e. ResNet50 + CBAM) achieves top-1 error of 22.66%, which is much lower than SE [28](i.e. ResNet50 + SE), as shown in Table 4.\"\\nAccuracy: 77.34']\n",
            "[10, 11, 12] 1533.5\n",
            "['404', '404', '404', '404', '404']\n",
            "[13, 14, 15] 2140.0\n",
            "['Expected Output:\\nSentence: \"To verify its efficacy, we conducted extensive experiments with various state-of-the-art models and confirmed that CBAM outperforms all the baselines on three different benchmark datasets: ImageNet-1K, MS COCO, and VOC 2007.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To verify its efficacy, we conducted extensive experiments with various state-of-the-art models and confirmed that CBAM outperforms all the baselines on three different benchmark datasets: ImageNet-1K, MS COCO, and VOC 2007.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To verify its efficacy, we conducted extensive experiments with various state-of-the-art models and confirmed that CBAM outperforms all the baselines on three different benchmark datasets: ImageNet-1K, MS COCO, and VOC 2007.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To verify its efficacy, we conducted extensive experiments with various state-of-the-art models and confirmed that CBAM outperforms all the baselines on three different benchmark datasets: ImageNet-1K, MS COCO, and VOC 2007.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To verify its efficacy, we conducted extensive experiments with various state-of-the-art models and confirmed that CBAM outperforms all the baselines on three different benchmark datasets: ImageNet-1K, MS COCO, and VOC 2007.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████████████████████████████████████████████████████████████████▋                                                                   | 49/100 [10:50:27<8:27:47, 597.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 1971.75\n",
            "['Expected Output:\\nSentence: \"We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains state-of-the-art performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains state-of-the-art performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains state-of-the-art performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains state-of-the-art performance.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains state-of-the-art performance.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 1953.5\n",
            "['404', '404', '404', '404', '404']\n",
            "[7, 8, 9] 2176.0\n",
            "['Expected Output:\\nSentence: \"The proposed CurriculumNet is evaluated on four benchmarks: WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2].\"\\nAccuracy: 404', '404', '404', '404', '404']\n",
            "[10, 11, 12] 1836.0\n",
            "['Expected Output:\\nSentence: \"The proposed method, with 3-subset curriculum learning, significantly outperforms the model trained on all data, with improvements of 30.16% →27.91% and 12.43% →10.82% on Top 1 and Top 5 errors, respectively.\"\\nAccuracy: 27.91', 'Expected Output:\\nSentence: \"Table 1. Top-1 and Top-5 errors (%) of four diﬀerent models with BN-Inception architecture on validation set. The models are trained on WebVision training set and tested on the WebVision and ILSVRC validation sets under various models.\"\\nAccuracy: 36.0', 'Expected Output:\\nSentence: \"Model-D: 27.91% (Top-1) and 10.82% (Top-5) on ImageNet.\"\\nAccuracy: 27.91', 'Expected Output:\\nSentence: \"Table 1. Top-1 and Top-5 errors (%) of four diﬀerent models with BN-Inception architecture on validation set. The models are trained on WebVision training set and tested on the WebVision and ILSVRC validation sets under various models.\"\\nAccuracy: 36.0', 'Expected Output:\\nSentence: \"The proposed method, with 3-subset curriculum learning, significantly outperforms the model trained on all data, with improvements of 30.16% →27.91% and 12.43% →10.82% on Top 1 and Top 5 errors, respectively.\"\\nAccuracy: 27.91']\n",
            "[13, 14, 15] 2189.5\n",
            "['Expected Output:\\nSentence: \"CurriculumNet boosts the performance from a Top 5 error of 8.6% to 7.1%, by leveraging additional noisy data (e.g., WebVision). This performance gain is signiﬁcant on ImageNet, which further conﬁrms the strong capability of CurriculumNet on learning from noisy data.\"\\nAccuracy: 35.2', 'Expected Output:\\nSentence: \"CurriculumNet boosts the performance from a Top 5 error of 8.6% to 7.1%, by leveraging additional noisy data (e.g., WebVision). This performance gain is signiﬁcant on ImageNet, which further conﬁrms the strong capability of CurriculumNet on learning from noisy data.\"\\nAccuracy: 35.2', 'Expected Output:\\nSentence: \"CurriculumNet boosts the performance from a Top 5 error of 8.6% to 7.1%, by leveraging additional noisy data (e.g., WebVision). This performance gain is signiﬁcant on ImageNet, which further conﬁrms the strong capability of CurriculumNet on learning from noisy data.\"\\nAccuracy: 35.2', 'Expected Output:\\nSentence: \"CurriculumNet boosts the performance from a Top 5 error of 8.6% to 7.1%, by leveraging additional noisy data (e.g., WebVision). This performance gain is signiﬁcant on ImageNet, which further conﬁrms the strong capability of CurriculumNet on learning from noisy data.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"CurriculumNet boosts the performance from a Top 5 error of 8.6% to 7.1%, by leveraging additional noisy data (e.g., WebVision). This performance gain is signiﬁcant on ImageNet, which further conﬁrms the strong capability of CurriculumNet on learning from noisy data.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|██████████████████████████████████████████████████████████████████                                                                  | 50/100 [11:01:35<8:35:19, 618.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3751.75\n",
            "['Expected Output:\\nSentence: \"For the pre-trained models, we discard the fully-connected layers and classification layers, and keep the feature layers to extract feature maps for CXR images. Through the encoder, an original image X (224×224×3) is encoded to C feature maps with size S ×S, represented by En(X; Φe) where Φe is the set of trainable parameters of the encoder.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For the pre-trained models, we discard the fully-connected layers and classification layers, and keep the feature layers to extract feature maps for CXR images. Through the encoder, an original image X (224×224×3) is encoded to C feature maps with size S ×S, represented by En(X; Φe) where Φe is the set of trainable parameters of the encoder.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For the pre-trained models, we discard the fully-connected layers and classification layers, and keep the feature layers to extract feature maps for CXR images. Through the encoder, an original image X (224×224×3) is encoded to C feature maps with size S ×S, represented by En(X; Φe) where Φe is the set of trainable parameters of the encoder.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For the pre-trained models, we discard the fully-connected layers and classification layers, and keep the feature layers to extract feature maps for CXR images. Through the encoder, an original image X (224×224×3) is encoded to C feature maps with size S ×S, represented by En(X; Φe) where Φe is the set of trainable parameters of the encoder.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"For the pre-trained models, we discard the fully-connected layers and classification layers, and keep the feature layers to extract feature maps for CXR images. Through the encoder, an original image X (224×224×3) is encoded to C feature maps with size S ×S, represented by En(X; Φe) where Φe is the set of trainable parameters of the encoder.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3910.25\n",
            "['Expected Output:\\nSentence: \"In our experiments, we tried 6 pretrained models as the encoder in our architecture, including AlexNet [23], VGGNet16 [25], ResNet50 [24] and DenseNet121 [26], DenseNet161 [26], DenseNet201 [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, we tried 6 pretrained models as the encoder in our architecture, including AlexNet [23], VGGNet16 [25], ResNet50 [24] and DenseNet121 [26], DenseNet161 [26], DenseNet201 [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, we tried 6 pretrained models as the encoder in our architecture, including AlexNet [23], VGGNet16 [25], ResNet50 [24] and DenseNet121 [26], DenseNet161 [26], DenseNet201 [26]. As described in Section III-A1, we discarded the high-level fully-connected layers and classiﬁcation layers of the pretrained models and only used the feature layers as the encoder. As shown in Fig. 2, different encoders have different inner structure and have different parameters. We respectively denote the DGC based on these encoder as DGC-AlexNet (G-AN), DGC-VGGNet16 (G-VN16), DGC-ResNet50 (G-RN50), DGC-DenseNet121 (G-DN121), DGC-DenseNet161 (G-DN161) and DGC-DenseNet201 (G-DN201).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, we tried 6 pretrained models as the encoder in our architecture, including AlexNet [23], VGGNet16 [25], ResNet50 [24] and DenseNet121 [26], DenseNet161 [26], DenseNet201 [26].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our experiments, we tried 6 pretrained models as the encoder in our architecture, including AlexNet [23], VGGNet16 [25], ResNet50 [24] and DenseNet121 [26], DenseNet161 [26], DenseNet201 [26].\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3511.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our architecture, we used a complex model to identify the 14 diseases, and learned the model by a sparsity-weighted cross entropy loss, while the weights for different diseases are the same, i.e., we equally regarded all the 14 diseases and learned a generic latent low-dimensional distribution for different diseases.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"In our architecture, we used a complex model to identify the 14 diseases, and learned the model by a sparsity-weighted cross entropy loss, while the weights for different diseases are the same, i.e., we equally regarded all the 14 diseases and learned a generic latent low-dimensional distribution for different diseases.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|██████████████████████████████████████████████████████████████████▊                                                                | 51/100 [11:18:24<10:00:47, 735.66s/it]MuPDF error: syntax error: could not parse color space (635 0 R)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2019.75\n",
            "['404', '404', '404', '404', '404']\n",
            "[4, 5, 6] 1621.0\n",
            "['Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', '404', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1', 'Expected Output:\\nSentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\\nAccuracy: 82.1']\n",
            "[7, 8, 9] 2287.0\n",
            "['404', '404', '404', '404', '404']\n",
            "[10, 11, 12] 1433.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[13, 14, 15] 1811.75\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 1885.25\n",
            "['404', '404', '404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|████████████████████████████████████████████████████████████████████                                                               | 52/100 [11:31:40<10:03:05, 753.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2305.5\n",
            "['Expected Output:\\nSentence: \"On ImageNet classiﬁcation, ResNet-50 architecture with DropBlock achieves 78.13% accuracy, which is more than 1.6% improvement on the baseline.\"\\nAccuracy: 78.13', 'Expected Output:\\nSentence: \"On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78.13% accuracy, which is more than 1.6% improvement on the baseline.\"\\nAccuracy: 78.13', 'Expected Output:\\nSentence: \"On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78.13% accuracy, which is more than 1.6% improvement on the baseline.\"\\nAccuracy: 78.13', 'Sentence: \"On ImageNet classiﬁcation, ResNet-50 architecture with DropBlock achieves 78.13% accuracy, which is more than 1.6% improvement on the baseline.\"\\n\\nAccuracy: 78.13', 'Expected Output:\\nSentence: \"On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78.13% accuracy, which is more than 1.6% improvement on the baseline.\"\\nAccuracy: 78.13']\n",
            "[4, 5, 6] 2476.75\n",
            "['Sentence: \"ResNet-50 + DropBlock, (kp=0.9) 78.13 ± 0.05 94.02 ± 0.02\"\\n\\nAccuracy: 78.13', 'Sentence: \"ResNet-50 + DropBlock, (kp=0.9) achieved a top-1 accuracy of 78.13 ± 0.05.\"\\n\\nAccuracy: 78.13', 'Sentence: \"ResNet-50 + DropBlock, (kp=0.9) 78.13 ± 0.05 94.02 ± 0.02\"\\nAccuracy: 78.13', 'Sentence: \"ResNet-50 + DropBlock, (kp=0.9) 78.13 ± 0.05 94.02 ± 0.02\"\\n\\nAccuracy: 78.13', 'Sentence: \"ResNet-50 + DropBlock, (kp=0.9) achieved a top-1 accuracy of 78.13 ± 0.05.\"\\n\\nAccuracy: 78.13']\n",
            "[7, 8, 9] 2437.75\n",
            "['Expected Output:\\nSentence: \"We use class activation maps (CAM) introduced in [29] to visualize conv5_3 class activations of ResNet-50 on ImageNet validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We use class activation maps (CAM) introduced in [29] to visualize conv5_3 class activations of ResNet-50 on ImageNet validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We use class activation maps (CAM) introduced in [29] to visualize conv5_3 class activations of ResNet-50 on ImageNet validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We use class activation maps (CAM) introduced in [29] to visualize conv5_3 class activations of ResNet-50 on ImageNet validation set.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We use class activation maps (CAM) introduced in [29] to visualize conv5_3 class activations of ResNet-50 on ImageNet validation set.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████████████████████████████████████████████████████████████████████▉                                                              | 53/100 [11:38:25<8:28:24, 649.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2630.0\n",
            "['Expected Output:\\nSentence: \"The classification algorithm achieved an accuracy score of 0.81.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The classification algorithm achieved an accuracy score of 0.81.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The classification algorithm achieved an accuracy score of 0.81.\"\\nAccuracy: 81.0', 'Expected Output:\\nSentence: \"The classification algorithm achieved an accuracy score of 0.81.\"\\nAccuracy: 81.0', 'Expected Output:\\nSentence: \"The classification algorithm achieved an accuracy score of 0.81.\"\\nAccuracy: 81.0']\n",
            "[4, 5, 6] 1338.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[7, 8, 9] 1829.25\n",
            "['Expected Output:\\nSentence: \"As a result of this strong feature representation, recent deep networks (He et al., 2016a; Szegedy et al., 2015; Simonyan et al., 2014; Huang et al., 2017) have achieved remarkable accuracy in large-scale image recognition tasks (Deng et al., 2009).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"As a result of this strong feature representation, recent deep networks (He et al., 2016a; Szegedy et al., 2015; Simonyan et al., 2014; Huang et al., 2017) have achieved remarkable accuracy in large-scale image recognition tasks (Deng et al., 2009).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"As a result of this strong feature representation, recent deep networks (He et al., 2016a; Szegedy et al., 2015; Simonyan et al., 2014; Huang et al., 2017) have achieved remarkable accuracy in large-scale image recognition tasks (Deng et al., 2009).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"As a result of this strong feature representation, recent deep networks (He et al., 2016a; Szegedy et al., 2015; Simonyan et al., 2014; Huang et al., 2017) have achieved remarkable accuracy in large-scale image recognition tasks (Deng et al., 2009).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"As a result of this strong feature representation, recent deep networks (He et al., 2016a; Szegedy et al., 2015; Simonyan et al., 2014; Huang et al., 2017) have achieved remarkable accuracy in large-scale image recognition tasks (Deng et al., 2009).\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 1873.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[13, 14, 15] 2107.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[16, 17, 18] 1036.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[19, 20, 21] 1619.25\n",
            "['Expected Output:\\nSentence: \"Despite only achieving 0.4% greater accuracy than InceptionV3, ResNet32\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Despite only achieving 0.4% greater accuracy than InceptionV3, ResNet32\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Despite only achieving 0.4% greater accuracy than InceptionV3, ResNet32\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Despite only achieving 0.4% greater accuracy than InceptionV3, ResNet32\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Despite only achieving 0.4% greater accuracy than InceptionV3, ResNet32\"\\nAccuracy: 404']\n",
            "[22, 23, 24] 2374.0\n",
            "['Expected Output:\\nSentence: \"He, K., Zhang, X., Ren, S., Sun, J., 2015a. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed method achieves the greatest accuracy with a score of 0.81 as part of the digital pathology challenge at MICCAI 2017, highlighting the superior performance of our classification framework.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed method achieves the greatest accuracy with a score of 0.81 as part of the digital pathology challenge at MICCAI 2017, highlighting the superior performance of our classification framework.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The proposed method achieves the greatest accuracy with a score of 0.81 as part of the digital pathology challenge at MICCAI 2017, highlighting the superior performance of our classification framework.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|███████████████████████████████████████████████████████████████████████▎                                                            | 54/100 [11:56:09<9:53:02, 773.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3933.25\n",
            "['Expected Output:\\nSentence: \"We demonstrate by experiments on the ImageNet (Deng et al. 2009) dataset that RAEs can be misrecognized by unauthorized classifiers, while only the authorized classifier can recover the original images exactly.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We demonstrate by experiments on the ImageNet (Deng et al. 2009) dataset that RAEs can be misrecognized by unauthorized classifiers, while only the authorized classifier can recover the original images exactly.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We demonstrate by experiments on the ImageNet (Deng et al. 2009) dataset that RAEs can be misrecognized by unauthorized classifiers, while only the authorized classifier can recover the original images exactly.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We demonstrate by experiments on the ImageNet (Deng et al. 2009) dataset that RAEs can be misrecognized by unauthorized classifiers, while only the authorized classifier can recover the original images exactly.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We demonstrate by experiments on the ImageNet (Deng et al. 2009) dataset that RAEs can be misrecognized by unauthorized classifiers, while only the authorized classifier can recover the original images exactly.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 3727.75\n",
            "['Expected Output:\\nSentence: \"The pretrained Inception-v3 (Szegedy et al. 2016) is adopted as the default target attack model, which is evaluated with top-1 accuracy.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The pretrained Inception-v3 (Szegedy et al. 2016) is adopted as the default target attack model, which is evaluated with top-1 accuracy.\"\\nAccuracy: 404', '404', '404', '404']\n",
            "[7, 8, 9] 2348.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|████████████████████████████████████████████████████████████████████████▌                                                           | 55/100 [12:02:50<8:16:24, 661.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3845.75\n",
            "['Expected Output:\\nSentence: \"Then, we transfer the pre-trained model to extract features in small-scale datasets for image classification [13], as well as to be fine-tuned in the PascalVOC 2007 dataset [9] for image classification and object detection.\"\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: \"Then, we transfer the pre-trained model to extract features in small-scale datasets for image classiﬁcation [13], as well as to be ﬁne-tuned in the PascalVOC 2007 dataset [9] for image classiﬁcation and object detection.\"\\nAccuracy: 404', '404', '404']\n",
            "[4, 5, 6] 3399.25\n",
            "['Expected Output:\\nSentence: \"With only unary terms (Eqn 5 can be solved by the Hungarian algorithm), all network backbones achieve over 30% accuracy without mirror augmentation, which shows that weak visual cues can be combined to infer global patch contexts.\"\\nAccuracy: 30.0', '404', '404', '404', '404']\n",
            "[7, 8, 9] 3923.0\n",
            "['Expected Output:\\nSentence: \"Our models with AlexNet reports a 60.2% puzzle recognition accuracy which is lower than 71% reported in [27].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our models with AlexNet reports a 60.2% puzzle recognition accuracy which is lower than 71% reported in [27].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Our models with AlexNet reports a 60.2% puzzle recognition accuracy which is lower than 71% reported in [27].\"\\nAccuracy: 404', '404', '404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████████████████████████████████████████████████████████████████████████▉                                                          | 56/100 [12:09:29<7:07:26, 582.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3215.75\n",
            "['Sentence: \"On ImageNet, our model achieves 75.1% top-1 accuracy which is 3.1% higher than MobileNetV2 (Sandler et al., 2018) while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"On ImageNet, our model achieves 75.1% top-1 accuracy which is 3.1% higher than MobileNetV2 (Sandler et al., 2018) while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"On ImageNet, our model achieves 75.1% top-1 accuracy which is 3.1% higher than MobileNetV2 (Sandler et al., 2018) while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"On ImageNet, our model achieves 75.1% top-1 accuracy which is 3.1% higher than MobileNetV2 (Sandler et al., 2018) while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"On ImageNet, our model achieves 75.1% top-1 accuracy which is 3.1% higher than MobileNetV2 (Sandler et al., 2018) while being 1.2× faster.\"\\nAccuracy: 75.1']\n",
            "[4, 5, 6] 6316.75\n",
            "['404', '404', '404', 'Expected Output:\\nSentence: \"We demonstrate the effectiveness of our proposed method on two benchmark datasets (CIFAR-10 and ImageNet) for the image classiﬁcation task.\"\\nAccuracy: 404', '404']\n",
            "[7, 8, 9] 2888.25\n",
            "['Expected Output:\\nSentence: \"Specifically, compared to MobileNetV2 and MnasNet, our model improves the top-1 accuracy by 3.1% and 1.1% respectively while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"Specifically, compared to MobileNetV2 and MnasNet, our model improves the top-1 accuracy by 3.1% and 1.1% respectively while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"Specifically, to achieve the same level of top-1 accuracy performance (i.e. around 74.6%), MobileNetV2 has 143ms latency while our model only needs 78ms (1.83× faster).\"\\nAccuracy: 74.6', 'Expected Output:\\nSentence: \"Specifically, compared to MobileNetV2 and MnasNet, our model improves the top-1 accuracy by 3.1% and 1.1% respectively while being 1.2× faster.\"\\nAccuracy: 75.1', 'Expected Output:\\nSentence: \"Specifically, to achieve the same level of top-1 accuracy performance (i.e. around 74.6%), MobileNetV2 has 143ms latency while our model only needs 78ms (1.83× faster).\"\\nAccuracy: 74.6']\n",
            "[10, 11, 12] 2740.25\n",
            "['Expected Output:\\nSentence: \"Proxyless (GPU) achieved a top-1 accuracy of 75.1%.\"\\nAccuracy: 75.1', 'Sentence: \"Model Top-1 (%) GPU latency CPU latency Mobile latency Proxyless (GPU) 75.1 5.1ms 204.9ms 124ms Proxyless (CPU) 75.3 7.4ms 138.7ms 116ms Proxyless (mobile) 74.6 7.2ms 164.1ms 78ms\"\\nAccuracy: 75.3', 'Sentence: \"Proxyless (GPU) 75.1 5.1ms 204.9ms 124ms Proxyless (CPU) 75.3 7.4ms 138.7ms 116ms Proxyless (mobile) 74.6 7.2ms 164.1ms 78ms\"\\nAccuracy: 75.3', 'Expected Output:\\nSentence: \"Model Top-1 (%) GPU latency CPU latency Mobile latency Proxyless (GPU) 75.1 5.1ms 204.9ms 124ms Proxyless (CPU) 75.3 7.4ms 138.7ms 116ms Proxyless (mobile) 74.6 7.2ms 164.1ms 78ms\"\\nAccuracy: 75.1', 'Sentence: \"Proxyless (GPU) 75.1 5.1ms 204.9ms 124ms Proxyless (CPU) 75.3 7.4ms 138.7ms 116ms Proxyless (mobile) 74.6 7.2ms 164.1ms 78ms\"\\nAccuracy: 75.3']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|███████████████████████████████████████████████████████████████████████████▏                                                        | 57/100 [12:18:27<6:48:12, 569.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 3319.0\n",
            "['Expected Output:\\nSentence: \"We provide multiple benchmark results using various deep neural networks, such as Alexnet [19], VGG [34], DenseNet [15], as well as deep generative models, such as VAE [27].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We provide multiple benchmark results using various deep neural networks, such as Alexnet [19], VGG [34], DenseNet [15], as well as deep generative models, such as VAE [27].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We provide multiple benchmark results using various deep neural networks, such as Alexnet [19], VGG [34], DenseNet [15], as well as deep generative models, such as VAE [27].\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"We provide multiple benchmark results using various deep neural networks, such as Alexnet [19], VGG [34], DenseNet [15], as well as deep generative models, such as VAE [27].\"\\nAccuracy: 404', '404']\n",
            "[4, 5, 6] 3433.0\n",
            "['Expected Output:\\nSentence: \"We experiment with three different pretrained CNN architectures, namely AlexNet [19], VGG16 [34] and DenseNet-169 [15]. For AlexNet and VGG16, we extract feature vectors of size 4096 from the two last fully connected (FC) layers before the classification layer. The features from the nth hidden layer are denoted as AlexNetn and VGG16n. As an example, the last hidden FC layer in AlexNet is denoted as AlexNet7, the input of which is output from AlexNet6. For DenseNet-169, we extract the features of size 1664 from the average pooling layer before its classification layer.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"CNNs have been the state-of-the-art models in image classification ever since AlexNet [19] achieved the best classification accuracy in ILSVRC in 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"CNNs have been the state-of-the-art models in image classification ever since AlexNet [19] achieved the best classification accuracy in ILSVRC in 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"CNNs have been the state-of-the-art models in image classification ever since AlexNet [19] achieved the best classification accuracy in ILSVRC in 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"CNNs have been the state-of-the-art models in image classification ever since AlexNet [19] achieved the best classification accuracy in ILSVRC in 2012.\"\\nAccuracy: 404']\n",
            "[7, 8, 9] 3509.5\n",
            "['404', '404', 'Expected Output:\\nSentence: \"Fine-tuning the entire network results improves the classification performance consistently for each method in Table 1. The performance is clearly enhanced for features extracted from fine-tuned VGG16 and DenseNet-169, which improves the classification accuracy by 10% in most cases for SVM-ft, VAE+SVM-ft, and VAE-CCA+SVM-ft. For AlexNet and VGG16, we see that the performance drops when extracting the features from layer FC7 instead of FC6. The reason might be that the off-the-shelf features in FC7 are more difficult to transfer to other datasets since the weights are biased towards classifying objects in the ImageNet database. The performance drops also when we use fine-tuned features, which could be due to the small learning rate we use for the pretrained layers, such that the later layers are still ImageNet-specific.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Fine-tuning the entire network results improves the classification performance consistently for each method in Table 1. The performance is clearly enhanced for features extracted from fine-tuned VGG16 and DenseNet-169, which improves the classification accuracy by 10% in most cases for SVM-ft, VAE+SVM-ft, and VAE-CCA+SVM-ft. For AlexNet and VGG16, we see that the performance drops when extracting the features from layer FC7 instead of FC6. The reason might be that the off-the-shelf features in FC7 are more difficult to transfer to other datasets since the weights are biased towards classifying objects in the ImageNet database.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Fine-tuning the entire network results improves the classification performance consistently for each method in Table 1. The performance is clearly enhanced for features extracted from fine-tuned VGG16 and DenseNet-169, which improves the classification accuracy by 10% in most cases for SVM-ft, VAE+SVM-ft, and VAE-CCA+SVM-ft. For AlexNet and VGG16, we see that the performance drops when extracting the features from layer FC7 instead of FC6. The reason might be that the off-the-shelf features in FC7 are more difficult to transfer to other datasets since the weights are biased towards classifying objects in the ImageNet database. The performance drops also when we use fine-tuned features, which could be due to the small learning rate we use for the pretrained layers, such that the later layers are still ImageNet-specific.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|████████████████████████████████████████████████████████████████████████████▌                                                       | 58/100 [12:25:11<6:03:57, 519.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 2707.25\n",
            "['Expected Output:\\nSentence: \"unlike the classiﬁcation task on the imageNet dataset, which contains tens of millions of data points, there are only about 10,000 Chinese characters.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"unlike the classiﬁcation task on the imageNet dataset, which contains tens of millions of data points, there are only about 10,000 Chinese characters.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"unlike the classiﬁcation task on the imageNet dataset, which contains tens of millions of data points, there are only about 10,000 Chinese characters.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"unlike the classiﬁcation task on the imageNet dataset, which contains tens of millions of data points, there are only about 10,000 Chinese characters.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"unlike the classiﬁcation task on the imageNet dataset, which contains tens of millions of data points, there are only about 10,000 Chinese characters.\"\\nAccuracy: 404']\n",
            "[4, 5, 6] 2039.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[7, 8, 9] 2281.0\n",
            "['Expected Output:\\nSentence: \"Directly using deep CNNs in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of ImageNet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the ImageNet dataset utilizes tens of millions of different images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Directly using deep CNNs in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of ImageNet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the ImageNet dataset utilizes tens of millions of different images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Directly using deep CNNs in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of ImageNet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the ImageNet dataset utilizes tens of millions of different images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Directly using deep CNNs in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of ImageNet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the ImageNet dataset utilizes tens of millions of different images.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Directly using deep CNNs in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of ImageNet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the ImageNet dataset utilizes tens of millions of different images.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 1976.5\n",
            "['Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.\"\\nAccuracy: 404']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████████████████████████████████████████████████████████████████████████████▉                                                      | 59/100 [12:34:11<5:59:25, 525.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EOD #####\n",
            "[1, 2, 3] 823.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[4, 5, 6] 1812.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1']\n",
            "[7, 8, 9] 1746.5\n",
            "['Expected Output:\\nSentence: \"AlexNet, a convolutional neural network (CNN), managed to outperform all other competing, non deep learning (we call them shallow today) methods by a large margin on the object classiﬁcation (over 1000) categories challenge.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"AlexNet, a convolutional neural network (CNN), managed to outperform all other competing, non deep learning (we call them shallow today) methods by a large margin on the object classiﬁcation (over 1000) categories challenge.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"AlexNet, a convolutional neural network (CNN), managed to outperform all other competing, non deep learning (we call them shallow today) methods by a large margin on the object classiﬁcation (over 1000) categories challenge.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"AlexNet, a convolutional neural network (CNN), managed to outperform all other competing, non deep learning (we call them shallow today) methods by a large margin on the object classiﬁcation (over 1000) categories challenge.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"AlexNet, a convolutional neural network (CNN), managed to outperform all other competing, non deep learning (we call them shallow today) methods by a large margin on the object classiﬁcation (over 1000) categories challenge.\"\\nAccuracy: 404']\n",
            "[10, 11, 12] 1572.25\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[13, 14, 15] 1520.0\n",
            "['Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1']\n",
            "[16, 17, 18] 2475.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: \"The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\\nAccuracy: 57.1', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[19, 20, 21] 1719.75\n",
            "['Expected Output:\\nSentence: \"To further perform an analysis on a large-scale dataset, we also consider the Cross-Dataset Testbed introduced in [154] and specifically the Caltech-ImageNet setting.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To further perform an analysis on a large-scale dataset, we also consider the Cross-Dataset Testbed introduced in [154] and specifically the Caltech-ImageNet setting.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To further perform an analysis on a large-scale dataset, we also consider the Cross-Dataset Testbed introduced in [154] and specifically the Caltech-ImageNet setting.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To further perform an analysis on a large-scale dataset, we also consider the Cross-Dataset Testbed introduced in [154] and specifically the Caltech-ImageNet setting.\"\\nAccuracy: 404', 'Expected Output:\\nSentence: \"To further perform an analysis on a large-scale dataset, we also consider the Cross-Dataset Testbed introduced in [154] and specifically the Caltech-ImageNet setting.\"\\nAccuracy: 404']\n",
            "[22, 23, 24] 1558.0\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[25, 26, 27] 1563.5\n",
            "['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\n",
            "[28, 29, 30] 2032.5\n"
          ]
        }
      ],
      "source": [
        "prompt_self_veri = \"\"\"\n",
        "extract the top1 accuracy of ImageNet from the given text and return both the sentence containing the accuracy.\n",
        "Answer in a number, eg. 90.2% and the accuracy value in 1 number. 404 if it's not mentioned. Use the examples below as a guide.\n",
        "\n",
        "Example 1:\n",
        "Expected Output:\n",
        "Sentence: \"a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method \"Case 4: α ⊗β ⊗γ\" achieved a Top-1 accuracy of 57.1.\"\n",
        "Accuracy: 57.1\n",
        "\n",
        "Example 2:\n",
        "Expected Output:\n",
        "Sentence: \"In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions.\"\n",
        "Accuracy: 82.1\n",
        "\n",
        "Example 3:\n",
        "Expected Output:\n",
        "Sentence: \"The evaluation results showed a top-1 accuracy of 78.3% on the test data on Imagenet.\"\n",
        "Accuracy: 78.3\n",
        "\n",
        "Example 4:\n",
        "Expected Output:\n",
        "Sentence: \"Our proposed model achieved a top-1 accuracy of 74.2% when evaluated on the Imagenet dataset.\"\n",
        "Accuracy: 74.2\n",
        "\n",
        "Example 4:\n",
        "Expected Output:\n",
        "Sentence: \"Our proposed model achieved a top-5 accuracy of 66.2% when evaluated on the Imagenet dataset.\"\n",
        "Accuracy: -\n",
        "\n",
        "Now extract the top1 accuracy of ImageNet from the following texts, {page}\n",
        "\n",
        "Expected Output:\n",
        "\"\"\"\n",
        "\n",
        "prompt_vote_accuracy = \"\"\"Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.\n",
        "\n",
        "Example 1:\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "Expected Output: 92.4\n",
        "\n",
        "Example 2:\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4\"\\nAccuracy: 82.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 82.4',\n",
        "Expected Output: 82.4\n",
        "\n",
        "Example 3:\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: 404',\n",
        "Expected Output: -\n",
        "\n",
        "Example 4:\n",
        "'Sentence: \"It's not mentioned.\"\\nAccuracy: -',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's mentioned top-5 accuracy on ImageNet\"\\nAccuracy: -',\n",
        "'Sentence: \"Cocoa 23.3 21.2\"\\nAccuracy: 23.3',\n",
        "Expected Output: -\n",
        "\n",
        "Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}\n",
        "\n",
        "Expected Output:\n",
        "\"\"\"\n",
        "\n",
        "prompt_vote_accuracy = \"\"\"Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.\n",
        "\n",
        "Example 1:\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4\"\\nAccuracy: 92.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "Expected Output: 92.4\n",
        "\n",
        "Example 2:\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 74.6',\n",
        "'Sentence: \"ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4\"\\nAccuracy: 82.4',\n",
        "'Sentence: \"SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6\"\\nAccuracy: 82.4',\n",
        "Expected Output: 82.4\n",
        "\n",
        "Example 3:\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's not mentioned top-1 accuracy on ImageNet\"\\nAccuracy: 404',\n",
        "'Sentence: \"-\"\\nAccuracy: 404',\n",
        "Expected Output: -\n",
        "\n",
        "Example 4:\n",
        "'Sentence: \"It's not mentioned.\"\\nAccuracy: -',\n",
        "'Sentence: \"-\"\\nAccuracy: -',\n",
        "'Sentence: \"It's mentioned top-5 accuracy on ImageNet\"\\nAccuracy: -',\n",
        "'Sentence: \"Cocoa 23.3 21.2\"\\nAccuracy: 23.3',\n",
        "Expected Output: -\n",
        "\n",
        "Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}\n",
        "\n",
        "Expected Output:\n",
        "\"\"\"\n",
        "\n",
        "def parse_gpt_with_page_prompt(page_with_res, prompt):\n",
        "    openai_api_key = ''\n",
        "    prompt2 = ChatPromptTemplate.from_template(prompt)\n",
        "    output_parser = StrOutputParser()\n",
        "    model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key, temperature=0.0,)\n",
        "\n",
        "    chain = prompt2 | model | output_parser\n",
        "    try:\n",
        "        prompt_value = chain.invoke(\n",
        "            {'page': page_with_res}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        prompt_value = str(e)\n",
        "    return prompt_value\n",
        "\n",
        "def delayed_completion(delays_in_sec, **kwargs):\n",
        "    time.sleep(delays_in_sec)\n",
        "    return parse_gpt_with_page_prompt(**kwargs)\n",
        "\n",
        "def parse_gpt_with_vote(sentences_and_accuracies, prompt):\n",
        "    openai_api_key = ''\n",
        "    model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key, temperature=0.0, max_tokens=5)\n",
        "\n",
        "    chain = ChatPromptTemplate.from_template(prompt) | model | StrOutputParser()\n",
        "    try:\n",
        "        prompt_value = chain.invoke(\n",
        "            {'sentences_and_accuracies': sentences_and_accuracies}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        prompt_value = str(e)\n",
        "    return prompt_value\n",
        "\n",
        "\n",
        "# Calculate the delay based on your rate limit\n",
        "token_limit_per_minute = 10000\n",
        "delay = 60.0 / token_limit_per_minute * len(rel_page) / 3\n",
        "\n",
        "def chunk_text_keys(keys, stepsize):\n",
        "    lis = []\n",
        "    sublis = []\n",
        "    for k in keys:\n",
        "        sublis += [k]\n",
        "        if k%3 == 0:\n",
        "            lis.append(sublis)\n",
        "            sublis = []\n",
        "    return lis\n",
        "\n",
        "def _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy):\n",
        "    # print('expect ', groundtruth_df[['Paper Name','Model','Top-1 Accuracy']].iloc[idx].values)\n",
        "    text_dict = extract_text_from_pdf_as_dict(pdf_path)\n",
        "\n",
        "    # compose sublist\n",
        "    responses_lis = []\n",
        "    for _, sublis in enumerate(chunk_text_keys(text_dict.keys(), stepsize=4)):\n",
        "        rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in sublis])\n",
        "        print(sublis, len(rel_page)/4)\n",
        "        responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]\n",
        "        responses_lis.append(responses)\n",
        "        print(responses)\n",
        "\n",
        "    # assumption is that the vote_ensemble will consistently return NA or Results\n",
        "    relevant_response = [parse_gpt_with_vote(res, prompt_vote_accuracy) for res in responses_lis]\n",
        "    rel_res = [ans for ans in relevant_response if ans != '404']\n",
        "    final_res = parse_gpt_with_vote(rel_res, prompt_vote_accuracy) if rel_res else '404'\n",
        "    print('#### EOD #####')\n",
        "    return final_res\n",
        "\n",
        "res_df_whole_paper = pd.DataFrame(data=[[idx, find_fname(pdf_path), _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy)] \\\n",
        "                                          for idx, (fname, pdf_path) in tqdm(sorted_pdf_key_page_100.items())], \\\n",
        "                                    columns = ['file_idx', 'file_name', 'gpt_vote_ensemble_whole_paper'])\n",
        "\n",
        "reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='outer'), [template, res_df_whole_paper])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba75378-9760-4cd1-ac65-db9735b7ab88",
      "metadata": {
        "id": "aba75378-9760-4cd1-ac65-db9735b7ab88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7343e41f-c231-4648-b092-ec0864e06cc2",
      "metadata": {
        "id": "7343e41f-c231-4648-b092-ec0864e06cc2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3f87b2-2e4b-4bfc-b7f6-2eb4aa069e22",
      "metadata": {
        "id": "3d3f87b2-2e4b-4bfc-b7f6-2eb4aa069e22"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a45996e-2120-42a3-9669-fedb45560adb",
      "metadata": {
        "id": "3a45996e-2120-42a3-9669-fedb45560adb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82015f15-e033-4db5-8f14-84c12c71b5ae",
      "metadata": {
        "id": "82015f15-e033-4db5-8f14-84c12c71b5ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea38cdf-e8a7-4991-b39a-ec3b797a7270",
      "metadata": {
        "id": "3ea38cdf-e8a7-4991-b39a-ec3b797a7270"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}