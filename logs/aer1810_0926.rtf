{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red208\green208\blue208;\red254\green212\blue213;
\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\c87059;\cssrgb\c85098\c85098\c85098;\cssrgb\c100000\c86667\c86667;
\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf2 \cb3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\f1 \cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \cb4   0%|                                                                                                                                                     | 0/4 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 expect  ['OmniVec: Learning robust representations with cross modal sharing'\
 'OminiVec (FT)' '92.40%']\
groundtruth resulst\
expect  ['Deep Residual Learning for Image Recognition' 'ResNet-152' '-']\
[1, 2, 3] 3784.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                          | 1/4 [02:12<06:36, 132.23s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', 'Expected Output:\\nSentence: "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\'978\'d7 deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set."\\nAccuracy: 96.43', 'Expected Output:\\nSentence: "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\'978\'d7 deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set."\\nAccuracy: 96.43', 'Expected Output:\\nSentence: "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\'978\'d7 deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set."\\nAccuracy: 96.43']\
[4, 5, 6] 3251.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 2/4 [04:24<04:24, 132.13s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', 'Expected Output:\\nSentence: "ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left)."\\nAccuracy: 404', 'Expected Output:\\nSentence: "ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left)."\\nAccuracy: 404', 'Expected Output:\\nSentence: "ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left)."\\nAccuracy: 404']\
[7, 8, 9] 3972.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  75%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                    | 3/4 [06:35<02:11, 131.90s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "Our 152-layer ResNet has a single-model top-5 validation error of 4.49%."\\nAccuracy: 404', '404', '404', '404', '404']\
[10, 11, 12] 3848.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 4/4 [08:48<00:00, 132.16s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "Using ResNet-101 for predicting classes (4.6% top-5 classi\uc0\u64257 cation error, Table 4), the top-5 localization error is 14.4%."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Using ResNet-101 for predicting classes (4.6% top-5 classi\u64257 cation error, Table 4), the top-5 localization error is 14.4%."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Using ResNet-101 for predicting classes (4.6% top-5 classi\u64257 cation error, Table 4), the top-5 localization error is 14.4%."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Using ResNet-101 for predicting classes (4.6% top-5 classi\u64257 cation error, Table 4), the top-5 localization error is 14.4%."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Using ResNet-101 for predicting classes (4.6% top-5 classi\u64257 cation error, Table 4), the top-5 localization error is 14.4%."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 96.43\
404\
404\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/4 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 96.43\
---\
expect  ['Multiscale Dense Networks for Resource Efficient Image Classification'\
 'MSDNet' '75%']\
[1, 2, 3] 3385.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                          | 1/4 [02:10<06:32, 130.81s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
[4, 5, 6] 3616.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 2/4 [04:21<04:21, 130.92s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\
[7, 8, 9] 3439.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  75%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                    | 3/4 [06:34<02:11, 131.53s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "For instance, with an average budget of 1.7\'d710^9 FLOPs, MSDNet achieves a top-1 accuracy of \uc0\u8764 75%, which is \u8764 6% higher than that achieved by a ResNet with the same number of FLOPs."\\nAccuracy: 75.0', 'Expected Output:\\nSentence: "For instance, with an average budget of 1.7\'d710^9 FLOPs, MSDNet achieves a top-1 accuracy of \u8764 75%, which is \u8764 6% higher than that achieved by a ResNet with the same number of FLOPs."\\nAccuracy: 75.0', 'Expected Output:\\nSentence: "For instance, with an average budget of 1.7\'d710^9 FLOPs, MSDNet achieves a top-1 accuracy of \u8764 75%, which is \u8764 6% higher than that achieved by a ResNet with the same number of FLOPs."\\nAccuracy: 75.0', 'Expected Output:\\nSentence: "For instance, with an average budget of 1.7\'d710^9 FLOPs, MSDNet achieves a top-1 accuracy of \u8764 75%, which is \u8764 6% higher than that achieved by a ResNet with the same number of FLOPs."\\nAccuracy: 75.0', 'Expected Output:\\nSentence: "For instance, with an average budget of 1.7\'d710^9 FLOPs, MSDNet achieves a top-1 accuracy of \u8764 75%, which is \u8764 6% higher than that achieved by a ResNet with the same number of FLOPs."\\nAccuracy: 75.0']\
[10, 11, 12] 2515.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 4/4 [08:46<00:00, 131.57s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
404\
75.0\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/4 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 75.0\
---\
expect  ['Not All Samples Are Created Equal: Deep Learning with Importance Sampling'\
 'ResNet-50' '-']\
[1, 2, 3] 3303.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                          | 1/4 [02:11<06:33, 131.30s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', 'Expected Output:\\nSentence: "Compared to existing batch selection schemes, we show that our method consistently achieves lower training loss and test error for equalized wall-clock time."\\nAccuracy: 404', '404']\
[4, 5, 6] 3559.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 2/4 [04:24<04:23, 131.74s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "Our second experiment shows the application of importance sampling to the significant task of fine tuning a pre-trained large neural network on a new dataset. This task is of particular importance because there exists an abundance of powerful models pre-trained on large datasets such as ImageNet (Deng et al., 2009)."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our experimental setup is the following, we \uc0\u64257 ne-tune a ResNet-50 (He et al., 2015) previously trained on ImageNet."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our experimental setup is the following, we \u64257 ne-tune a ResNet-50 (He et al., 2015) previously trained on ImageNet."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our experimental setup is the following, we \u64257 ne-tune a ResNet-50 (He et al., 2015) previously trained on ImageNet."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our second experiment shows the application of importance sampling to the significant task of fine-tuning a pre-trained large neural network on a new dataset. This task is of particular importance because there exists an abundance of powerful models pre-trained on large datasets such as ImageNet (Deng et al., 2009)."\\nAccuracy: 404']\
[7, 8, 9] 2640.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  75%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                    | 3/4 [06:35<02:11, 131.63s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\
[10, 11, 12] 2104.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 4/4 [08:47<00:00, 131.95s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "We observe that SGD with momentum performs significantly better than all SVRG methods. Full batch SVRG and Katyusha perform a small number of parameter updates thus failing to optimize the networks. In all cases, the best variance reduced method achieves more than an order of magnitude higher training loss than our proposed importance sampling scheme."\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: "We observe that SGD with momentum performs significantly better than all SVRG methods. Full batch SVRG and Katyusha perform a small number of parameter updates thus failing to optimize the networks. In all cases, the best variance reduced method achieves more than an order of magnitude higher training loss than our proposed importance sampling scheme."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
404\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/3 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
---\
expect  ['Effects of Degradations on Deep Neural Network Architectures'\
 'V-CapsNet ' '-']\
[1, 2, 3] 3929.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  33%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                                                                                              | 1/3 [02:12<04:25, 132.84s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "ResNet-50 consists of 50 layers having only 25.6 million parameters. The architecture is 2.6 times deeper with 82.8% lesser parameters than VGG-19."\\nAccuracy: 82.8', 'Expected Output:\\nSentence: "The architecture is 2.6 times deeper with 82.8% lesser parameters than VGG-19."\\nAccuracy: 82.8', 'Expected Output:\\nSentence: "The architecture is 2.6 times deeper with 82.8% lesser parameters than VGG-19."\\nAccuracy: 82.8', 'Expected Output:\\nSentence: "The architecture is 2.6 times deeper with 82.8% lesser parameters than VGG-19."\\nAccuracy: 82.8', 'Expected Output:\\nSentence: "The architecture is 2.6 times deeper with 82.8% lesser parameters than VGG-19."\\nAccuracy: 82.8']\
[4, 5, 6] 1528.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\
 67%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                               | 2/3 [49:26<15:42, 942.94s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x13cc042d0>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)"))', 'Expected Output:\\nSentence: "In our experiments, the proposed V-CapsNet fusion architecture achieves 99.83% validation accuracy on the natural images dataset, improving the baseline performance of CapsuleNet by 6.2%."\\nAccuracy: 99.83', 'Expected Output:\\nSentence: "In our experiments, the proposed V-CapsNet fusion architecture achieves 99.83% validation accuracy on the natural images dataset, improving the baseline performance of CapsuleNet by 6.2%."\\nAccuracy: 99.83', 'Expected Output:\\nSentence: "In our experiments, the proposed V-CapsNet fusion architecture achieves 99.83% validation accuracy on the natural images dataset, improving the baseline performance of CapsuleNet by 6.2%."\\nAccuracy: 99.83', 'Expected Output:\\nSentence: "In our experiments, the proposed V-CapsNet fusion architecture achieves 99.83% validation accuracy on the natural images dataset, improving the baseline performance of CapsuleNet by 6.2%."\\nAccuracy: 99.83']\
[7, 8, 9] 2253.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 3/3 [51:38<00:00, 1032.95s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "CapsuleNet uses a novel dynamic routing algorithm to achieve classi\uc0\u64257 cation accuracy close to deep CNNs while using a much shallower architecture."\\nAccuracy: 404', 'Expected Output:\\nSentence: "For example, introducing the NTT layer in VGG-19 trades off 6% baseline accuracy in favor of up to 68% improvement in the presence of Gaussian color and salt-and-pepper noise."\\nAccuracy: 404', 'Expected Output:\\nSentence: "For example, introducing the NTT layer in VGG-19 trades off 6% baseline accuracy in favor of up to 68% improvement in the presence of Gaussian color and salt-and-pepper noise."\\nAccuracy: 404', 'Expected Output:\\nSentence: "For example, introducing the NTT layer in VGG-19 trades off 6% baseline accuracy in favor of up to 68% improvement in the presence of Gaussian color and salt-and-pepper noise."\\nAccuracy: 404', 'Expected Output:\\nSentence: "For example, introducing the NTT layer in VGG-19 trades off 6% baseline accuracy in favor of up to 68% improvement in the presence of Gaussian color and salt-and-pepper noise."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 82.8\
99.83\
404\
82.8\
---\
expect  ['A Unified Approximation Framework for Compressing and Accelerating Deep Neural Networks'\
 'AlexNet' '-']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/2 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 [1, 2, 3] 14736.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 1/2 [02:14<02:14, 134.16s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
[4, 5, 6] 5833.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 2/2 [04:26<00:00, 133.01s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/6 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
---\
expect  ['ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design '\
 'ShuffleNet v2-50' '77.20%']\
[1, 2, 3] 1664.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  17%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                                                                                                     | 1/6 [02:12<11:02, 132.58s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "For example, given the computation complexity budget of 40M FLOPs, Shu\uc0\u64260 eNet v2 is 3.5% and 3.7% more accurate than Shu\u64260 eNet v1 and MobileNet v2, respectively."\\nAccuracy: 404', '404', '404', '404', '404']\
[4, 5, 6] 1986.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  33%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                                                                                              | 2/6 [04:26<08:52, 133.06s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "They are both highly efficient and accurate on ImageNet classification task."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Although we only analyze these two networks, we note that they are representative for the current trend. At their core are group convolution and depth-wise convolution, which are also crucial components for other state-of-the-art networks, such as ResNeXt [7], Xception [12], MobileNet [13], and CondenseNet [16]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Although we only analyze these two networks, we note that they are representative for the current trend. At their core are group convolution and depth-wise convolution, which are also crucial components for other state-of-the-art networks, such as ResNeXt [7], Xception [12], MobileNet [13], and CondenseNet [16]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Although we only analyze these two networks, we note that they are representative for the current trend. At their core are group convolution and depth-wise convolution, which are also crucial components for other state-of-the-art networks, such as ResNeXt [7], Xception [12], MobileNet [13], and CondenseNet [16]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Although we only analyze these two networks, we note that they are representative for the current trend. At their core are group convolution and depth-wise convolution, which are also crucial components for other state-of-the-art networks, such as ResNeXt [7], Xception [12], MobileNet [13], and CondenseNet [16]."\\nAccuracy: 404']\
[7, 8, 9] 2024.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 3/6 [06:38<06:38, 132.76s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', 'Expected Output:\\nSentence: "Analysis of Network Accuracy Shu\uc0\u64260 eNet v2 is not only e\u64259 cient, but also accurate."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Analysis of Network Accuracy Shu\u64260 eNet v2 is not only e\u64259 cient, but also accurate."\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: "Analysis of Network Accuracy Shu\u64260 eNet v2 is not only e\u64259 cient, but also accurate."\\nAccuracy: 404']\
[10, 11, 12] 2093.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  67%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                               | 4/6 [08:49<04:24, 132.17s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
[13, 14, 15] 1880.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  83%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                        | 5/6 [11:17<02:16, 137.00s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "SE-Shu\uc0\u64260 eNet v2-164 (ours, with residual) 12.7G 18.56"\\nAccuracy: 81.44', 'Expected Output:\\nSentence: "SE-Shu\u64260 eNet v2-164 (ours, with residual) 12.7G 18.56"\\nAccuracy: 81.44', 'Expected Output:\\nSentence: "SE-Shu\u64260 eNet v2-164 (ours, with residual) 12.7G 18.56"\\nAccuracy: 81.44', 'Expected Output:\\nSentence: "SE-Shu\u64260 eNet v2-164 (ours, with residual) 12.7G 18.56"\\nAccuracy: 81.44', 'Expected Output:\\nSentence: "SE-Shu\u64260 eNet v2-164 (ours, with residual) 12.7G 18.56"\\nAccuracy: 81.44']\
[16, 17, 18] 1519.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 6/6 [14:07<00:00, 141.19s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x13cb2e650>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)"))', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
404\
404\
404\
81.44\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 81.44\
---\
expect  ['Extreme Network Compression via Filter Group Approximation' 'VGG16'\
 '77.86']\
[1, 2, 3] 2050.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  20%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                                 | 1/5 [02:10<08:43, 130.81s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
[4, 5, 6] 1639.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  40%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                     | 2/5 [04:23<06:33, 131.31s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', 'Expected Output:\\nSentence: "In our experiments, few epochs\'92 fine-tuning (usually less than 20 epochs) with a very small learning rate is enough to achieve better accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "In our experiments, few epochs\'92 fine-tuning (usually less than 20 epochs) with a very small learning rate is enough to achieve better accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "In our experiments, few epochs\'92 fine-tuning (usually less than 20 epochs) with a very small learning rate is enough to achieve better accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "In our experiments, few epochs\'92 fine-tuning (usually less than 20 epochs) with a very small learning rate is enough to achieve better accuracy."\\nAccuracy: 404']\
[7, 8, 9] 1492.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  60%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                         | 3/5 [06:34<04:22, 131.19s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "A variation of the original VGG16 from [22] (called VGG16(S) in our experiments) with top-1 accuracy 73.26% is used to evaluate the compression on CIFAR100 dataset."\\nAccuracy: 73.26', '404', '404', '404', '404']\
[10, 11, 12] 1679.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  80%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                             | 4/5 [08:48<02:12, 132.27s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "Ours 84.84% 1.22% 0.62 116 9.54 Asym.(3D) 84.04% 2.78% 0.67 128 18.33 processes of the compressed networks, we use an initial learning rate of 1e-4, and divide it by 10 when training losses are stable."\\nAccuracy: 84.84', 'Expected Output:\\nSentence: "Ours 84.84% 1.22% 0.62 116 9.54 Asym.(3D) 84.04% 2.78% 0.67 128 18.33 processes of the compressed networks, we use an initial learning rate of 1e-4, and divide it by 10 when training losses are stable."\\nAccuracy: 84.84', 'Expected Output:\\nSentence: "Ours 84.84% 1.22% 0.62 116 9.54 Asym.(3D) 84.04% 2.78% 0.67 128 18.33 processes of the compressed networks, we use an initial learning rate of 1e-4, and divide it by 10 when training losses are stable."\\nAccuracy: 84.84', 'Expected Output:\\nSentence: "Ours 84.84% 1.22% 0.62 116 9.54 Asym.(3D) 84.04% 2.78% 0.67 128 18.33 processes of the compressed networks, we use an initial learning rate of 1e-4, and divide it by 10 when training losses are stable."\\nAccuracy: 84.84', 'Expected Output:\\nSentence: "Ours 84.84% 1.22% 0.62 116 9.54 Asym.(3D) 84.04% 2.78% 0.67 128 18.33 processes of the compressed networks, we use an initial learning rate of 1e-4, and divide it by 10 when training losses are stable."\\nAccuracy: 84.84']\
[13, 14, 15] 1682.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 5/5 [11:01<00:00, 132.20s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "The experimental results demonstrated that our proposed method can achieve extreme compression ratio with tiny loss in accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The experimental results demonstrated that our proposed method can achieve extreme compression ratio with tiny loss in accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The experimental results demonstrated that our proposed method can achieve extreme compression ratio with tiny loss in accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The experimental results demonstrated that our proposed method can achieve extreme compression ratio with tiny loss in accuracy."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The experimental results demonstrated that our proposed method can achieve extreme compression ratio with tiny loss in accuracy."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
404\
404\
84.84\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 84.84\
---\
expect  ['Improving Transferability of Deep Neural Networks' 'ResNet-27' '-']\
[1, 2, 3] 1390.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  20%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                                 | 1/5 [02:12<08:51, 133.00s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: "We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates."\\nAccuracy: 404', 'Expected Output:\\nSentence: "We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates."\\nAccuracy: 404', 'Expected Output:\\nSentence: "We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates."\\nAccuracy: 404', 'Expected Output:\\nSentence: "We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates."\\nAccuracy: 404']\
[4, 5, 6] 1463.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  40%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                     | 2/5 [04:28<06:41, 133.75s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "The training of the source and target models was done using Caffe [11] and a ResNet-27 model [10]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The training of the source and target models was done using Caffe [11] and a ResNet-27 model [10]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The training of the source and target models was done using Caffe [11] and a ResNet-27 model [10]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The training of the source and target models was done using Caffe [11] and a ResNet-27 model [10]."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The training of the source and target models was done using Caffe [11] and a ResNet-27 model [10]."\\nAccuracy: 404']\
[7, 8, 9] 1326.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  60%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                         | 3/5 [06:43<04:28, 134.14s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "Table 1 compares the difference in accuracy of trained model for two different values of learning rate of the last layer, 0.01 and 0.1, corresponding to experiments IL\uc0\u8722 0/LL\u8722 0.01 and IL \u8722 0/LL \u8722 0.1. Observe that the accuracy is sensitive to the choice of LL and significant gains in accuracy (up to 127%) are achievable for certain domains by just choosing the best value of LL."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Table 1 compares the difference in accuracy of trained model for two different values of learning rate of the last layer, 0.01 and 0.1, corresponding to experiments IL\u8722 0/LL\u8722 0.01 and IL \u8722 0/LL \u8722 0.1. Observe that the accuracy is sensitive to the choice of LL and significant gains in accuracy (up to 127%) are achievable for certain domains by just choosing the best value of LL."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Table 1 compares the difference in accuracy of trained model for two different values of learning rate of the last layer, 0.01 and 0.1, corresponding to experiments IL\u8722 0/LL\u8722 0.01 and IL \u8722 0/LL \u8722 0.1. Observe that the accuracy is sensitive to the choice of LL and significant gains in accuracy (up to 127%) are achievable for certain domains by just choosing the best value of LL."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Table 1 compares the difference in accuracy of trained model for two different values of learning rate of the last layer, 0.01 and 0.1, corresponding to experiments IL\u8722 0/LL\u8722 0.01 and IL \u8722 0/LL \u8722 0.1. Observe that the accuracy is sensitive to the choice of LL and significant gains in accuracy (up to 127%) are achievable for certain domains by just choosing the best value of LL."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Table 1 compares the difference in accuracy of trained model for two different values of learning rate of the last layer, 0.01 and 0.1, corresponding to experiments IL\u8722 0/LL\u8722 0.01 and IL \u8722 0/LL \u8722 0.1. Observe that the accuracy is sensitive to the choice of LL and significant gains in accuracy (up to 127%) are achievable for certain domains by just choosing the best value of LL."\\nAccuracy: 404']\
[10, 11, 12] 1246.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\
Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x13cbe5f50>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)")).\
 80%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                             | 4/5 [09:20<02:20, 140.91s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "The average top-1 accuracy across the 70 tasks was 78.1%."\\nAccuracy: 78.1', 'Expected Output:\\nSentence: "The average top-1 accuracy across the 70 tasks was 78.1%."\\nAccuracy: 78.1', 'Expected Output:\\nSentence: "The average top-1 accuracy across the 70 tasks was 78.1%."\\nAccuracy: 78.1', 'Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x13cc046d0>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)"))', 'Error communicating with OpenAI: HTTPSConnectionPool(host=\\'api.openai.com\\', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x13cbaf090>: Failed to resolve \\'api.openai.com\\' ([Errno 8] nodename nor servname provided, or not known)"))']\
[13, 14, 15] 1260.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 5/5 [11:38<00:00, 139.73s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Sentence: "We found that if we picked the individual scale which maximized the accuracy for each job, the mean top-1 accuracy across all tasks improved from 78.1% to 88.0%, a significant gain."\\n\\nAccuracy: 88.0', 'Sentence: "We found that if we picked the individual scale which maximized the accuracy for each job, the mean top-1 accuracy across all tasks improved from 78.1% to 88.0%, a significant gain."\\n\\nAccuracy: 88.0', 'Sentence: "We found that if we picked the individual scale which maximized the accuracy for each job, the mean top-1 accuracy across all tasks improved from 78.1% to 88.0%, a significant gain."\\n\\nAccuracy: 88.0', 'Sentence: "We found that if we picked the individual scale which maximized the accuracy for each job, the mean top-1 accuracy across all tasks improved from 78.1% to 88.0%, a significant gain."\\n\\nAccuracy: 88.0', 'Sentence: "We found that if we picked the individual scale which maximized the accuracy for each job, the mean top-1 accuracy across all tasks improved from 78.1% to 88.0%, a significant gain."\\nAccuracy: 88.0']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
404\
404\
78.1\
88.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/3 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 88.0\
---\
expect  ['MnasNet: Platform-Aware Neural Architecture Search for Mobile '\
 'MnasNet ' '75.20%']\
[1, 2, 3] 3322.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  33%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                                                                                              | 1/3 [02:14<04:29, 134.72s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\'d7 faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3\'d7 faster than NASNet [36] with 1.2% higher accuracy."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\'d7 faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3\'d7 faster than NASNet [36] with 1.2% higher accuracy."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\'d7 faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3\'d7 faster than NASNet [36] with 1.2% higher accuracy."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\'d7 faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3\'d7 faster than NASNet [36] with 1.2% higher accuracy."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\'d7 faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3\'d7 faster than NASNet [36] with 1.2% higher accuracy."\\nAccuracy: 75.2']\
[4, 5, 6] 3073.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  67%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                               | 2/3 [04:30<02:14, 134.95s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "As shown in the table, our MnasNet A1 model achieves 75.2% top-1 / 92.5% top-5 accuracy with 78ms latency and 3.9M parameters / 312M multiply-adds, achieving a new state-of-the-art accuracy for this typical mobile latency constraint."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "As shown in the table, our MnasNet A1 model achieves 75.2% top-1 / 92.5% top-5 accuracy with 78ms latency and 3.9M parameters / 312M multiply-adds, achieving a new state-of-the-art accuracy for this typical mobile latency constraint."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "As shown in the table, our MnasNet A1 model achieves 75.2% top-1 / 92.5% top-5 accuracy with 78ms latency and 3.9M parameters / 312M multiply-adds, achieving a new state-of-the-art accuracy for this typical mobile latency constraint."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "As shown in the table, our MnasNet A1 model achieves 75.2% top-1 / 92.5% top-5 accuracy with 78ms latency and 3.9M parameters / 312M multiply-adds, achieving a new state-of-the-art accuracy for this typical mobile latency constraint."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "As shown in the table, our MnasNet A1 model achieves 75.2% top-1 / 92.5% top-5 accuracy with 78ms latency and 3.9M parameters / 312M multiply-adds, achieving a new state-of-the-art accuracy for this typical mobile latency constraint."\\nAccuracy: 75.2']\
[7, 8, 9] 3324.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 3/3 [06:50<00:00, 136.86s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 60.3\\n\\nExpected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 64.1\\n\\nExpected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 64.9\\n\\nExpected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 66.0\\n\\nExpected Output:\\nSentence: "Comparison of Decoupled Search Space and Reward Design \'96 Multi-obj denotes our multi-objective reward; Single-obj denotes only optimizing accuracy."\\nAccuracy: 74.0\\n\\nExpected Output:\\nSentence: "Comparison of Decoupled Search Space and Reward Design \'96 Multi-obj denotes our multi-objective reward; Single-obj denotes only optimizing accuracy."\\nAccuracy: 72.0\\n\\nExpected Output:\\nSentence: "Comparison of Decoupled Search Space and Reward Design \'96 Multi-obj denotes our multi-objective reward; Single-obj denotes only optimizing accuracy."\\nAccuracy: 75.2\\n\\nExpected Output:\\nSentence: "Performance Comparison of MnasNet and Its Variants \'96 MnasNet-A1 denotes the model shown in Figure 7(a); others are variants that repeat a single type of layer throughout the network."\\nAccuracy: 75.2\\n\\nExpected Output:\\nSentence: "Performance Comparison of MnasNet and Its Variants \'96 MnasNet-A1 denotes the model shown in Figure 7(a); others are variants that repeat a single type of layer throughout the network."\\nAccuracy: 71.8\\n\\nExpected Output:\\nSentence: "Performance Comparison of MnasNet and Its Variants \'96 MnasNet-A1 denotes the model shown in Figure 7(a); others are variants that repeat a single type of layer throughout the network."\\nAccuracy: 72.5\\n\\nExpected Output:\\nSentence: "Performance Comparison of MnasNet and Its Variants \'96 MnasNet-A1 denotes the model shown in Figure 7(a); others are variants that repeat a single type of layer throughout the network."\\nAccuracy: 74.9\\n\\nExpected Output:\\nSentence: "Performance Comparison of MnasNet and Its Variants \'96 MnasNet-A1 denotes the model shown in Figure 7(a); others are variants that repeat a single type of layer throughout the network."\\nAccuracy: 75.6', 'Expected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 66.0\\n\\nExpected Output:\\nSentence: "Our proposed model achieved a top-1 accuracy of 75.2% when evaluated on the Imagenet dataset."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 66.0\\n\\nExpected Output:\\nSentence: "Our proposed model achieved a top-1 accuracy of 75.2% when evaluated on the Imagenet dataset."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 66.0\\n\\nExpected Output:\\nSentence: "Our proposed model achieved a top-1 accuracy of 75.2% when evaluated on the Imagenet dataset."\\nAccuracy: 75.2', 'Expected Output:\\nSentence: "MobileNetV2 (0.35x) and MnasNet-A1 (0.35x) denote scaling the baseline models with depth multiplier 0.35; MnasNet-search1/2 denotes models from a new architecture search that targets 22ms latency constraint."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 75.2\
75.2\
66.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/3 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 75.2\
---\
expect  ['Anchor Loss: Modulating Loss Scale based on Prediction Difficulty'\
 'ResNet-50' '76.82%']\
[1, 2, 3] 3493.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  33%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                                                                                              | 1/3 [02:14<04:28, 134.01s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods."\\nAccuracy: 404', 'Expected Output:\\nSentence: "To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods."\\nAccuracy: 404', 'Expected Output:\\nSentence: "To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods."\\nAccuracy: 404', 'Expected Output:\\nSentence: "To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods."\\nAccuracy: 404', 'Expected Output:\\nSentence: "To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods."\\nAccuracy: 404']\
[4, 5, 6] 3034.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  67%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                               | 2/3 [05:03<02:24, 144.80s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "In addition, we train ResNet-50 models on ImageNet using different loss functions. We use 8 GPUs and batch size of 224. To accelerate training, we employ a mixed-precision. We apply minimal data augmentation, i.e., random cropping of 224 \'d7 224 and horizontal \uc0\u64258 ipping. The learning rate starts from 0.1 and decays 0.1 every 30 epoch. We also perform learning rate warmup strategy for first 5 epochs as proposed in [19]. Results. For CIFAR, we train and test the network three times and report the mean and standard deviation in Table 1. We report top-1 and top-5 accuracy and compare the score with other loss functions and OHEM. OHEM computes the loss values for all samples in a batch, chooses the samples of high loss contribution with a ratio of \u961 , and updates the gradient only using those samples. As we can see in the Table 1, our loss function has shown improvements over all loss functions we evaluated. For CIFAR 100, performance improved by simply replacing the cross entropy to the binary cross entropy, and anchor loss gives further gain by exploiting the automated re-scaling scheme. With our experimental setting, we found that sampling hard examples (OHEM) does not help. We tried out few different sampling Table 3. Ablation studies on CIFAR-100 (ResNet-110) Top-1 Top-5 Static anchor probabilities \u947  = 0.5 q\u8727 = 0.8 73.74 92.45 \u947  = 0.5 q\u8727 = 0.5 73.77 92.30 \u947  = 0.5 q\u8727 = 0.1 73.11 92.08 Dynamic anchor probabilities \u947  = 0.5 - 74.25 92.62 \u947  = 1.0 - 73.59 92.04 \u947  = 2.0 - 71.86 91.46 ratio settings, but found performance degradation over all ratios. Ablation Studies. As an ablation study, we report the top-1 and top-5 accuracy on CIFAR-100 by varying the \u947  in Table 3. For classification task, low \u947  yielded a good performance. We also perform experiments with fixed anchor probabilities to see how the automated sample difficulty from the network helps training. The results in Table 3 show that using the network output to define sample difficulty and rescale the loss based on this value helps the network keep a good learning signal. CE warmup strategy. To accelerate and stabilize the training process, we use CE for first few epochs and then replace loss function to AL. We tested CE warmup on CIFAR-100 for the first 5 epochs (Figure 5). With the warmup strategy, the ratio of hard samples was decreased; in other words, loss function less fluctuated. As a result, we achieved the highest top-1 accuracy of 74.38% (averaged out multiple runs) regardless of a high \u947  = 2 value. 4.2. Human Pose Estimation We evaluate our method on two different human pose estimation datasets: single-person pose on MPII [1] and LSP [26] dataset. The single-person pose estimation problem assumes that the position and the scale information of a target person are given. Implementation details. For the task of human pose estimation, we use the Hourglass network [34] as a baseline and only replace the loss function with the proposed loss during training. Note that we put sigmoid activation layer on top of the standard architecture to perform classification. Pose models are trained using Torch [12] framework. The input size is set to 256\'d7256, batch size is 6, and the model is trained with a single NVIDIA Tesla V100 GPU. Learning rate is set to 0.001 for the first 100 epochs and dropped by half and 0.2 iteratively at every 20 epoch. Testing is held by averaging the heatmaps over six-scale image pyramid with flipping."\\nAccuracy: 76.82', 'Expected Output:\\nSentence: "In addition, we train ResNet-50 models on ImageNet using different loss functions. We use 8 GPUs and batch size of 224. To accelerate training, we employ a mixed-precision. We apply minimal data augmentation, i.e., random cropping of 224 \'d7 224 and horizontal \u64258 ipping. The learning rate starts from 0.1 and decays 0.1 every 30 epoch. We also perform learning rate warmup strategy for \u64257 rst 5 epochs as proposed in [19]. Results. For CIFAR, we train and test the network three times and report the mean and standard deviation in Table 1. We report top-1 and top-5 accuracy and compare the score with other loss functions and OHEM. OHEM computes the loss values for all samples in a batch, chooses the samples of high loss contribution with a ratio of \u961 , and updates the gradient only using those samples. As we can see in the Table 1, our loss function has shown improvements over all loss functions we evaluated. For CIFAR 100, performance improved by simply replacing the cross entropy to the binary cross entropy, and anchor loss gives further gain by exploiting the automated re-scaling scheme. With our experimental setting, we found that sampling hard examples (OHEM) does not help. We tried out few different sampling Table 3. Ablation studies on CIFAR-100 (ResNet-110) Top-1 Top-5 Static anchor probabilities \u947  = 0.5 q\u8727 = 0.8 73.74 92.45 \u947  = 0.5 q\u8727 = 0.5 73.77 92.30 \u947  = 0.5 q\u8727 = 0.1 73.11 92.08 Dynamic anchor probabilities \u947  = 0.5 - 74.25 92.62 \u947  = 1.0 - 73.59 92.04 \u947  = 2.0 - 71.86 91.46 ratio settings, but found performance degradation over all ratios. Ablation Studies. As an ablation study, we report the top-1 and top-5 accuracy on CIFAR-100 by varying the \u947  in Table 3. For classi\u64257 cation task, low \u947  yielded a good performance. We also perform experiments with \u64257 xed anchor probabilities to see how the automated sample dif\u64257 culty from the network helps training. The results in Table 3 show that using the network output to de\u64257 ne sample dif\u64257 culty and rescale the loss based on this value helps the network keep a good learning signal. CE warmup strategy. To accelerate and stabilize the training process, we use CE for \u64257 rst few epochs and then replace loss function to AL. We tested CE warmup on CIFAR-100 for the \u64257 rst 5 epochs (Figure 5). With the warmup strategy, the ratio of hard samples was decreased; in other words, loss function less \u64258 uctuated. As a result, we achieved the highest top-1 accuracy of 74.38% (averaged out multiple runs) regardless of a high \u947  = 2 value. 4.2. Human Pose Estimation We evaluate our method on two different human pose estimation datasets: single-person pose on MPII [1] and LSP [26] dataset. The single-person pose estimation problem assumes that the position and the scale information of a target person are given. Implementation details. For the task of human pose es- timation, we use the Hourglass network [34] as a baseline and only replace the loss function with the proposed loss during training. Note that we put sigmoid activation layer on top of the standard architecture to perform classi\u64257 cation. Pose models are trained using Torch [12] framework. The input size is set to 256\'d7256, batch size is 6, and the model is trained with a single NVIDIA Tesla V100 GPU. Learning rate is set to 0.001 for the \u64257 rst 100 epochs and dropped by half and 0.2 iteratively at every 20 epoch. Testing is held by averaging the heatmaps over six-scale image pyramid with \u64258 ipping."\\nAccuracy: 76.82', 'Expected Output:\\nSentence: "In addition, we train ResNet-50 models on ImageNet using different loss functions. We use 8 GPUs and batch size of 224. To accelerate training, we employ a mixed-precision. We apply minimal data augmentation, i.e., random cropping of 224 \'d7 224 and horizontal \u64258 ipping. The learning rate starts from 0.1 and decays 0.1 every 30 epoch. We also perform learning rate warmup strategy for \u64257 rst 5 epochs as proposed in [19]. Results. For CIFAR, we train and test the network three times and report the mean and standard deviation in Table 1. We report top-1 and top-5 accuracy and compare the score with other loss functions and OHEM. OHEM computes the loss values for all samples in a batch, chooses the samples of high loss contribution with a ratio of \u961 , and updates the gradient only using those samples. As we can see in the Table 1, our loss function has shown improvements over all loss functions we evaluated. For CIFAR 100, performance improved by simply replacing the cross entropy to the binary cross entropy, and anchor loss gives further gain by exploiting the automated re-scaling scheme. With our experimental setting, we found that sampling hard examples (OHEM) does not help. We tried out few different sampling Table 3. Ablation studies on CIFAR-100 (ResNet-110) Top-1 Top-5 Static anchor probabilities \u947  = 0.5 q\u8727 = 0.8 73.74 92.45 \u947  = 0.5 q\u8727 = 0.5 73.77 92.30 \u947  = 0.5 q\u8727 = 0.1 73.11 92.08 Dynamic anchor probabilities \u947  = 0.5 - 74.25 92.62 \u947  = 1.0 - 73.59 92.04 \u947  = 2.0 - 71.86 91.46 ratio settings, but found performance degradation over all ratios. Ablation Studies. As an ablation study, we report the top-1 and top-5 accuracy on CIFAR-100 by varying the \u947  in Table 3. For classi\u64257 cation task, low \u947  yielded a good performance. We also perform experiments with \u64257 xed anchor probabilities to see how the automated sample dif\u64257 culty from the network helps training. The results in Table 3 show that using the network output to de\u64257 ne sample dif\u64257 culty and rescale the loss based on this value helps the network keep a good learning signal. CE warmup strategy. To accelerate and stabilize the training process, we use CE for \u64257 rst few epochs and then replace loss function to AL. We tested CE warmup on CIFAR-100 for the \u64257 rst 5 epochs (Figure 5). With the warmup strategy, the ratio of hard samples was decreased; in other words, loss function less \u64258 uctuated. As a result, we achieved the highest top-1 accuracy of 74.38% (averaged out multiple runs) regardless of a high \u947  = 2 value. 4.2. Human Pose Estimation We evaluate our method on two different human pose estimation datasets: single-person pose on MPII [1] and LSP [26] dataset. The single-person pose estimation problem assumes that the position and the scale information of a target person are given. Implementation details. For the task of human pose estimation, we use the Hourglass network [34] as a baseline and only replace the loss function with the proposed loss during training. Note that we put sigmoid activation layer on top of the standard architecture to perform classi\u64257 cation. Pose models are trained using Torch [12] framework. The input size is set to 256\'d7256, batch size is 6, and the model is trained with a single NVIDIA Tesla V100 GPU. Learning rate is set to 0.001 for the \u64257 rst 100 epochs and dropped by half and 0.2 iteratively at every 20 epoch. Testing is held by averaging the heatmaps over six-scale image pyramid with \u64258 ipping."\\n\\nAccuracy: 76.82', 'Expected Output:\\nSentence: "In addition, we train ResNet-50 models on ImageNet using different loss functions. We use 8 GPUs and batch size of 224. To accelerate training, we employ a mixed-precision. We apply minimal data augmentation, i.e., random cropping of 224 \'d7 224 and horizontal \u64258 ipping. The learning rate starts from 0.1 and decays 0.1 every 30 epoch. We also perform learning rate warmup strategy for \u64257 rst 5 epochs as proposed in [19]. Results. For CIFAR, we train and test the network three times and report the mean and standard deviation in Table 1. We report top-1 and top-5 accuracy and compare the score with other loss functions and OHEM. OHEM computes the loss values for all samples in a batch, chooses the samples of high loss contribution with a ratio of \u961 , and updates the gradient only using those samples. As we can see in the Table 1, our loss function has shown improvements over all loss functions we evaluated. For CIFAR 100, performance improved by simply replacing the cross entropy to the binary cross entropy, and anchor loss gives further gain by exploiting the automated re-scaling scheme. With our experimental setting, we found that sampling hard examples (OHEM) does not help. We tried out few different sampling Table 3. Ablation studies on CIFAR-100 (ResNet-110) Top-1 Top-5 Static anchor probabilities \u947  = 0.5 q\u8727 = 0.8 73.74 92.45 \u947  = 0.5 q\u8727 = 0.5 73.77 92.30 \u947  = 0.5 q\u8727 = 0.1 73.11 92.08 Dynamic anchor probabilities \u947  = 0.5 - 74.25 92.62 \u947  = 1.0 - 73.59 92.04 \u947  = 2.0 - 71.86 91.46 ratio settings, but found performance degradation over all ratios. Ablation Studies. As an ablation study, we report the top-1 and top-5 accuracy on CIFAR-100 by varying the \u947  in Table 3. For classi\u64257 cation task, low \u947  yielded a good performance. We also perform experiments with \u64257 xed anchor probabilities to see how the automated sample dif\u64257 culty from the network helps training. The results in Table 3 show that using the network output to de\u64257 ne sample dif\u64257 culty and rescale the loss based on this value helps the network keep a good learning signal. CE warmup strategy. To accelerate and stabilize the training process, we use CE for \u64257 rst few epochs and then replace loss function to AL. We tested CE warmup on CIFAR-100 for the \u64257 rst 5 epochs (Figure 5). With the warmup strategy, the ratio of hard samples was decreased; in other words, loss function less \u64258 uctuated. As a result, we achieved the highest top-1 accuracy of 74.38% (averaged out multiple runs) regardless of a high \u947  = 2 value. 4.2. Human Pose Estimation We evaluate our method on two different human pose estimation datasets: single-person pose on MPII [1] and LSP [26] dataset. The single-person pose estimation problem assumes that the position and the scale information of a target person are given. Implementation details. For the task of human pose es- timation, we use the Hourglass network [34] as a baseline and only replace the loss function with the proposed loss during training. Note that we put sigmoid activation layer on top of the standard architecture to perform classi\u64257 cation. Pose models are trained using Torch [12] framework. The input size is set to 256\'d7256, batch size is 6, and the model is trained with a single NVIDIA Tesla V100 GPU. Learning rate is set to 0.001 for the \u64257 rst 100 epochs and dropped by half and 0.2 iteratively at every 20 epoch. Testing is held by averaging the heatmaps over six-scale image pyramid with \u64258 ipping."\\nAccuracy: 76.82', 'Expected Output:\\nSentence: "In addition, we train ResNet-50 models on ImageNet using different loss functions. We use 8 GPUs and batch size of 224. To accelerate training, we employ a mixed-precision. We apply minimal data augmentation, i.e., random cropping of 224 \'d7 224 and horizontal \u64258 ipping. The learning rate starts from 0.1 and decays 0.1 every 30 epoch. We also perform learning rate warmup strategy for \u64257 rst 5 epochs as proposed in [19]. Results. For CIFAR, we train and test the network three times and report the mean and standard deviation in Table 1. We report top-1 and top-5 accuracy and compare the score with other loss functions and OHEM. OHEM computes the loss values for all samples in a batch, chooses the samples of high loss contribution with a ratio of \u961 , and updates the gradient only using those samples. As we can see in the Table 1, our loss function has shown improvements over all loss functions we evaluated. For CIFAR 100, performance improved by simply replacing the cross entropy to the binary cross entropy, and anchor loss gives further gain by exploiting the automated re-scaling scheme. With our experimental setting, we found that sampling hard examples (OHEM) does not help. We tried out few different sampling Table 3. Ablation studies on CIFAR-100 (ResNet-110) Top-1 Top-5 Static anchor probabilities \u947  = 0.5 q\u8727 = 0.8 73.74 92.45 \u947  = 0.5 q\u8727 = 0.5 73.77 92.30 \u947  = 0.5 q\u8727 = 0.1 73.11 92.08 Dynamic anchor probabilities \u947  = 0.5 - 74.25 92.62 \u947  = 1.0 - 73.59 92.04 \u947  = 2.0 - 71.86 91.46 ratio settings, but found performance degradation over all ratios. Ablation Studies. As an ablation study, we report the top-1 and top-5 accuracy on CIFAR-100 by varying the \u947  in Table 3. For classi\u64257 cation task, low \u947  yielded a good performance. We also perform experiments with \u64257 xed anchor probabilities to see how the automated sample dif\u64257 culty from the network helps training. The results in Table 3 show that using the network output to de\u64257 ne sample dif\u64257 culty and rescale the loss based on this value helps the network keep a good learning signal. CE warmup strategy. To accelerate and stabilize the training process, we use CE for \u64257 rst few epochs and then replace loss function to AL. We tested CE warmup on CIFAR-100 for the \u64257 rst 5 epochs (Figure 5). With the warmup strategy, the ratio of hard samples was decreased; in other words, loss function less \u64258 uctuated. As a result, we achieved the highest top-1 accuracy of 74.38% (averaged out multiple runs) regardless of a high \u947  = 2 value. 4.2. Human Pose Estimation We evaluate our method on two different human pose estimation datasets: single-person pose on MPII [1] and LSP [26] dataset. The single-person pose estimation problem assumes that the position and the scale information of a target person are given. Implementation details. For the task of human pose es- timation, we use the Hourglass network [34] as a baseline and only replace the loss function with the proposed loss during training. Note that we put sigmoid activation layer on top of the standard architecture to perform classi\u64257 cation. Pose models are trained using Torch [12] framework. The input size is set to 256\'d7256, batch size is 6, and the model is trained with a single NVIDIA Tesla V100 GPU. Learning rate is set to 0.001 for the \u64257 rst 100 epochs and dropped by half and 0.2 iteratively at every 20 epoch. Testing is held by averaging the heatmaps over six-scale image pyramid with \u64258 ipping."\\nAccuracy: 76.82']\
[7, 8, 9] 3623.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 3/3 [07:16<00:00, 145.43s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
76.82\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/4 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 76.82\
---\
expect  ['XNOR-Net++: Improved Binary Neural Networks' 'Binary ResNet-18' '57.10%']\
[1, 2, 3] 2527.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                          | 1/4 [02:12<06:38, 132.87s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "For example, there is \uc0\u8764 18% drop in top-1 accuracy between a real-valued ResNet-18 and its binary counterpart on ImageNet [28]."\\nAccuracy: 18.0', '404', 'Expected Output:\\nSentence: "For example, there is \u8764 18% drop in top-1 accuracy between a real-valued ResNet-18 and its binary counterpart on ImageNet [28]."\\nAccuracy: 404', '404', '404']\
[4, 5, 6] 2231.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 2/4 [04:26<04:26, 133.12s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: \uc0\u945  \u8855 \u946  \u8855 \u947 " achieved a Top-1 accuracy of 57.1."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: \u945  \u8855 \u946  \u8855 \u947 " achieved a Top-1 accuracy of 57.1."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: \u945  \u8855 \u946  \u8855 \u947 " achieved a Top-1 accuracy of 57.1."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: \u945  \u8855 \u946  \u8855 \u947 " achieved a Top-1 accuracy of 57.1."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: \u945  \u8855 \u946  \u8855 \u947 " achieved a Top-1 accuracy of 57.1."\\nAccuracy: 57.1']\
[7, 8, 9] 1774.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  75%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                    | 3/4 [06:43<02:14, 134.31s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Sentence: "Our results are summarized in Table 3: when using ResNet-18, our method signi\uc0\u64257 cantly outperforms the state-of-the-art by about 6% in terms of absolute error using both Top-1 and Top-5 metrics."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "Our results are summarized in Table 3: when using ResNet-18, our method significantly outperforms the state-of-the-art by about 6% in terms of absolute error using both Top-1 and Top-5 metrics."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "Our results are summarized in Table 3: when using ResNet-18, our method significantly outperforms the state-of-the-art by about 6% in terms of absolute error using both Top-1 and Top-5 metrics."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "Our results are summarized in Table 3: when using ResNet-18, our method significantly outperforms the state-of-the-art by about 6% in terms of absolute error using both Top-1 and Top-5 metrics."\\nAccuracy: 57.1', 'Expected Output:\\nSentence: "Our results are summarized in Table 3: when using ResNet-18, our method significantly outperforms the state-of-the-art by about 6% in terms of absolute error using both Top-1 and Top-5 metrics."\\nAccuracy: 57.1']\
[10, 11, 12] 1908.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 4/4 [08:54<00:00, 133.64s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
57.1\
57.1\
404\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4   0%|                                                                                                                                                     | 0/6 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 57.1\
---\
expect  ['OmniVec: Learning robust representations with cross modal sharing'\
 'OminiVec (FT)' '92.40%']\
[1, 2, 3] 4011.25\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  17%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                                                                                                     | 1/6 [02:11<10:55, 131.19s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['404', '404', '404', '404', '404']\
[4, 5, 6] 3774.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  33%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                                                                                              | 2/6 [04:23<08:46, 131.66s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "We surpass the state of the art on iNaturalist with a top-1 accuracy of 93.8%, compared to InternImage\'92s 92.6%."\\nAccuracy: 93.8', '404', 'Expected Output:\\nSentence: "We surpass the state of the art on iNaturalist with a top-1 accuracy of 93.8%, compared to InternImage\'92s 92.6%."\\nAccuracy: 93.8', '404', 'Expected Output:\\nSentence: "We surpass the state of the art on iNaturalist with a top-1 accuracy of 93.8%, compared to InternImage\'92s 92.6%."\\nAccuracy: 93.8']\
[7, 8, 9] 3342.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 3/6 [06:38<06:37, 132.41s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "For example, on ImageNet1K, the top-1 accuracy went from 88.5% with the base ViT [19] to 89.1% with the augmented ViT having a similar parameter count, while OmniVec achieved 92.4%."\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "For example, on ImageNet1K, the top-1 accuracy went from 88.5% with the base ViT [19] to 89.1% with the augmented ViT having a similar parameter count, while OmniVec achieved 92.4%."\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "For example, on ImageNet1K, the top-1 accuracy went from 88.5% with the base ViT [19] to 89.1% with the augmented ViT having a similar parameter count, while OmniVec achieved 92.4%."\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "For example, on ImageNet1K, the top-1 accuracy went from 88.5% with the base ViT [19] to 89.1% with the augmented ViT having a similar parameter count, while OmniVec achieved 92.4%."\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "For example, on ImageNet1K, the top-1 accuracy went from 88.5% with the base ViT [19] to 89.1% with the augmented ViT having a similar parameter count, while OmniVec achieved 92.4%."\\nAccuracy: 92.4']\
[10, 11, 12] 3361.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  67%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                               | 4/6 [08:51<04:25, 132.59s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\\nAccuracy: 92.4', 'Expected Output:\\nSentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\\nAccuracy: 92.4']\
[13, 14, 15] 3707.5\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4  83%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9611                        | 5/6 [11:04<02:12, 132.78s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: "Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11"\\nAccuracy: 404', 'Expected Output:\\nSentence: "Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11"\\nAccuracy: 404', 'Expected Output:\\nSentence: "Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11"\\nAccuracy: 404', 'Expected Output:\\nSentence: "Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11"\\nAccuracy: 404', 'Expected Output:\\nSentence: "Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11"\\nAccuracy: 404']\
[16, 17, 18] 75.0\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 6/6 [13:15<00:00, 132.59s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 ['Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404', 'Expected Output:\\nSentence: -\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 404\
93.8\
92.4\
92.4\
404\
404\
92.4\
---}