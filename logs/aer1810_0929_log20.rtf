{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red254\green212\blue213;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\c87059;\cssrgb\c100000\c86667\c86667;\cssrgb\c100000\c100000\c100000;
}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf2 \cb3 \expnd0\expndtw0\kerning0
  0%|                                                                                                                                                     | 0/8 [00:00<?, ?it/s]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 expect  ['OmniVec: Learning robust representations with cross modal sharing'\
 'OminiVec (FT)' '92.40%']\
groundtruth resulst\
[1, 2, 3] 3815.75\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 /Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\
  warn_deprecated(\
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 ['404', '404', '404', '404', '404']\
[4, 5, 6] 3545.75\
['Expected Output:\\nSentence: "In each case, the models are pre-trained on the ImageNet [8] and then fine-tuned on the target datasets."\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: "In each case, the models are pre-trained on the ImageNet [8] and then \uc0\u64257 ne-tuned on the target datasets."\\nAccuracy: 404', '404', '404']\
[7, 8, 9] 3841.0\
['404', '404', '404', '404', '404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 12%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9612                                                                                                                           | 1/8 [06:36<46:18, 396.95s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3863.0\
['Expected Output:\\nSentence: "A major advantage of the proposed method in comparison to previous work is that, is does not require any additive image dataset nor very costly manual annotation, while it achieves state-of-the-art performances on four publicly available benchmarks in image classification."\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: "Above the proposal itself and the demonstration of it performance in practice, the other major contribution of this paper lies in Section 4 in which we analyze and clarify the reasons why our approach works. Compared to representations obtained from standard CNNs trained with specific labels, an advantage of MuCaLe appears when the filters fail at the subordinate-level (e.g. in Fig 1, the filters for Tesla and Ford are both weakly activated), which is often the case since the categories are finer thus harder to identify. With our proposal, the descriptor at least contains features that capture common properties among basic-level categories (e.g. filters of Car are highly activated), making it more robust for classification problems."\\nAccuracy: 404', '404', '404']\
[4, 5, 6] 3755.5\
['404', '404', '404', '404', '404']\
[7, 8, 9] 3658.25\
['404', '404', '404', '404', 'Expected Output:\\nSentence: "Szegedy et al. [33]* GoogleNet 90.5 77.7 82.7 81.9 Simonyan et al. [32]* VGG-16 88.8 78.0 86.1 84.5 He et al. [13]* ResNet-50 90.8 78.9 84.4 83.1 He et al. [13]* ResNet-101 91.4 80.1 85.6 84.4"\\nAccuracy: 90.8']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                                                          | 2/8 [13:17<39:47, 397.93s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3324.25\
['404', '404', '404', '404', '404']\
[4, 5, 6] 3113.5\
['404', '404', 'Expected Output:\\nSentence: "We can see that the overall accuracy is 61.54%, and some pairs of classes are very confusing with each other (e.g. Knitwear and Sweater), which means that the noisy labels are not so reliable."\\nAccuracy: 61.54', 'Expected Output:\\nSentence: "We can see that the overall accuracy is 61.54%, and some pairs of classes are very confusing with each other (e.g. Knitwear and Sweater), which means that the noisy labels are not so reliable."\\nAccuracy: 61.54', 'Expected Output:\\nSentence: "We can see that the overall accuracy is 61.54%, and some pairs of classes are very confusing with each other (e.g. Knitwear and Sweater), which means that the noisy labels are not so reliable."\\nAccuracy: 61.54']\
[7, 8, 9] 3131.0\
['Expected Output:\\nSentence: "However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2."\\nAccuracy: 404', 'Expected Output:\\nSentence: "However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2."\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: "However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2."\\nAccuracy: 404', 'Expected Output:\\nSentence: "However, by comparing row #2 and #3, we find that training with random initialization on additional massive noisy labeled data is better than finetuning only on the clean data, which demonstrates the power of using large-scale yet easily obtained noisy labeled data. The accuracy can be further improved if we finetune the model either from an ImageNet pretrained one or model #2."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 38%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9612                                                                                        | 3/8 [19:56<33:12, 398.44s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3512.0\
['404', '404', '404', '404', '404']\
[4, 5, 6] 3278.0\
['Expected Output:\\nSentence: "Our instance of the model attains an error rate of 41.6% on the validation set."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our instance of the model attains an error rate of 41.6% on the validation set."\\nAccuracy: 58.4', 'Expected Output:\\nSentence: "Our instance of the model attains an error rate of 41.6% on the validation set."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our instance of the model attains an error rate of 41.6% on the validation set."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Our instance of the model attains an error rate of 41.6% on the validation set."\\nAccuracy: 58.4']\
[7, 8, 9] 3605.75\
['Expected Output:\\nSentence: "The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data."\\nAccuracy: 404', 'Expected Output:\\nSentence: "The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data."\\nAccuracy: 404', '404', 'Expected Output:\\nSentence: "The performance of the baseline ImageNet-Feat-LR on Stanford Dogs data is not reported because the this dataset is a subset of ImageNet data."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 50%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                                                       | 4/8 [26:34<26:32, 398.18s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3668.75\
['404', '404', '404', '404', '404']\
[4, 5, 6] 3624.5\
['Expected Output:\\nSentence: "The top-1 and top-5 errors are 39.76% and 17.71%, respectively."\\nAccuracy: 39.76', 'Expected Output:\\nSentence: "The top-1 and top-5 errors are 39.76% and 17.71%, respectively."\\nAccuracy: 39.76', 'Expected Output:\\nSentence: "The top-1 and top-5 errors are 39.76% and 17.71%, respectively."\\nAccuracy: 39.76', 'Expected Output:\\nSentence: "The top-1 and top-5 errors are 39.76% and 17.71%, respectively."\\nAccuracy: 39.76', '404']\
[7, 8, 9] 3754.25\
['Expected Output:\\nSentence: "On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively."\\nAccuracy: 24.79', 'Expected Output:\\nSentence: "On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively."\\nAccuracy: 24.79', 'Expected Output:\\nSentence: "On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively."\\nAccuracy: 24.79', 'Expected Output:\\nSentence: "On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively."\\nAccuracy: 24.79', 'Expected Output:\\nSentence: "On the ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors of 24.79% and 7.50% respectively."\\nAccuracy: 24.79']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 62%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9612                                                     | 5/8 [33:12<19:54, 398.13s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3682.5\
['Expected Output:\\nSentence: "Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Empirical evaluations in Sec. 4 confirm that MI feature selection has a clear edge over PQ and other compression methods in large scale image classification."\\nAccuracy: 404']\
[4, 5, 6] 3270.25\
['Expected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -', 'Expected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Because we did not \uc0\u64257 nd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classi\u64257 ers using a DCD linear SVM classi\u64257 er."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 2. Top-5 accuracy on the ILSVRC 2010 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We evaluate the proposed mutual information based im- portance sorting feature selection method on several large scale benchmarks."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "It is compared with PQ (product quan- tization) and BPBC."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Since the datasets are large scale and time consuming to evaluate, we use PQ results from the lit- erature when they are available for a dataset, otherwise we report PQ results from our own implementation."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We use the Fisher Vector to represent all images, follow- ing the setup in [22]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Only the mean and variance part in FV are used."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The base visual descriptor is SIFT, which is re- duced from 128 to 64 dimensional using PCA."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The number of Gaussian components is 256."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We use the spatial pyramid matching structure in [3] which extracts 8 spatial regions from an image."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Its structure is: the whole image, three hori- zontal regions, and two by two split regions."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The total num- ber of dimensions in FV is D = 64\'d72\'d7256\'d78 = 262144."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We revise the dual coordinate descent algorithm to learn a linear SVM classi\u64257 er from our selected and quantized fea- tures or BPBC; and use the LIBLINEAR software package in our PQ experiments."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The following benchmark datasets are used:"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "\'95 VOC 2007 [6]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "It has 20 object classes."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Each image may contain more than one object."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We use all the train- ing and validation images (5K) for training and the test- ing images (5K) for testing."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "\'95 ILSVRC 2010 [2]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "It has 1000 classes and 1.2M train- ing images."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We use all provided training and testing images for training and testing, respectively."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "\'95 SUN 397 [28]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "It has 397 classes."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In each class, we use 50 training images and 50 testing images."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "\'95 Scene 15 [15]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "It has 15 classes."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In each class, 100 images are used for training, and the rest images are used for testing."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In PQ, we use the segment length d = 8, which has the overall best performance in [22] under different compres- sion ratios and also used in BPBC [10]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We use k-means to generate codebooks."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Then, we change the codebook size K to achieve different compression ratios in PQ."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In BPBC, we reshaped FV into a 128\'d72048 matrix, and learn bilinear projections to achieve different compression ratios."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "BPBC parameters need iterative updates, for which a maximum of 10 iterations is used in our experiments."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The results are averaged on 5 random train/test splits in Scene 15 and SUN 397."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In VOC 2007, we use the prede- \u64257 ned split, but run 5 times to get different GMM models and report the average mAP."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "For ILSVRC 2010, we run one time using the given split."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "All experiments are tested on a computer with Intel i7-3930K CPU and 32G main mem- ory."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "All CPU cores are used during feature compression."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In classi\u64257 er learning and testing, only one core is used."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We \u64257 rst report the absolute classi\u64257 cation performance (top 1 accuracy, top 5 accuracy, or mAP)."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Where space permits, we also report the loss of performance (delta be- tween the performance obtained from uncompressed and compressed data) for easier comparisons."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Finally, we com- pare the ef\u64257 ciency of feature selection or feature compres- sion, classi\u64257 er learning and testing for these three methods."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "4.1."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "VOC 2007 Mean average precisions (mAP) of various methods are shown in Table 1."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We use the code from [3]2 to generate FV with the same length as [22], thus it is fair to compare MI\'92s performance with the PQ result from [22]."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Under the same compression ratio, MI\'92s mAP is higher than that of PQ on VOC 2007."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The uncompressed result in our experiment and two cited PQ methods are close, but the accuracy loss of MI is less than that of PQ."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "For example, when the ratio is 256, MI only loses 1.75% mAP, while PQ in [26] lost 8.5%."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In MI, a compression ratio 32 means that all dimen- sions are kept but quantized."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Similarly, ratio 128 means that a quarter dimensions are selected and quantized."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Ratio 32 provides the best discriminative ability in classi\u64257 cation, which con\u64257 rms yet another time that the 1-BIT quantization is effective in keeping useful information in features."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Another important fact is that when compression ratio is smaller than 128, MI\'92s mAP is higher than the uncom- pressed one."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "For example, ratio 64 (half dimensions used) has almost the same mAP as ratio 32 (all dimensions used)."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "This observation corroborates that removing (a large por- tion of) noisy features will not hurt classi\u64257 cation."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In contrast, PQ\'92s accuracy decreases quickly and mono- tonically when the compression ratio increases."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "The clas- si\u64257 cation results of MI with more compression ratios are shown from 32 to 1024."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "When the compression ratio is 256, MI is comparable to that of PQ with compression ratio 32."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Even with compression ratio 1024, MI\'92s mAP (46.52%) is still acceptable\'97remember that only 1024 bytes are needed to store the FV for an image at this compression level!"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "MI\'92s classi\u64257 er training time on VOC 2007 is shown in Fig. 4."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "When the compression ratio changes from 32 to 1024 (doubling each time), the training time approximately halves each time."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "In other words, training time is roughly linearly proportional to storage size."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "4.2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "ILSVRC 2010 We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Limited by our computer\'92s memory capacity, we need to start from compression ratio 64 in MI."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "As shown in Table 2, MI\'92s result is better than PQ\'92s [22] with the same FV setup and the same compression ratio."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "MI with com- pression ratio 128 has similar result as PQ at ratio 32."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "If absolute accuracy rates are concerned, [19] reported that PQ with a well-tailored SGD classi\u64257 er achieves 66.5% top-5 accuracy."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "When combining more visual descriptors like color descriptors [22, 23] and LBP [16], higher accu- racy can be achieved on this dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We conjecture that the proposed feature selection framework can also achieve better results than PQ in these richer representations."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "4.3."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "SUN 397 We show accuracy of MI and PQ on the SUN 397 dataset in Table 3."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Limited by our main memory size, we do not evaluate accuracy of the uncompressed dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Because we did not \u64257 nd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classi\u64257 ers using a DCD linear SVM classi\u64257 er."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with PQ, MI is 0.8% in- ferior to PQ when compression ratio is 32, but better than"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Method Compression ratio Accuracy (%) dense FV [23] 1 43.3 multiple features [28] 1 38.0 spatial HOG [8] 1 26.8 MI 32 41.88\'b10.31 64 42.05\'b10.36 128 40.42\'b10.40 256 37.36\'b10.34 PQ 32 42.72\'b10.45 64 41.74\'b10.38 128 40.13\'b10.33 256 37.84\'b10.33"\\nAccuracy: 43.3', 'Expected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -', 'Expected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Because we did not \u64257 nd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classi\u64257 ers using a DCD linear SVM classi\u64257 er."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than"\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 2. Top-5 accuracy on the ILSVRC 2010 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Because we did not \u64257 nd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classi\u64257 ers using a DCD linear SVM classi\u64257 er."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than"\\nAccuracy: -\\n\\nExpected Output:\\n', 'Expected Output:\\nSentence: "We report top-5 accuracy on the ILSVRC 2010 dataset in Table 2."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Table 3. Top-1 accuracy on the SUN 397 dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with uncompressed FV [23], MI is inferior yet close to its accuracy when the compression is small, e.g., 32 and 64."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Note that when color descriptors are combined with SIFT, higher absolute accuracy is achieved [23] on this dataset."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Because we did not \u64257 nd any PQ compression results on this dataset, we implemented and evaluated our own PQ feature compression, and learned classi\u64257 ers using a DCD linear SVM classi\u64257 er."\\nAccuracy: -\\n\\nExpected Output:\\nSentence: "Comparing with PQ, MI is 0.8% inferior to PQ when compression ratio is 32, but better than"\\nAccuracy: -']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 75%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608                                    | 6/8 [38:05<12:13, 366.65s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3713.75\
['404', '404', '404', '404', '404']\
[4, 5, 6] 3794.25\
['404', '404', '404', '404', '404']\
[7, 8, 9] 4022.0\
['Expected Output:\\nSentence: "ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: "ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: "ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: "ImageNet AlexNet 42.6 19.6 MLNN-1 42.2 19.0 MLNN-2 41.3 18.5 MLNN-3 41.1 18.2 ResNet-50 24.53 7.89 Res-MLNN-50 23.27 7.02"\\nAccuracy: 24.53', 'Expected Output:\\nSentence: "Table 5 reports the top-1 and top-5 error rates on the validation data. Convolutional nets with MLNN outperform the base models by a large margin (0.4%-1.5% in top-1 error rate and 0.6%-1.4% in top-5 error rate)."\\nAccuracy: 404']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
 88%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9612                  | 7/8 [44:45<06:16, 376.70s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####\
[1, 2, 3] 3815.5\
['Expected Output:\\nSentence: "Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated."\\nAccuracy: 404', 'Expected Output:\\nSentence: "Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated."\\nAccuracy: 404']\
[4, 5, 6] 3595.5\
['404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: "In Table 1, we show two settings of pre-training schemes. The R-CNN [16] for object detection and segmentation adopted strategy 2 in training (denoted by S1)."\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404', 'Expected Output:\\nSentence: 404\\nAccuracy: 404']\
[7, 8, 9] 2353.5\
['Expected Output:\\nSentence: "Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy."\\nAccuracy: 67.0', 'Expected Output:\\nSentence: "Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy."\\nAccuracy: 67.0', 'Expected Output:\\nSentence: "Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy."\\nAccuracy: 67.0', 'Expected Output:\\nSentence: "Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy."\\nAccuracy: 67.0', 'Expected Output:\\nSentence: "Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy."\\nAccuracy: 67.0']\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 8/8 [51:27<00:00, 385.99s/it]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 #### EOD #####}