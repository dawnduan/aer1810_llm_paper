# -*- coding: utf-8 -*-
"""token_length_experiment-extended_size.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SurvzBQ60Bfa4FAXI3V9P8I5BueAne8G

The motivation of the experiments is to populate comparsion study for the impact of pdf versus text to GPT.
- inputs consists of text, pdf varied from token length, 10, 20, 30k, full length.
- GPT agents consist of chatgpt and GPT4

Base assumption is that results stay close with each other.
"""

# pdf
import fitz  # PyMuPDF
from PyPDF2 import PdfReader
import pymupdf
# string parse

# langchain
from langchain.chat_models import ChatOpenAI
from langchain import LLMChain
from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
openai_api_key = ''
# dataframe processor
import numpy as np
# os
import os, requests, sys, pathlib
import time
import pandas as pd
from tqdm import tqdm
from utils.groundtruth_parser import extract_text_from_pdf_as_dict, find_fname

# """##
#
# ## GPT (FULL TEXT, 10K, 30K)
#
# ### preprocess for groundtruths, relevant result page parsing
# """
#
# fn = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/groundtruth_table.csv'
# groundtruth_df = pd.read_csv(fn)
# # groundtruth_df
#
# find_fname = lambda pdf_path: str(pdf_path).split('/')[-1]
#
# def extract_text_from_pdf_as_dict(pdf_path):
#     """
#     Extract text from a PDF at the specified path using PyMuPDF.
#     """
#     with fitz.open(pdf_path) as doc:
#         return {
#             i+1:page.get_text() # starting from 1
#             for i,page in enumerate(doc)
#         }
#
#
# key_indices = groundtruth_df.page_key.values
# accuracies = groundtruth_df['Top-5 Accuracy'].values
#
# # first key is file nm; second is page number for answer
# ground_truths = {
#     idx: [find_fname(path), key_indices[idx]]
#     for idx, path in enumerate(sorted_pdf_paths)
# }
# ground_truths
# pdf_keys = {
#     path : key_indices[idx]
#     for idx, path in enumerate(sorted_pdf_paths)
# }
# pdf_page_lengths = {
#     idx: [path, key_indices[idx], max(extract_text_from_pdf_as_dict(path))]
#     for idx, path in enumerate(sorted_pdf_paths)
# }
#
#
# # pdf_page_lengths
# sorted_pdf_key_page = {
#     idx: [path, key_indices[idx], extract_text_from_pdf_as_dict(path)[key_indices[idx]] if key_indices[idx]>0 else '']
#     for idx, path in enumerate(sorted_pdf_paths)
# }
#
# # sorted_pdf_key_page
#
# folder_path_20 = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_20/'
# sorted_pdf_paths_20 = sorted([path for path in Path(folder_path_20).iterdir() if is_pdf(path)])
# sorted_pdf_key_page_20 = {
#         idx: [find_fname(path), path]
#         for idx, path in enumerate(sorted_pdf_paths_20)
#     }
# df_20 = pd.DataFrame.from_dict(
#     sorted_pdf_key_page_20,orient='index',
#     columns=['file_name', 'path',]
# )
# from functools import reduce
# fn = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/groundtruth_table.csv'
# groundtruth_df = pd.read_csv(fn)
# # groundtruth_df
# template = groundtruth_df.copy()[['file_name', 'Paper Name', 'Model', 'Top-1 Accuracy']]
# reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='right'), [template, df_20])
#
#
#
#
# folder_path_100 = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_100/'
# sorted_pdf_paths_100 = sorted([path for path in Path(folder_path_100).iterdir() if is_pdf(path)])
# sorted_pdf_key_page_100 = {
#         idx: [find_fname(path), path]
#         for idx, path in enumerate(sorted_pdf_paths_100)
#     }
# df_100 = pd.DataFrame.from_dict(
#     sorted_pdf_key_page_100,orient='index',
#     columns=['file_name', 'path',]
# )
# reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='outer'), [template, df_100])


"""## Chunk methods

## Results with increased paper size (12, 20, 100)
"""

prompt_self_veri = """
extract the top1 accuracy of ImageNet from the given text and return both the sentence containing the accuracy.
Answer in a number, eg. 90.2% and the accuracy value in 1 number. 404 if it's not mentioned. Use the examples below as a guide.

Example 1:
Expected Output:
Sentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: α ⊗β ⊗γ" achieved a Top-1 accuracy of 57.1."
Accuracy: 57.1

Example 2:
Expected Output:
Sentence: "In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions."
Accuracy: 82.1

Example 3:
Expected Output:
Sentence: "The evaluation results showed a top-1 accuracy of 78.3% on the test data on Imagenet."
Accuracy: 78.3

Example 4:
Expected Output:
Sentence: "Our proposed model achieved a top-1 accuracy of 74.2% when evaluated on the Imagenet dataset."
Accuracy: 74.2

Example 4:
Expected Output:
Sentence: "Our proposed model achieved a top-5 accuracy of 66.2% when evaluated on the Imagenet dataset."
Accuracy: -

Now extract the top1 accuracy of ImageNet from the following texts, {page}

Expected Output:
"""

prompt_vote_accuracy = """Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.

Example 1:
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 92.4',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
Expected Output: 92.4

Example 2:
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4"\nAccuracy: 82.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 82.4',
Expected Output: 82.4

Example 3:
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: 404',
Expected Output: -

Example 4:
'Sentence: "It's not mentioned."\nAccuracy: -',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's mentioned top-5 accuracy on ImageNet"\nAccuracy: -',
'Sentence: "Cocoa 23.3 21.2"\nAccuracy: 23.3',
Expected Output: -

Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}

Expected Output:
"""

prompt_vote_accuracy = """Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.

Example 1:
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 92.4',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
Expected Output: 92.4

Example 2:
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4"\nAccuracy: 82.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 82.4',
Expected Output: 82.4

Example 3:
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: 404',
Expected Output: -

Example 4:
'Sentence: "It's not mentioned."\nAccuracy: -',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's mentioned top-5 accuracy on ImageNet"\nAccuracy: -',
'Sentence: "Cocoa 23.3 21.2"\nAccuracy: 23.3',
Expected Output: -

Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}

Expected Output:
"""

def parse_gpt_with_page_prompt(page_with_res, prompt):
    openai_api_key = ''
    prompt2 = ChatPromptTemplate.from_template(prompt)
    output_parser = StrOutputParser()
    model = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0.0,)

    chain = prompt2 | model | output_parser
    try:
        prompt_value = chain.invoke(
            {'page': page_with_res}
        )
    except Exception as e:
        prompt_value = str(e)
    return prompt_value

def delayed_completion(delays_in_sec, **kwargs):
    time.sleep(delays_in_sec)
    return parse_gpt_with_page_prompt(**kwargs)

def parse_gpt_with_vote(sentences_and_accuracies, prompt):
    openai_api_key = ''
    model = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0.0, max_tokens=5)

    chain = ChatPromptTemplate.from_template(prompt) | model | StrOutputParser()
    try:
        prompt_value = chain.invoke(
            {'sentences_and_accuracies': sentences_and_accuracies}
        )
    except Exception as e:
        prompt_value = str(e)
    return prompt_value

##### PARTIAL PROCESS FOR SINGLE PAPER ############
idx = 11
pdf_path, rel_key, rel_page = sorted_pdf_key_page[idx]
print('expect ', groundtruth_df[['Paper Name','Model','Top-1 Accuracy']].iloc[idx].values)
rel_key = rel_key if rel_key>0 else 2
rel_pg_num = [rel_key, rel_key-1, rel_key+1]
rel_pg_num.sort()
text_dict = extract_text_from_pdf_as_dict(pdf_path)
rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in rel_pg_num])

# Calculate the delay based on your rate limit
token_limit_per_minute = 10000
delay = 60.0 / token_limit_per_minute * len(rel_page) / 3

print('groundtruth resulst')
# responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]
# print(responses)

# print('groundtruth resulst')
def chunk_text_keys(keys, stepsize):
    lis = []
    sublis = []
    for k in keys:
        sublis += [k]
        if k%3 == 0:
            lis.append(sublis)
            sublis = []
    return lis

# responses_lis = []
# for sublis in tqdm(chunk_text_keys(text_dict.keys(), stepsize=3)):
#     rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in sublis])
#     print(sublis, len(rel_page)/4)
#     responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]
#     print(responses)
#     responses_lis.append(responses)

# [parse_gpt_with_vote(res, prompt_vote_accuracy) for res in responses_lis]
##### PARTIAL PROCESS FOR SINGLE PAPER END ############
def _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy):
    # print('expect ', groundtruth_df[['Paper Name','Model','Top-1 Accuracy']].iloc[idx].values)
    text_dict = extract_text_from_pdf_as_dict(pdf_path)

    # compose sublist
    responses_lis = []
    for _, sublis in enumerate(chunk_text_keys(text_dict.keys(), stepsize=4)):
        rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in sublis])
        print(sublis, len(rel_page)/4)
        responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]
        responses_lis.append(responses)
        print(responses)

    # assumption is that the vote_ensemble will consistently return NA or Results
    relevant_response = [parse_gpt_with_vote(res, prompt_vote_accuracy) for res in responses_lis]
    rel_res = [ans for ans in relevant_response if ans != '404']
    final_res = parse_gpt_with_vote(rel_res, prompt_vote_accuracy) if rel_res else '404'
    print('#### EOD #####')
    return final_res

res_df_whole_paper = pd.DataFrame(data=[[idx, find_fname(pdf_path), _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy)] \
                                          for idx, (fname, pdf_path) in tqdm(sorted_pdf_key_page_20.items())], \
                                    columns = ['file_idx', 'file_name', 'gpt_vote_ensemble_whole_paper'])

reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='right'), [template, new])

reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='outer'), [template, res_df_whole_paper])

"""## Results with increased paper size (100)

#### Parse 100 pdfs to the destination folders
"""

import shutil
import os

# Define source and destination folder paths
source_folder = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/'
destination_folder = '/Users/dawn.duan/Library/CloudStorage/OneDrive-CanadianTire/Documents/tetris/ivado_or/tetris-api-worker/optimization/local_experiments/aer1810/10_benchmark_datasets/all_papers_100/'

existing_fns = set([find_fname(pdf_path) for pdf_path in sorted_pdf_paths] + [find_fname(pdf_path) for pdf_path in sorted_pdf_paths_20])
files = os.listdir(source_folder)
pdf_files = [f for f in files if f.endswith('.pdf') if f not in existing_fns]
pdf_files

# Loop through the list of PDF files and move them
for pdf_file in pdf_files[:100]:  # Move only 100 PDF files
    source_file = os.path.join(source_folder, pdf_file)
    destination_file = os.path.join(destination_folder, pdf_file)

    # Move the file
    shutil.move(source_file, destination_file)

print(f'Moved {len(pdf_files[:100])} PDF files from {source_folder} to {destination_folder}')

"""## Chunk 100 pdfs"""

prompt_self_veri = """
extract the top1 accuracy of ImageNet from the given text and return both the sentence containing the accuracy.
Answer in a number, eg. 90.2% and the accuracy value in 1 number. 404 if it's not mentioned. Use the examples below as a guide.

Example 1:
Expected Output:
Sentence: "a table showing the Top-1 and Top-5 classification accuracy using a binarized ResNet-18 on Imagenet for various ways of constructing the scaling factor. The method "Case 4: α ⊗β ⊗γ" achieved a Top-1 accuracy of 57.1."
Accuracy: 57.1

Example 2:
Expected Output:
Sentence: "In our experiments, the model reported a top-1 accuracy of 82.1% on Imagenet under the given conditions."
Accuracy: 82.1

Example 3:
Expected Output:
Sentence: "The evaluation results showed a top-1 accuracy of 78.3% on the test data on Imagenet."
Accuracy: 78.3

Example 4:
Expected Output:
Sentence: "Our proposed model achieved a top-1 accuracy of 74.2% when evaluated on the Imagenet dataset."
Accuracy: 74.2

Example 4:
Expected Output:
Sentence: "Our proposed model achieved a top-5 accuracy of 66.2% when evaluated on the Imagenet dataset."
Accuracy: -

Now extract the top1 accuracy of ImageNet from the following texts, {page}

Expected Output:
"""

prompt_vote_accuracy = """Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.

Example 1:
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 92.4',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
Expected Output: 92.4

Example 2:
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4"\nAccuracy: 82.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 82.4',
Expected Output: 82.4

Example 3:
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: 404',
Expected Output: -

Example 4:
'Sentence: "It's not mentioned."\nAccuracy: -',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's mentioned top-5 accuracy on ImageNet"\nAccuracy: -',
'Sentence: "Cocoa 23.3 21.2"\nAccuracy: 23.3',
Expected Output: -

Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}

Expected Output:
"""

prompt_vote_accuracy = """Find the accuracy value associated with most common sentences from the list of sentences and accuracies. Only output the accuracy value.

Example 1:
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 92.4',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4"\nAccuracy: 92.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
Expected Output: 92.4

Example 2:
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 92.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 74.6',
'Sentence: "ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4"\nAccuracy: 82.4',
'Sentence: "SSv2 Top-1 Accuracy ViViT 65.4 68.6 80.1 85.4 ImageNet1K Top-1 Accuracy ViT 88.5 89.1 88.6 82.4 Sun RGBD Top-1 Accuracy Simple3D-former 57.3 62.4 71.4 74.6"\nAccuracy: 82.4',
Expected Output: 82.4

Example 3:
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's not mentioned top-1 accuracy on ImageNet"\nAccuracy: 404',
'Sentence: "-"\nAccuracy: 404',
Expected Output: -

Example 4:
'Sentence: "It's not mentioned."\nAccuracy: -',
'Sentence: "-"\nAccuracy: -',
'Sentence: "It's mentioned top-5 accuracy on ImageNet"\nAccuracy: -',
'Sentence: "Cocoa 23.3 21.2"\nAccuracy: 23.3',
Expected Output: -

Now extract the accuracy value associated with most common sentences, {sentences_and_accuracies}

Expected Output:
"""

def parse_gpt_with_page_prompt(page_with_res, prompt):
    openai_api_key = ''
    prompt2 = ChatPromptTemplate.from_template(prompt)
    output_parser = StrOutputParser()
    model = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0.0,)

    chain = prompt2 | model | output_parser
    try:
        prompt_value = chain.invoke(
            {'page': page_with_res}
        )
    except Exception as e:
        prompt_value = str(e)
    return prompt_value

def delayed_completion(delays_in_sec, **kwargs):
    time.sleep(delays_in_sec)
    return parse_gpt_with_page_prompt(**kwargs)

def parse_gpt_with_vote(sentences_and_accuracies, prompt):
    openai_api_key = ''
    model = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0.0, max_tokens=5)

    chain = ChatPromptTemplate.from_template(prompt) | model | StrOutputParser()
    try:
        prompt_value = chain.invoke(
            {'sentences_and_accuracies': sentences_and_accuracies}
        )
    except Exception as e:
        prompt_value = str(e)
    return prompt_value


# Calculate the delay based on your rate limit
token_limit_per_minute = 10000
delay = 60.0 / token_limit_per_minute * len(rel_page) / 3

def chunk_text_keys(keys, stepsize):
    lis = []
    sublis = []
    for k in keys:
        sublis += [k]
        if k%3 == 0:
            lis.append(sublis)
            sublis = []
    return lis

def _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy):
    # print('expect ', groundtruth_df[['Paper Name','Model','Top-1 Accuracy']].iloc[idx].values)
    text_dict = extract_text_from_pdf_as_dict(pdf_path)

    # compose sublist
    responses_lis = []
    for _, sublis in enumerate(chunk_text_keys(text_dict.keys(), stepsize=4)):
        rel_page = ''.join([texts for pg_num, texts in text_dict.items() if pg_num in sublis])
        print(sublis, len(rel_page)/4)
        responses = [delayed_completion(delays_in_sec=delay, page_with_res=rel_page, prompt=prompt_self_veri) for i in range(5)]
        responses_lis.append(responses)
        print(responses)

    # assumption is that the vote_ensemble will consistently return NA or Results
    relevant_response = [parse_gpt_with_vote(res, prompt_vote_accuracy) for res in responses_lis]
    rel_res = [ans for ans in relevant_response if ans != '404']
    final_res = parse_gpt_with_vote(rel_res, prompt_vote_accuracy) if rel_res else '404'
    print('#### EOD #####')
    return final_res

res_df_whole_paper = pd.DataFrame(data=[[idx, find_fname(pdf_path), _vote_emsemble_across_paper(idx, pdf_path, prompt_self_veri, prompt_vote_accuracy)] \
                                          for idx, (fname, pdf_path) in tqdm(sorted_pdf_key_page_100.items())], \
                                    columns = ['file_idx', 'file_name', 'gpt_vote_ensemble_whole_paper'])

reduce(lambda l, r: pd.merge(l, r, on=['file_name'],how='outer'), [template, res_df_whole_paper])


def main():
    pass

if __name__ == "__main__":
    main()






